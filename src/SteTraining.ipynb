{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models\n",
    "\n",
    "This notebook contains the code used for training the following learning models:\n",
    "\n",
    "-  **Standard GBDT** (_baseline 1_)\n",
    "-  **Adversarial Boosting** (_baseline 2_)\n",
    "-  **Non-Interferent GBDT** (our proposal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    " - http://lightgbm.readthedocs.io/en/latest/\n",
    " - http://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
    " - https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "import pickle\n",
    "import json\n",
    "import functools\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(dataset, categorical_features):\n",
    "    dataset_le = dataset.copy()\n",
    "    for column in dataset_le.columns:\n",
    "        if column in categorical_features:\n",
    "            dataset_le[column] = dataset_le[column].astype('category')\n",
    "            dataset_le[column] = dataset_le[column].cat.codes.astype(np.int32)\n",
    "    return dataset_le\n",
    "\n",
    "def load_atk_train_valid_test(atk_train_file, atk_valid_file, atk_test_file, \n",
    "                              train_split=0.6, valid_split=0.2, force=True):\n",
    "    \n",
    "    \n",
    "    if  (force or \n",
    "          not os.path.exists(atk_train_file+\".cat.bz2\") or\n",
    "          not os.path.exists(atk_valid_file+\".cat.bz2\") or\n",
    "          not os.path.exists(atk_test_file+\".cat.bz2\") or \n",
    "          not os.path.exists(atk_train_file+\".cat.json\") ):\n",
    "    \n",
    "        print (\"Pre-processing original files...\")\n",
    "\n",
    "        print (\"Loading:\", atk_train_file)\n",
    "        print (\"Loading:\", atk_valid_file)\n",
    "        print (\"Loading:\", atk_test_file)\n",
    "\n",
    "        train = pd.read_csv(atk_train_file)\n",
    "        valid = pd.read_csv(atk_valid_file)\n",
    "        test  = pd.read_csv(atk_test_file)\n",
    "        \n",
    "        print (\"Train/Valid/Test sizes:\", train.shape, valid.shape, test.shape)\n",
    "        print (\"Train/Valid/Test split: {:.2f} {:.2f} {:.2f}\"\n",
    "                   .format( train.shape[0]/(train.shape[0]+valid.shape[0]+test.shape[0]),\n",
    "                            valid.shape[0]/(train.shape[0]+valid.shape[0]+test.shape[0]),\n",
    "                            test.shape[0] /(train.shape[0]+valid.shape[0]+test.shape[0]) ) )\n",
    "\n",
    "\n",
    "        # split-back into train valid test\n",
    "        if 'instance_id' in train.columns.values:\n",
    "            print ('   ... with instance ids')\n",
    "            valid['instance_id'] += train.iloc[-1,0]\n",
    "            test['instance_id']  += valid.iloc[-1,0]\n",
    "            assert max(train['instance_id'])<min(valid['instance_id']), \"Instance ID mismatch\"\n",
    "            assert max(valid['instance_id'])<min(test['instance_id']), \"Instance ID mismatch\"\n",
    "            \n",
    "            groups = np.concatenate( [ train['instance_id'].value_counts().sort_index().values,\n",
    "                                       valid['instance_id'].value_counts().sort_index().values,\n",
    "                                       test['instance_id'].value_counts().sort_index().values ] )\n",
    "            \n",
    "            num_train_groups = int( len(groups)*train_split )\n",
    "            train_size = sum(groups[:num_train_groups])\n",
    "            num_valid_groups = int( len(groups)*valid_split )\n",
    "            valid_size = sum(groups[num_train_groups:num_train_groups+num_valid_groups])\n",
    "        else:\n",
    "            full_size = len(train) + len(valid) + len(test)\n",
    "            train_size = int( full_size*train_split )\n",
    "            valid_size = int( full_size*valid_split )\n",
    "        \n",
    "        # concat to process correctly label encoding\n",
    "        full = pd.concat( [train, valid, test] )\n",
    "\n",
    "        # get index of categorical features (-1 because of instance_id)\n",
    "        cat_fx = full.columns.values[np.where(full.dtypes=='object')[0]]\n",
    "        cat_fx = list(cat_fx)    \n",
    "        full = label_encode(full, cat_fx)\n",
    "        with open(atk_train_file+\".cat.json\", 'w') as fp:\n",
    "            json.dump(cat_fx, fp)\n",
    "        print (\"CatFX:\", cat_fx)\n",
    "\n",
    "        train_cat = full.iloc[0:train_size,:]\n",
    "        valid_cat = full.iloc[train_size:train_size+valid_size,:]\n",
    "        test_cat  = full.iloc[train_size+valid_size:,:]\n",
    "        \n",
    "        assert len(train_cat)+len(valid_cat)+len(test_cat)==len(full), \"Split sizes mismatch\"\n",
    "        \n",
    "\n",
    "        print (\"Train/Valid/Test sizes:\", train_cat.shape, valid_cat.shape, test_cat.shape)\n",
    "        print (\"Train/Valid/Test split: {:.2f} {:.2f} {:.2f}\"\n",
    "                   .format( train_cat.shape[0]/(train_cat.shape[0]+valid_cat.shape[0]+test_cat.shape[0]),\n",
    "                            valid_cat.shape[0]/(train_cat.shape[0]+valid_cat.shape[0]+test_cat.shape[0]),\n",
    "                            test_cat.shape[0] /(train_cat.shape[0]+valid_cat.shape[0]+test_cat.shape[0]) ) )\n",
    "\n",
    "        # save to file\n",
    "        print (\"Saving processed files *.cat.bz2\")\n",
    "        train_cat.to_csv(atk_train_file+\".cat.bz2\", compression=\"bz2\", index=False)\n",
    "        valid_cat.to_csv(atk_valid_file+\".cat.bz2\", compression=\"bz2\", index=False)\n",
    "        test_cat.to_csv (atk_test_file+\".cat.bz2\",  compression=\"bz2\", index=False)\n",
    "        \n",
    "    else:\n",
    "        print (\"Loading pre-processed files...\")\n",
    "\n",
    "        train_cat = pd.read_csv(atk_train_file+\".cat.bz2\")\n",
    "        valid_cat = pd.read_csv(atk_valid_file+\".cat.bz2\")\n",
    "        test_cat  = pd.read_csv(atk_test_file+\".cat.bz2\")\n",
    "        \n",
    "        with open(atk_train_file+\".cat.json\", 'r') as fp:\n",
    "            cat_fx = json.load(fp)\n",
    "    \n",
    "    # return data\n",
    "    return train_cat, valid_cat, test_cat, cat_fx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard\n",
    "\n",
    "The following function, called <code>optimize_log_loss</code>, is the one that should be optimized (i.e., minimized) for learning _standard_ and _baseline_ approaches. More specifically, this is the standard binary log loss which is used to train any _standard_ or _baseline_ model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $L$ = <code>optimize_log_loss</code>\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}}\\ell(h(\\mathbf{x}), y)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\ell(h(\\mathbf{x}), y) = log(1+e^{(-yh(\\mathbf{x}))})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_log_loss(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    exp_pl = np.exp(preds * labels)\n",
    "    # http://www.wolframalpha.com/input/?i=differentiate+log(1+%2B+exp(-kx)+)\n",
    "    grads = -labels / (1.0 +  exp_pl)  \n",
    "    # http://www.wolframalpha.com/input/?i=d%5E2%2Fdx%5E2+log(1+%2B+exp(-kx)+)\n",
    "    hess = labels**2 * exp_pl / (1.0 + exp_pl)**2 \n",
    "\n",
    "    # this is to optimize average logloss\n",
    "    norm = 1.0/len(preds)\n",
    "    grads *= norm\n",
    "    hess *= norm\n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom\n",
    "\n",
    "In addition to the standard binary log loss used to train a model, we introduce our custom <code>optimize_non_interferent_log_loss</code>, which is computed as the weighted combination of two objective functions, as follows:\n",
    "\n",
    "-  $L$ = <code>optimize_log_loss</code> (standard, already seen above);\n",
    "-  $L^A$ = <code>optimize_log_loss_uma</code> (custom, defined below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $L^A$ = <code>optimize_log_loss_uma</code>\n",
    "\n",
    "This function is used to train a **full** _non-interferent_ model; in other words, full non-interferent models are learned by optimizing (i.e., minimizing) the function which measures the binary log loss **under the maximal attack** possible.\n",
    "\n",
    "$$\n",
    "L^A = \\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}} \\log  \\left( \\sum_{\\mathbf{x}' \\in \\mathit{MaxAtk}({\\mathbf{x}},{A})} e^{\\ell(h(\\mathbf{x}'), y)} \\right).\n",
    "$$\n",
    "\n",
    "where still:\n",
    "\n",
    "$$\n",
    "\\ell(h(\\mathbf{x}), y) = log(1+e^{(-yh(\\mathbf{x}))})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_log_loss_uma(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    grads = np.zeros_like(labels, dtype=np.float64)\n",
    "    hess = np.zeros_like(grads)\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "\n",
    "        norm = 1.0 / float(len(attack_lens))\n",
    "\n",
    "        offset = 0\n",
    "        for atk in attack_lens:\n",
    "            exp_pl = np.exp(- preds[offset:offset+atk] * labels[offset:offset+atk])\n",
    "\n",
    "            inv_sum = 1.0 / np.sum(1.0 + exp_pl)\n",
    "\n",
    "            x_grad = inv_sum * exp_pl\n",
    "\n",
    "            grads[offset:offset+atk] = norm * x_grad * (- labels[offset:offset+atk])\n",
    "            hess[offset:offset+atk]  = norm * x_grad * (1.0 - x_grad)\n",
    "\n",
    "            offset += atk    \n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_log_loss_uma_calza(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    grads = np.zeros_like(labels, dtype=np.float64)\n",
    "    hess = np.zeros_like(grads)\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "\n",
    "        norm = 1.0 / float(len(attack_lens))\n",
    "\n",
    "        offset = 0\n",
    "        for atk in attack_lens:\n",
    "            exp_pl = np.exp(- preds[offset:offset+atk] * labels[offset:offset+atk])\n",
    "\n",
    "            inv_sum = 1.0 / np.sum(1.0 + exp_pl)\n",
    "\n",
    "            x_grad = inv_sum * exp_pl\n",
    "\n",
    "            grads[offset:offset+atk] = norm * x_grad * (- labels[offset:offset+atk])\n",
    "            hess[offset:offset+atk]  = norm * x_grad * (1.0 - x_grad)\n",
    "\n",
    "            offset += atk    \n",
    "            \n",
    "    offset = 0\n",
    "    temp = optimize_log_loss(preds,train_data)\n",
    "    for atk in attack_lens:\n",
    "            grads[offset+1:offset+atk] = temp[0][offset+1:offset+atk]\n",
    "            hess[offset+1:offset+atk] = temp[1][offset+1:offset+atk]\n",
    "            \n",
    "            offset += atk       \n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>optimize_non_interferent_log_loss</code>\n",
    "\n",
    "$$\n",
    "\\alpha\\cdot L^A + (1-\\alpha)\\cdot L\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha \\cdot \\underbrace{\\Bigg[\\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}} \\log  \\left( \\sum_{\\mathbf{x}' \\in \\mathit{MaxAtk}({\\mathbf{x}},{A})} e^{\\ell(h(\\mathbf{x}'), y)} \\right)\\Bigg]}_{L^A} + (1-\\alpha) \\cdot \\underbrace{\\Bigg[\\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}} \\ell(h(\\mathbf{x}, y))\\Bigg]}_{L}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_non_interferent_log_loss(preds, train_data, alpha=1.0):\n",
    "    # binary logloss under maximal attack\n",
    "    grads_uma, hess_uma = optimize_log_loss_uma(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    grads_plain, hess_plain = optimize_log_loss(preds, train_data)\n",
    "    \n",
    "    # combine the above two losses together\n",
    "    grads = alpha*grads_uma + (1.0-alpha)*grads_plain\n",
    "    hess  = alpha*hess_uma  + (1.0-alpha)*hess_plain\n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_non_interferent_log_loss_nonatk(preds, train_data, alpha=1.0):\n",
    "    # binary logloss under maximal attack\n",
    "    grads_uma, hess_uma = optimize_log_loss_uma(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    grads_plain, hess_plain = optimize_log_loss(preds, train_data)\n",
    "    \n",
    "    # wipe out !\n",
    "    attack_lens = train_data.get_group()\n",
    "    if attack_lens is not None:\n",
    "        offset = 0\n",
    "        for atk in attack_lens:\n",
    "            grads_uma   [offset+1:offset+atk] = 0.0\n",
    "            hess_uma    [offset+1:offset+atk] = 0.0\n",
    "            grads_plain [offset+1:offset+atk] = 0.0\n",
    "            hess_plain  [offset+1:offset+atk] = 0.0\n",
    "\n",
    "            offset += atk       \n",
    "    \n",
    "    # combine the above two losses together\n",
    "    grads = alpha*grads_uma + (1.0-alpha)*grads_plain\n",
    "    hess  = alpha*hess_uma  + (1.0-alpha)*hess_plain\n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using one objective function for both _standard_ and _non-interferent_ learning\n",
    "\n",
    "The advantage of the <code>optimize_non_interferent_log_loss</code> function defined above is that we can wrap it so that we can use it as the only objective function (<code>fobj</code>) passed in to LightGBM. \n",
    "\n",
    "In other words, if we call <code>fobj=optimize_non_interferent_log_loss</code> with <code>alpha=0.0</code>, this will end up optimizing (i.e., minimizing) the \"vanilla\" objective function (i.e., the standard binary log loss, defined by the function <code>optimize_log_loss</code> above).\n",
    "\n",
    "Conversely, calling <code>fobj=optimize_non_interferent_log_loss</code> with <code>alpha=1.0</code> turns into optimizing (i.e., minimizing) the full non-interferent objective function (i.e., the custom binary log loss under max attack, defined by the function <code>optimize_log_loss_uma</code> above).\n",
    "\n",
    "Anything that sits in between (i.e., <code>0 < alpha < 1</code>) optimizes an objective function that trades off between the standard and the full non-interferent term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard\n",
    "\n",
    "The following function is the one used for evaluating the quality of the learned model (either _standard_, _adversarial-boosting_, or _non-interferent_). This is the standard <code>avg_log_loss</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(p):\n",
    "    return np.log(p/(1-p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>avg_log_loss</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss(preds, train_data):\n",
    "    \n",
    "    labels = train_data.get_label()\n",
    "    losses = np.log(1.0 + np.exp(-preds*labels))\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return 'avg_binary_log_loss', avg_loss, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom\n",
    "\n",
    "Similarly to what we have done for <code>fobj</code>, <code>feval</code> can be computed from a weighted combination of two evaluation metrics:\n",
    "\n",
    "-  <code>avg_log_loss</code> (standard, defined above);\n",
    "-  <code>avg_log_loss_uma</code> (custom, defined below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>avg_log_loss_uma</code>\n",
    "\n",
    "This is the binary log loss yet modified to operate on groups of perturbed instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom metrics\n",
    "def binary_log_loss(pred, true_label):\n",
    "\n",
    "    return np.log(1.0 + np.exp(-pred * true_label))\n",
    "\n",
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss_uma(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    offset = 0\n",
    "    max_logloss = []\n",
    "    avg_max_logloss = 0.0\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "    \n",
    "        for atk in attack_lens:\n",
    "            losses = [binary_log_loss(h,t) for h,t in zip(preds[offset:offset+atk], labels[offset:offset+atk])]\n",
    "            max_logloss.append(max(losses))\n",
    "\n",
    "            offset += atk\n",
    "        \n",
    "        avg_max_logloss = np.mean(max_logloss)  \n",
    "\n",
    "    return 'avg_binary_log_loss_under_max_attack', avg_max_logloss, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>feval=avg_non_interferent_log_loss</code>\n",
    "\n",
    "Used for measuring the validity of any model (either _standard_, _baseline_, or _non-interferent_). More precisely, <code>avg_non_interferent_log_loss</code> is the weighted sum of the binary log loss and the binary log loss under maximal attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_non_interferent_log_loss(preds, train_data, alpha=1.0):\n",
    "    \n",
    "    # binary logloss under maximal attack\n",
    "    _, loss_uma, _    = avg_log_loss_uma(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    _, loss_plain, _  = avg_log_loss(preds, train_data)\n",
    "    ids = []\n",
    "    attack_lens = train_data.get_group()\n",
    "    if attack_lens is not None:\n",
    "        offset=0\n",
    "        for atk in attack_lens:\n",
    "            ids += [offset]\n",
    "            offset += atk      \n",
    "            \n",
    "    ids = np.array(ids)\n",
    "    labels = train_data.get_label()\n",
    "    losses = np.log(1.0 + np.exp(-preds[ids]*labels[ids]))\n",
    "    loss_plain = np.mean(losses)\n",
    "\n",
    "    # combine the above two losses together\n",
    "    weighted_loss = alpha*loss_uma + (1.0-alpha)*loss_plain\n",
    "\n",
    "    return 'avg_non_interferent_log_loss [alpha={:.2f}]'.format(alpha), weighted_loss, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adv_boosting_data(model, data, groups):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    groups : grouping of same attacked instance \n",
    "    returns the new data matrix and new groups\n",
    "    \n",
    "    WARNING: currently works only for binary classification\n",
    "    '''\n",
    "    # score the datataset\n",
    "    labels = data[:,-1]\n",
    "    \n",
    "    predictions = model.predict(data[:,:-1]) # exclude labels\n",
    "    # binarize\n",
    "    predictions = (predictions>0).astype(np.float)\n",
    "    predictions = 2*predictions - 1\n",
    "    \n",
    "    # check mispredictions\n",
    "    matchings = labels * predictions\n",
    "    \n",
    "    # select original data + attacked instances\n",
    "    new_selected = [] # id of selected instances\n",
    "    new_groups   = []\n",
    "    \n",
    "    offset = 0\n",
    "    for g in groups:\n",
    "        if g==0:\n",
    "            print (\"Error !!!!\")\n",
    "        elif g==1:\n",
    "            # there are no attacks, just add original\n",
    "            new_selected += [offset]\n",
    "            new_groups   += [1]\n",
    "        else:\n",
    "            # get a slice of the matching scores\n",
    "            g_matchings = matchings[offset:offset+g]\n",
    "\n",
    "            # most misclassified (smallest margin)\n",
    "            # skip original\n",
    "            adv_instance = np.argmin(g_matchings[1:])+1\n",
    "\n",
    "            # add original and adversarial\n",
    "            new_selected += [offset, adv_instance]\n",
    "            new_groups   += [2]\n",
    "        \n",
    "        offset += g\n",
    "    \n",
    "    new_dataset = data[new_selected,:]\n",
    "    \n",
    "    return new_dataset, new_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_adv_boosting_model(train, valid, cat_fx, input_model=None, num_trees=1, params=None):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    returns the new model (is model modified inplace?)\n",
    "    '''\n",
    "    \n",
    "    if cat_fx is None or len(cat_fx)==0:\n",
    "        cat_fx = \"auto\"\n",
    "        \n",
    "    assert train.shape[1]==valid.shape[1], \"Train/Valid Mismatch!\"\n",
    "\n",
    "    lgbm_train = lightgbm.Dataset(data=train[:,:-1], \n",
    "                                  label=train[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_valid = lightgbm.Dataset(data=valid[:,:-1], \n",
    "                                  label=valid[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_info = {}\n",
    "    lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                num_boost_round = num_trees, \n",
    "                                init_model = input_model,\n",
    "                                fobj = optimize_log_loss, \n",
    "                                feval = avg_log_loss,\n",
    "                                evals_result = lgbm_info,\n",
    "                                valid_sets   = [lgbm_train, lgbm_valid], \n",
    "                                valid_names  = ['train', 'valid'],\n",
    "                                verbose_eval=5)\n",
    "\n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdvBoosting(atk_train, atk_valid, trees, \n",
    "                 cat_fx,\n",
    "                 params,\n",
    "                 output_model_file,\n",
    "                 partial_save=1000, \n",
    "                 adv_rounds=1):\n",
    "    ''' \n",
    "    atk_data: full dataset including all valid attacks\n",
    "    atk_groups: lenght of each attack set\n",
    "    trees: total number of trees to be produced\n",
    "    adv_rounds: adversarial instance injecting frequency\n",
    "    '''\n",
    "    # temp lgbm file\n",
    "    temp = output_model_file+\".tmp\"\n",
    "    \n",
    "    # get groups and remove instance ids\n",
    "    atk_groups = atk_train['instance_id'].value_counts().sort_index().values\n",
    "    atk_valid_groups = atk_valid['instance_id'].value_counts().sort_index().values\n",
    "    \n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(atk_train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])  \n",
    "    # print (\"CatFX:\", atk_train.columns.values[cat_fx])\n",
    "\n",
    "    # prepare data (avoiding pandas)\n",
    "    atk_data   = atk_train.iloc[:,1:].values\n",
    "    atk_valid  = atk_valid.iloc[:,1:].values\n",
    "    cat_fx = [x - 1 for x in cat_fx]\n",
    "\n",
    "    # train first trees\n",
    "    original_ids = np.cumsum(atk_groups[:-1])\n",
    "    original_ids = np.insert(original_ids, 0, 0)\n",
    "    \n",
    "    original_valid_ids = np.cumsum(atk_valid_groups[:-1])\n",
    "    original_valid_ids = np.insert(original_valid_ids, 0, 0)\n",
    "    \n",
    "    model, model_info = extend_adv_boosting_model(atk_data[original_ids, :], \n",
    "                                                  atk_valid[original_valid_ids, :],\n",
    "                                                  cat_fx=cat_fx,\n",
    "                                                  input_model=None, \n",
    "                                                  num_trees=adv_rounds, \n",
    "                                                  params=params)\n",
    "    \n",
    "    best_model = model\n",
    "    best_info = model_info\n",
    "    best_loss = np.min(model_info['valid']['avg_binary_log_loss'])\n",
    "    best_round = 1\n",
    "        \n",
    "    # train remaining trees\n",
    "    for t in range(adv_rounds+1, trees+1, adv_rounds):\n",
    "        # attack dataset\n",
    "        adv_data, _       = gen_adv_boosting_data(model, atk_data, atk_groups)\n",
    "        adv_valid_data, _ = gen_adv_boosting_data(model, atk_valid, atk_valid_groups)\n",
    "        \n",
    "        # train additional trees\n",
    "        model.save_model(temp)\n",
    "        model, model_info = extend_adv_boosting_model(adv_data, \n",
    "                                                      adv_valid_data,\n",
    "                                                      cat_fx=cat_fx,\n",
    "                                                      input_model=temp, \n",
    "                                                      num_trees=adv_rounds, \n",
    "                                                      params=params)\n",
    "\n",
    "        if np.min(model_info['valid']['avg_binary_log_loss']) < best_loss:\n",
    "            best_model = model\n",
    "            best_info  = model_info\n",
    "            best_loss  = np.min(model_info['valid']['avg_binary_log_loss'])\n",
    "            best_round = t\n",
    "        \n",
    "        # save partial model\n",
    "        if t % partial_save == 0 and t != trees:\n",
    "            partial_filename = \"{:s}_T{:d}-of-{:d}_S{:04d}_L{:d}.model.tmp\".format(output_model_file, \n",
    "                                                                                   t, \n",
    "                                                                                   trees, \n",
    "                                                                                   int(params['learning_rate'] * 1000),\n",
    "                                                                                   params['num_leaves']\n",
    "                                                                                  )\n",
    "            \n",
    "            print(\"Save partial model to {}\".format(partial_filename))\n",
    "            model.save_model(filename=partial_filename)\n",
    "            \n",
    "    \n",
    "    return model, model_info, best_loss, best_round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Standard GBDT (_baseline 1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gradient_boosting_baseline( train_file, valid_file, test_file,\n",
    "                                output_model_file):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'best_round', 'avg_binary_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    \n",
    "    assert \"instance_id\" not in train.columns.values, \"Wrong training set file for GBDT\"\n",
    "\n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])  \n",
    "    print (\"CatFX:\", train.columns.values[cat_fx])\n",
    "    \n",
    "\n",
    "    for num_trees in [500]:\n",
    "        best_model = None\n",
    "        best_info = None\n",
    "        best_loss = np.inf\n",
    "        for learning_rate in [0.1]: #[0.01, 0.05, 0.1]:\n",
    "            for num_leaves in [24]: #[8, 16, 24]:\n",
    "                # datasets\n",
    "                lgbm_train = lightgbm.Dataset(data=train.iloc[:,:-1].values, \n",
    "                                              label=train.iloc[:,-1].values,\n",
    "                                              categorical_feature = cat_fx)\n",
    "\n",
    "                lgbm_valid = lightgbm.Dataset(data=valid.iloc[:,:-1].values, \n",
    "                                              label=valid.iloc[:,-1].values,\n",
    "                                              categorical_feature = cat_fx)\n",
    "\n",
    "                # run train\n",
    "                lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                'num_leaves': num_leaves} \n",
    "                lgbm_info = {}\n",
    "                lgbm_model = lightgbm.train(lgbm_params, lgbm_train, \n",
    "                                            num_boost_round = num_trees,\n",
    "                                            fobj            = optimize_log_loss, \n",
    "                                            feval           = avg_log_loss,\n",
    "                                            evals_result    = lgbm_info,\n",
    "                                            valid_sets      = [lgbm_train, lgbm_valid], \n",
    "                                            valid_names     = ['train', 'valid'],\n",
    "                                            verbose_eval    = 5)\n",
    "                \n",
    "                if np.min(lgbm_info['valid']['avg_binary_log_loss']) < best_loss:\n",
    "                    best_model = lgbm_model\n",
    "                    best_info = lgbm_info\n",
    "                    best_loss = np.min(lgbm_info['valid']['avg_binary_log_loss'])\n",
    "                    best_info['num_trees'] = num_trees\n",
    "                    best_info['learning_rate'] = learning_rate\n",
    "                    best_info['num_leaves'] = num_leaves\n",
    "                    \n",
    "                    \n",
    "                best_valid_iter = np.argmin(lgbm_info['valid']['avg_binary_log_loss'])\n",
    "                \n",
    "                # update experimental results\n",
    "                exp = exp.append({'num_trees': num_trees, \n",
    "                                  'learning_rate':learning_rate,\n",
    "                                  'num_leaves':num_leaves, \n",
    "                                  'best_round':best_valid_iter+1, \n",
    "                                  'avg_binary_log_loss':lgbm_info['valid']['avg_binary_log_loss'][best_valid_iter]},\n",
    "                                 ignore_index=True)\n",
    "                \n",
    "        \n",
    "        # save file\n",
    "        best_valid_iter = np.argmin(best_info['valid']['avg_binary_log_loss'])\n",
    "\n",
    "        model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                        best_info['num_trees'],\n",
    "                                                                        int(best_info['learning_rate']*1000),\n",
    "                                                                        best_info['num_leaves'],\n",
    "                                                                        best_valid_iter + 1\n",
    "                                                                       )\n",
    "        \n",
    "        best_model.save_model(model_file_name)\n",
    "        print (\"Model saved to\", model_file_name)\n",
    "\n",
    "    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing original files...\n",
      "Loading: ../data/census/train_ori.csv.bz2\n",
      "Loading: ../data/census/valid_ori.csv.bz2\n",
      "Loading: ../data/census/test_ori.csv.bz2\n",
      "Train/Valid/Test sizes: (27145, 14) (3017, 14) (15060, 14)\n",
      "Train/Valid/Test split: 0.60 0.07 0.33\n",
      "CatFX: ['workclass', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
      "Train/Valid/Test sizes: (27133, 14) (9044, 14) (9045, 14)\n",
      "Train/Valid/Test split: 0.60 0.20 0.20\n",
      "Saving processed files *.cat.bz2\n",
      "CatFX: ['workclass' 'marital_status' 'occupation' 'relationship' 'race' 'sex'\n",
      " 'native_country']\n",
      "[5]\ttrain's avg_binary_log_loss: 0.516117\tvalid's avg_binary_log_loss: 0.517323\n",
      "[10]\ttrain's avg_binary_log_loss: 0.438493\tvalid's avg_binary_log_loss: 0.440024\n",
      "[15]\ttrain's avg_binary_log_loss: 0.40015\tvalid's avg_binary_log_loss: 0.401834\n",
      "[20]\ttrain's avg_binary_log_loss: 0.379741\tvalid's avg_binary_log_loss: 0.381521\n",
      "[25]\ttrain's avg_binary_log_loss: 0.367839\tvalid's avg_binary_log_loss: 0.370259\n",
      "[30]\ttrain's avg_binary_log_loss: 0.359868\tvalid's avg_binary_log_loss: 0.362934\n",
      "[35]\ttrain's avg_binary_log_loss: 0.354365\tvalid's avg_binary_log_loss: 0.357923\n",
      "[40]\ttrain's avg_binary_log_loss: 0.342736\tvalid's avg_binary_log_loss: 0.347383\n",
      "[45]\ttrain's avg_binary_log_loss: 0.334168\tvalid's avg_binary_log_loss: 0.340133\n",
      "[50]\ttrain's avg_binary_log_loss: 0.322578\tvalid's avg_binary_log_loss: 0.330098\n",
      "[55]\ttrain's avg_binary_log_loss: 0.314703\tvalid's avg_binary_log_loss: 0.32338\n",
      "[60]\ttrain's avg_binary_log_loss: 0.309001\tvalid's avg_binary_log_loss: 0.31893\n",
      "[65]\ttrain's avg_binary_log_loss: 0.306029\tvalid's avg_binary_log_loss: 0.316995\n",
      "[70]\ttrain's avg_binary_log_loss: 0.303376\tvalid's avg_binary_log_loss: 0.3151\n",
      "[75]\ttrain's avg_binary_log_loss: 0.300064\tvalid's avg_binary_log_loss: 0.312694\n",
      "[80]\ttrain's avg_binary_log_loss: 0.297271\tvalid's avg_binary_log_loss: 0.310638\n",
      "[85]\ttrain's avg_binary_log_loss: 0.294936\tvalid's avg_binary_log_loss: 0.308804\n",
      "[90]\ttrain's avg_binary_log_loss: 0.292655\tvalid's avg_binary_log_loss: 0.30741\n",
      "[95]\ttrain's avg_binary_log_loss: 0.290052\tvalid's avg_binary_log_loss: 0.305533\n",
      "[100]\ttrain's avg_binary_log_loss: 0.289211\tvalid's avg_binary_log_loss: 0.305254\n",
      "[105]\ttrain's avg_binary_log_loss: 0.28759\tvalid's avg_binary_log_loss: 0.304225\n",
      "[110]\ttrain's avg_binary_log_loss: 0.286925\tvalid's avg_binary_log_loss: 0.304185\n",
      "[115]\ttrain's avg_binary_log_loss: 0.285405\tvalid's avg_binary_log_loss: 0.303244\n",
      "[120]\ttrain's avg_binary_log_loss: 0.283969\tvalid's avg_binary_log_loss: 0.302574\n",
      "[125]\ttrain's avg_binary_log_loss: 0.283056\tvalid's avg_binary_log_loss: 0.302167\n",
      "[130]\ttrain's avg_binary_log_loss: 0.282143\tvalid's avg_binary_log_loss: 0.301896\n",
      "[135]\ttrain's avg_binary_log_loss: 0.281502\tvalid's avg_binary_log_loss: 0.301719\n",
      "[140]\ttrain's avg_binary_log_loss: 0.280675\tvalid's avg_binary_log_loss: 0.30142\n",
      "[145]\ttrain's avg_binary_log_loss: 0.279745\tvalid's avg_binary_log_loss: 0.301037\n",
      "[150]\ttrain's avg_binary_log_loss: 0.279109\tvalid's avg_binary_log_loss: 0.300976\n",
      "[155]\ttrain's avg_binary_log_loss: 0.278581\tvalid's avg_binary_log_loss: 0.301056\n",
      "[160]\ttrain's avg_binary_log_loss: 0.27777\tvalid's avg_binary_log_loss: 0.300893\n",
      "[165]\ttrain's avg_binary_log_loss: 0.277024\tvalid's avg_binary_log_loss: 0.300668\n",
      "[170]\ttrain's avg_binary_log_loss: 0.276285\tvalid's avg_binary_log_loss: 0.30062\n",
      "[175]\ttrain's avg_binary_log_loss: 0.275514\tvalid's avg_binary_log_loss: 0.30033\n",
      "[180]\ttrain's avg_binary_log_loss: 0.274742\tvalid's avg_binary_log_loss: 0.300021\n",
      "[185]\ttrain's avg_binary_log_loss: 0.274057\tvalid's avg_binary_log_loss: 0.299819\n",
      "[190]\ttrain's avg_binary_log_loss: 0.273522\tvalid's avg_binary_log_loss: 0.299804\n",
      "[195]\ttrain's avg_binary_log_loss: 0.272797\tvalid's avg_binary_log_loss: 0.29951\n",
      "[200]\ttrain's avg_binary_log_loss: 0.272344\tvalid's avg_binary_log_loss: 0.299526\n",
      "[205]\ttrain's avg_binary_log_loss: 0.271518\tvalid's avg_binary_log_loss: 0.299386\n",
      "[210]\ttrain's avg_binary_log_loss: 0.270807\tvalid's avg_binary_log_loss: 0.299128\n",
      "[215]\ttrain's avg_binary_log_loss: 0.270127\tvalid's avg_binary_log_loss: 0.298936\n",
      "[220]\ttrain's avg_binary_log_loss: 0.269509\tvalid's avg_binary_log_loss: 0.298865\n",
      "[225]\ttrain's avg_binary_log_loss: 0.269081\tvalid's avg_binary_log_loss: 0.298889\n",
      "[230]\ttrain's avg_binary_log_loss: 0.268497\tvalid's avg_binary_log_loss: 0.298697\n",
      "[235]\ttrain's avg_binary_log_loss: 0.26815\tvalid's avg_binary_log_loss: 0.298649\n",
      "[240]\ttrain's avg_binary_log_loss: 0.267782\tvalid's avg_binary_log_loss: 0.298754\n",
      "[245]\ttrain's avg_binary_log_loss: 0.267454\tvalid's avg_binary_log_loss: 0.298882\n",
      "[250]\ttrain's avg_binary_log_loss: 0.267094\tvalid's avg_binary_log_loss: 0.29886\n",
      "[255]\ttrain's avg_binary_log_loss: 0.266759\tvalid's avg_binary_log_loss: 0.299013\n",
      "[260]\ttrain's avg_binary_log_loss: 0.26635\tvalid's avg_binary_log_loss: 0.299114\n",
      "[265]\ttrain's avg_binary_log_loss: 0.266031\tvalid's avg_binary_log_loss: 0.299121\n",
      "[270]\ttrain's avg_binary_log_loss: 0.265399\tvalid's avg_binary_log_loss: 0.2991\n",
      "[275]\ttrain's avg_binary_log_loss: 0.264979\tvalid's avg_binary_log_loss: 0.299219\n",
      "[280]\ttrain's avg_binary_log_loss: 0.264435\tvalid's avg_binary_log_loss: 0.299171\n",
      "[285]\ttrain's avg_binary_log_loss: 0.264133\tvalid's avg_binary_log_loss: 0.29908\n",
      "[290]\ttrain's avg_binary_log_loss: 0.263544\tvalid's avg_binary_log_loss: 0.298855\n",
      "[295]\ttrain's avg_binary_log_loss: 0.263166\tvalid's avg_binary_log_loss: 0.298917\n",
      "[300]\ttrain's avg_binary_log_loss: 0.262837\tvalid's avg_binary_log_loss: 0.299024\n",
      "[305]\ttrain's avg_binary_log_loss: 0.262284\tvalid's avg_binary_log_loss: 0.298884\n",
      "[310]\ttrain's avg_binary_log_loss: 0.261919\tvalid's avg_binary_log_loss: 0.298917\n",
      "[315]\ttrain's avg_binary_log_loss: 0.261389\tvalid's avg_binary_log_loss: 0.298975\n",
      "[320]\ttrain's avg_binary_log_loss: 0.261005\tvalid's avg_binary_log_loss: 0.299057\n",
      "[325]\ttrain's avg_binary_log_loss: 0.260622\tvalid's avg_binary_log_loss: 0.299045\n",
      "[330]\ttrain's avg_binary_log_loss: 0.260235\tvalid's avg_binary_log_loss: 0.299141\n",
      "[335]\ttrain's avg_binary_log_loss: 0.259929\tvalid's avg_binary_log_loss: 0.299139\n",
      "[340]\ttrain's avg_binary_log_loss: 0.259431\tvalid's avg_binary_log_loss: 0.299093\n",
      "[345]\ttrain's avg_binary_log_loss: 0.259154\tvalid's avg_binary_log_loss: 0.299216\n",
      "[350]\ttrain's avg_binary_log_loss: 0.258877\tvalid's avg_binary_log_loss: 0.299266\n",
      "[355]\ttrain's avg_binary_log_loss: 0.258215\tvalid's avg_binary_log_loss: 0.299036\n",
      "[360]\ttrain's avg_binary_log_loss: 0.257831\tvalid's avg_binary_log_loss: 0.298982\n",
      "[365]\ttrain's avg_binary_log_loss: 0.257506\tvalid's avg_binary_log_loss: 0.299113\n",
      "[370]\ttrain's avg_binary_log_loss: 0.257179\tvalid's avg_binary_log_loss: 0.299145\n",
      "[375]\ttrain's avg_binary_log_loss: 0.256742\tvalid's avg_binary_log_loss: 0.299044\n",
      "[380]\ttrain's avg_binary_log_loss: 0.256415\tvalid's avg_binary_log_loss: 0.299049\n",
      "[385]\ttrain's avg_binary_log_loss: 0.256175\tvalid's avg_binary_log_loss: 0.299026\n",
      "[390]\ttrain's avg_binary_log_loss: 0.25588\tvalid's avg_binary_log_loss: 0.299095\n",
      "[395]\ttrain's avg_binary_log_loss: 0.255629\tvalid's avg_binary_log_loss: 0.299091\n",
      "[400]\ttrain's avg_binary_log_loss: 0.255017\tvalid's avg_binary_log_loss: 0.298998\n",
      "[405]\ttrain's avg_binary_log_loss: 0.254752\tvalid's avg_binary_log_loss: 0.299128\n",
      "[410]\ttrain's avg_binary_log_loss: 0.254512\tvalid's avg_binary_log_loss: 0.299208\n",
      "[415]\ttrain's avg_binary_log_loss: 0.254239\tvalid's avg_binary_log_loss: 0.299329\n",
      "[420]\ttrain's avg_binary_log_loss: 0.253947\tvalid's avg_binary_log_loss: 0.299451\n",
      "[425]\ttrain's avg_binary_log_loss: 0.253557\tvalid's avg_binary_log_loss: 0.299327\n",
      "[430]\ttrain's avg_binary_log_loss: 0.253097\tvalid's avg_binary_log_loss: 0.299362\n",
      "[435]\ttrain's avg_binary_log_loss: 0.252665\tvalid's avg_binary_log_loss: 0.29935\n",
      "[440]\ttrain's avg_binary_log_loss: 0.252408\tvalid's avg_binary_log_loss: 0.299381\n",
      "[445]\ttrain's avg_binary_log_loss: 0.252134\tvalid's avg_binary_log_loss: 0.299441\n",
      "[450]\ttrain's avg_binary_log_loss: 0.251914\tvalid's avg_binary_log_loss: 0.299636\n",
      "[455]\ttrain's avg_binary_log_loss: 0.25168\tvalid's avg_binary_log_loss: 0.299864\n",
      "[460]\ttrain's avg_binary_log_loss: 0.25146\tvalid's avg_binary_log_loss: 0.299919\n",
      "[465]\ttrain's avg_binary_log_loss: 0.251097\tvalid's avg_binary_log_loss: 0.299912\n",
      "[470]\ttrain's avg_binary_log_loss: 0.250836\tvalid's avg_binary_log_loss: 0.299972\n",
      "[475]\ttrain's avg_binary_log_loss: 0.250598\tvalid's avg_binary_log_loss: 0.30006\n",
      "[480]\ttrain's avg_binary_log_loss: 0.250407\tvalid's avg_binary_log_loss: 0.300114\n",
      "[485]\ttrain's avg_binary_log_loss: 0.250219\tvalid's avg_binary_log_loss: 0.300199\n",
      "[490]\ttrain's avg_binary_log_loss: 0.249859\tvalid's avg_binary_log_loss: 0.300145\n",
      "[495]\ttrain's avg_binary_log_loss: 0.249465\tvalid's avg_binary_log_loss: 0.30015\n",
      "[500]\ttrain's avg_binary_log_loss: 0.249079\tvalid's avg_binary_log_loss: 0.300114\n",
      "Model saved to ../out/models/std_gbdt_census_T500_S0100_L24_R234.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_trees  learning_rate  num_leaves  best_round  avg_binary_log_loss\n",
      "0      500.0            0.1        24.0       234.0             0.298616\n"
     ]
    }
   ],
   "source": [
    "# enable/disable LGBM Baseline\n",
    "if True:\n",
    "    experiments = train_gradient_boosting_baseline(\"../data/census/train_ori.csv.bz2\",\n",
    "                                                     \"../data/census/valid_ori.csv.bz2\",\n",
    "                                                     \"../data/census/test_ori.csv.bz2\",\n",
    "                                                     \"../out/models/std_gbdt_census\")  \n",
    "\n",
    "    experiments.to_csv('../out/models/std_gbdt_census.csv', index=False)\n",
    "\n",
    "    print (experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Model saved to ../out/models/std_gbdt_census_T500_S0100_L8_R493.model\n",
    "       num_trees  learning_rate  num_leaves  best_round  avg_binary_log_loss\n",
    "    0      500.0           0.01         8.0       500.0             0.358958\n",
    "    1      500.0           0.01        16.0       500.0             0.336156\n",
    "    2      500.0           0.01        24.0       500.0             0.329954\n",
    "    3      500.0           0.05         8.0       500.0             0.298972\n",
    "    4      500.0           0.05        16.0       498.0             0.296245\n",
    "    5      500.0           0.05        24.0       493.0             0.298961\n",
    "    6      500.0           0.10         8.0       493.0             0.292258\n",
    "    7      500.0           0.10        16.0       394.0             0.295525\n",
    "    8      500.0           0.10        24.0       234.0             0.298616\n",
    "    \n",
    "Best model\n",
    "\n",
    "    6      500.0           0.10         8.0       493.0             0.292258\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Adversarial Boosting (_baseline 2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial_boosting(train_file, valid_file, test_file, output_model_file):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'best_round', 'avg_binary_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    assert \"instance_id\" in train.columns.values, \"Wrong training set file for GBDT\"\n",
    "\n",
    "    for num_trees in [500]:\n",
    "        for learning_rate in [0.1]: #[0.01, 0.05, 0.1]:\n",
    "            for num_leaves in [24]: #[8, 16, 24]:\n",
    "                      \n",
    "                lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                'num_leaves': num_leaves} \n",
    "                \n",
    "                lgbm_model, lgbm_info, best_loss, best_valid_iter = AdvBoosting(train,\n",
    "                                                    valid,\n",
    "                                                    trees=num_trees, \n",
    "                                                    cat_fx = cat_fx, \n",
    "                                                    output_model_file=output_model_file, \n",
    "                                                    adv_rounds=1,\n",
    "                                                    params=lgbm_params)\n",
    "                \n",
    "\n",
    "                # update experimental results\n",
    "                exp = exp.append({'num_trees': num_trees, \n",
    "                                  'learning_rate':learning_rate,\n",
    "                                  'num_leaves':num_leaves, \n",
    "                                  'best_round':best_valid_iter, \n",
    "                                  'avg_binary_log_loss':best_loss},\n",
    "                                 ignore_index=True)\n",
    "        \n",
    "        # save file\n",
    "        model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                                num_trees,\n",
    "                                                                                int(learning_rate*1000),\n",
    "                                                                                num_leaves,\n",
    "                                                                                best_valid_iter\n",
    "                                                                               )\n",
    "        lgbm_model.save_model(model_file_name)\n",
    "        print (\"Model saved to\", model_file_name)\n",
    "                \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing original files...\n",
      "Loading: ../data/census/train_B150.csv.bz2\n",
      "Loading: ../data/census/valid_B150.csv.bz2\n",
      "Loading: ../data/census/test_B150.csv.bz2\n",
      "Train/Valid/Test sizes: (1261180, 15) (137558, 15) (701292, 15)\n",
      "Train/Valid/Test split: 0.60 0.07 0.33\n",
      "   ... with instance ids\n",
      "CatFX: ['workclass', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
      "Train/Valid/Test sizes: (1260493, 15) (417914, 15) (421623, 15)\n",
      "Train/Valid/Test split: 0.60 0.20 0.20\n",
      "Saving processed files *.cat.bz2\n",
      "[5]\ttrain's avg_binary_log_loss: 0.448326\tvalid's avg_binary_log_loss: 0.469327\n",
      "[10]\ttrain's avg_binary_log_loss: 0.337176\tvalid's avg_binary_log_loss: 0.35789\n",
      "[15]\ttrain's avg_binary_log_loss: 0.279679\tvalid's avg_binary_log_loss: 0.298205\n",
      "[20]\ttrain's avg_binary_log_loss: 0.24806\tvalid's avg_binary_log_loss: 0.272539\n",
      "[25]\ttrain's avg_binary_log_loss: 0.22964\tvalid's avg_binary_log_loss: 0.257995\n",
      "[30]\ttrain's avg_binary_log_loss: 0.218051\tvalid's avg_binary_log_loss: 0.248396\n",
      "[35]\ttrain's avg_binary_log_loss: 0.210964\tvalid's avg_binary_log_loss: 0.240704\n",
      "[40]\ttrain's avg_binary_log_loss: 0.20661\tvalid's avg_binary_log_loss: 0.239383\n",
      "[45]\ttrain's avg_binary_log_loss: 0.203848\tvalid's avg_binary_log_loss: 0.238644\n",
      "[50]\ttrain's avg_binary_log_loss: 0.19701\tvalid's avg_binary_log_loss: 0.23156\n",
      "[55]\ttrain's avg_binary_log_loss: 0.188106\tvalid's avg_binary_log_loss: 0.220986\n",
      "[60]\ttrain's avg_binary_log_loss: 0.181872\tvalid's avg_binary_log_loss: 0.213135\n",
      "[65]\ttrain's avg_binary_log_loss: 0.180014\tvalid's avg_binary_log_loss: 0.211042\n",
      "[70]\ttrain's avg_binary_log_loss: 0.177685\tvalid's avg_binary_log_loss: 0.207451\n",
      "[75]\ttrain's avg_binary_log_loss: 0.177154\tvalid's avg_binary_log_loss: 0.206715\n",
      "[80]\ttrain's avg_binary_log_loss: 0.17596\tvalid's avg_binary_log_loss: 0.205982\n",
      "[85]\ttrain's avg_binary_log_loss: 0.174972\tvalid's avg_binary_log_loss: 0.205143\n",
      "[90]\ttrain's avg_binary_log_loss: 0.174537\tvalid's avg_binary_log_loss: 0.204982\n",
      "[95]\ttrain's avg_binary_log_loss: 0.174137\tvalid's avg_binary_log_loss: 0.20382\n",
      "[100]\ttrain's avg_binary_log_loss: 0.173778\tvalid's avg_binary_log_loss: 0.204152\n",
      "[105]\ttrain's avg_binary_log_loss: 0.172849\tvalid's avg_binary_log_loss: 0.202179\n",
      "[110]\ttrain's avg_binary_log_loss: 0.171569\tvalid's avg_binary_log_loss: 0.200819\n",
      "[115]\ttrain's avg_binary_log_loss: 0.170826\tvalid's avg_binary_log_loss: 0.200445\n",
      "[120]\ttrain's avg_binary_log_loss: 0.170067\tvalid's avg_binary_log_loss: 0.199761\n",
      "[125]\ttrain's avg_binary_log_loss: 0.16973\tvalid's avg_binary_log_loss: 0.200403\n",
      "[130]\ttrain's avg_binary_log_loss: 0.1694\tvalid's avg_binary_log_loss: 0.199575\n",
      "[135]\ttrain's avg_binary_log_loss: 0.169065\tvalid's avg_binary_log_loss: 0.198969\n",
      "[140]\ttrain's avg_binary_log_loss: 0.16845\tvalid's avg_binary_log_loss: 0.198526\n",
      "[145]\ttrain's avg_binary_log_loss: 0.168145\tvalid's avg_binary_log_loss: 0.198091\n",
      "[150]\ttrain's avg_binary_log_loss: 0.167837\tvalid's avg_binary_log_loss: 0.197997\n",
      "[155]\ttrain's avg_binary_log_loss: 0.167536\tvalid's avg_binary_log_loss: 0.198149\n",
      "[160]\ttrain's avg_binary_log_loss: 0.167003\tvalid's avg_binary_log_loss: 0.197597\n",
      "[165]\ttrain's avg_binary_log_loss: 0.16637\tvalid's avg_binary_log_loss: 0.195796\n",
      "[170]\ttrain's avg_binary_log_loss: 0.166106\tvalid's avg_binary_log_loss: 0.196232\n",
      "[175]\ttrain's avg_binary_log_loss: 0.165843\tvalid's avg_binary_log_loss: 0.196723\n",
      "[180]\ttrain's avg_binary_log_loss: 0.165591\tvalid's avg_binary_log_loss: 0.197139\n",
      "[185]\ttrain's avg_binary_log_loss: 0.164985\tvalid's avg_binary_log_loss: 0.196724\n",
      "[190]\ttrain's avg_binary_log_loss: 0.164739\tvalid's avg_binary_log_loss: 0.196626\n",
      "[195]\ttrain's avg_binary_log_loss: 0.164487\tvalid's avg_binary_log_loss: 0.196647\n",
      "[200]\ttrain's avg_binary_log_loss: 0.164255\tvalid's avg_binary_log_loss: 0.196408\n",
      "[205]\ttrain's avg_binary_log_loss: 0.164041\tvalid's avg_binary_log_loss: 0.196837\n",
      "[210]\ttrain's avg_binary_log_loss: 0.1635\tvalid's avg_binary_log_loss: 0.196089\n",
      "[215]\ttrain's avg_binary_log_loss: 0.162993\tvalid's avg_binary_log_loss: 0.196558\n",
      "[220]\ttrain's avg_binary_log_loss: 0.162786\tvalid's avg_binary_log_loss: 0.196408\n",
      "[225]\ttrain's avg_binary_log_loss: 0.162575\tvalid's avg_binary_log_loss: 0.196303\n",
      "[230]\ttrain's avg_binary_log_loss: 0.162132\tvalid's avg_binary_log_loss: 0.195306\n",
      "[235]\ttrain's avg_binary_log_loss: 0.161885\tvalid's avg_binary_log_loss: 0.195708\n",
      "[240]\ttrain's avg_binary_log_loss: 0.161693\tvalid's avg_binary_log_loss: 0.195612\n",
      "[245]\ttrain's avg_binary_log_loss: 0.161469\tvalid's avg_binary_log_loss: 0.19531\n",
      "[250]\ttrain's avg_binary_log_loss: 0.161268\tvalid's avg_binary_log_loss: 0.194993\n",
      "[255]\ttrain's avg_binary_log_loss: 0.161052\tvalid's avg_binary_log_loss: 0.195118\n",
      "[260]\ttrain's avg_binary_log_loss: 0.160874\tvalid's avg_binary_log_loss: 0.195205\n",
      "[265]\ttrain's avg_binary_log_loss: 0.160414\tvalid's avg_binary_log_loss: 0.194358\n",
      "[270]\ttrain's avg_binary_log_loss: 0.160243\tvalid's avg_binary_log_loss: 0.194017\n",
      "[275]\ttrain's avg_binary_log_loss: 0.159813\tvalid's avg_binary_log_loss: 0.193015\n",
      "[280]\ttrain's avg_binary_log_loss: 0.159589\tvalid's avg_binary_log_loss: 0.193477\n",
      "[285]\ttrain's avg_binary_log_loss: 0.159194\tvalid's avg_binary_log_loss: 0.193184\n",
      "[290]\ttrain's avg_binary_log_loss: 0.159026\tvalid's avg_binary_log_loss: 0.192727\n",
      "[295]\ttrain's avg_binary_log_loss: 0.158842\tvalid's avg_binary_log_loss: 0.192664\n",
      "[300]\ttrain's avg_binary_log_loss: 0.158663\tvalid's avg_binary_log_loss: 0.19261\n",
      "[305]\ttrain's avg_binary_log_loss: 0.158276\tvalid's avg_binary_log_loss: 0.19188\n",
      "[310]\ttrain's avg_binary_log_loss: 0.158097\tvalid's avg_binary_log_loss: 0.192133\n",
      "[315]\ttrain's avg_binary_log_loss: 0.157942\tvalid's avg_binary_log_loss: 0.191971\n",
      "[320]\ttrain's avg_binary_log_loss: 0.157776\tvalid's avg_binary_log_loss: 0.19142\n",
      "[325]\ttrain's avg_binary_log_loss: 0.157623\tvalid's avg_binary_log_loss: 0.191999\n",
      "[330]\ttrain's avg_binary_log_loss: 0.157457\tvalid's avg_binary_log_loss: 0.191883\n",
      "[335]\ttrain's avg_binary_log_loss: 0.157267\tvalid's avg_binary_log_loss: 0.191985\n",
      "[340]\ttrain's avg_binary_log_loss: 0.15712\tvalid's avg_binary_log_loss: 0.192443\n",
      "[345]\ttrain's avg_binary_log_loss: 0.156958\tvalid's avg_binary_log_loss: 0.192543\n",
      "[350]\ttrain's avg_binary_log_loss: 0.156821\tvalid's avg_binary_log_loss: 0.192714\n",
      "[355]\ttrain's avg_binary_log_loss: 0.156458\tvalid's avg_binary_log_loss: 0.191678\n",
      "[360]\ttrain's avg_binary_log_loss: 0.156104\tvalid's avg_binary_log_loss: 0.1912\n",
      "[365]\ttrain's avg_binary_log_loss: 0.155779\tvalid's avg_binary_log_loss: 0.191075\n",
      "[370]\ttrain's avg_binary_log_loss: 0.15547\tvalid's avg_binary_log_loss: 0.190726\n",
      "[375]\ttrain's avg_binary_log_loss: 0.155325\tvalid's avg_binary_log_loss: 0.191151\n",
      "[380]\ttrain's avg_binary_log_loss: 0.155151\tvalid's avg_binary_log_loss: 0.190819\n",
      "[385]\ttrain's avg_binary_log_loss: 0.154999\tvalid's avg_binary_log_loss: 0.191409\n",
      "[390]\ttrain's avg_binary_log_loss: 0.154818\tvalid's avg_binary_log_loss: 0.191355\n",
      "[395]\ttrain's avg_binary_log_loss: 0.154667\tvalid's avg_binary_log_loss: 0.191475\n",
      "[400]\ttrain's avg_binary_log_loss: 0.154538\tvalid's avg_binary_log_loss: 0.191666\n",
      "[405]\ttrain's avg_binary_log_loss: 0.154386\tvalid's avg_binary_log_loss: 0.192426\n",
      "[410]\ttrain's avg_binary_log_loss: 0.154253\tvalid's avg_binary_log_loss: 0.192497\n",
      "[415]\ttrain's avg_binary_log_loss: 0.153904\tvalid's avg_binary_log_loss: 0.192278\n",
      "[420]\ttrain's avg_binary_log_loss: 0.1536\tvalid's avg_binary_log_loss: 0.19184\n",
      "[425]\ttrain's avg_binary_log_loss: 0.153441\tvalid's avg_binary_log_loss: 0.192042\n",
      "[430]\ttrain's avg_binary_log_loss: 0.153294\tvalid's avg_binary_log_loss: 0.192683\n",
      "[435]\ttrain's avg_binary_log_loss: 0.15301\tvalid's avg_binary_log_loss: 0.192206\n",
      "[440]\ttrain's avg_binary_log_loss: 0.152681\tvalid's avg_binary_log_loss: 0.191183\n",
      "[445]\ttrain's avg_binary_log_loss: 0.152536\tvalid's avg_binary_log_loss: 0.191531\n",
      "[450]\ttrain's avg_binary_log_loss: 0.15241\tvalid's avg_binary_log_loss: 0.191815\n",
      "[455]\ttrain's avg_binary_log_loss: 0.15214\tvalid's avg_binary_log_loss: 0.191429\n",
      "[460]\ttrain's avg_binary_log_loss: 0.151865\tvalid's avg_binary_log_loss: 0.190577\n",
      "[465]\ttrain's avg_binary_log_loss: 0.151724\tvalid's avg_binary_log_loss: 0.190147\n",
      "[470]\ttrain's avg_binary_log_loss: 0.1516\tvalid's avg_binary_log_loss: 0.189851\n",
      "[475]\ttrain's avg_binary_log_loss: 0.151459\tvalid's avg_binary_log_loss: 0.190309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[480]\ttrain's avg_binary_log_loss: 0.151218\tvalid's avg_binary_log_loss: 0.189771\n",
      "[485]\ttrain's avg_binary_log_loss: 0.151074\tvalid's avg_binary_log_loss: 0.19005\n",
      "[490]\ttrain's avg_binary_log_loss: 0.150962\tvalid's avg_binary_log_loss: 0.189923\n",
      "[495]\ttrain's avg_binary_log_loss: 0.150836\tvalid's avg_binary_log_loss: 0.190206\n",
      "[500]\ttrain's avg_binary_log_loss: 0.150725\tvalid's avg_binary_log_loss: 0.189817\n",
      "Model saved to ../out/models/adv_boosting_census_B150_T500_S0100_L24_R469.model\n",
      "   num_trees  learning_rate  num_leaves  best_round  avg_binary_log_loss\n",
      "0      500.0            0.1        24.0       469.0              0.18974\n"
     ]
    }
   ],
   "source": [
    "# enable/disable\n",
    "if True:\n",
    "    for B in [150]: #[5, 15, 150, 300]:\n",
    "\n",
    "        experiments = train_adversarial_boosting ( \"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/test_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../out/models/adv_boosting_census_B{:d}\".format(B))  \n",
    "\n",
    "        experiments.to_csv('../out/models/adv_boosting_census_B{:d}.csv'.format(B), index=False)\n",
    "\n",
    "        print (experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Non-Interferent GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_non_interferent(train_file, valid_file, test_file, output_model_file):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'alpha', 'best_round', 'avg_non_interferent_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    train_groups = train['instance_id'].value_counts().sort_index().values\n",
    "    valid_groups = valid['instance_id'].value_counts().sort_index().values\n",
    "    test_groups = test['instance_id'].value_counts().sort_index().values\n",
    "    \n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])\n",
    "    print (\"CatFX:\", train.columns.values[cat_fx])\n",
    "    \n",
    "    # prepare data\n",
    "    train = train.iloc[:,1:]\n",
    "    valid = valid.iloc[:,1:]\n",
    "    test = test.iloc[:,1:]\n",
    "    \n",
    "    cat_fx = [x - 1 for x in cat_fx]\n",
    "    \n",
    "\n",
    "    for num_trees in [50]:\n",
    "        for alpha in [0.2]: #[0.25, 0.50, 0.75, 1.00]:\n",
    "            best_model = None\n",
    "            best_info = None\n",
    "            best_loss = np.inf\n",
    "            awesome_hack = \"avg_non_interferent_log_loss\" + \" [alpha={:.2f}]\".format(alpha)\n",
    "            \n",
    "            for learning_rate in [0.1]: #[0.01, 0.05, 0.1]:\n",
    "                for num_leaves in [8]: #[8, 16, 24]:\n",
    "                    # datasets\n",
    "                    lgbm_train = lightgbm.Dataset(data=train.iloc[:,:-1].values,\n",
    "                                                  label=train.iloc[:,-1].values,\n",
    "                                                  group=train_groups,\n",
    "                                                  categorical_feature = cat_fx)\n",
    "\n",
    "                    lgbm_valid = lightgbm.Dataset(data=valid.iloc[:,:-1].values,\n",
    "                                                  label=valid.iloc[:,-1].values,\n",
    "                                                  group=valid_groups,\n",
    "                                                  #reference=lgbm_train, \n",
    "                                                  #free_raw_data=False,\n",
    "                                                  categorical_feature = cat_fx)\n",
    "\n",
    "                    # run train\n",
    "                    lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                    'num_leaves': num_leaves} \n",
    "                    lgbm_info = {}\n",
    "                    lgbm_model = lightgbm.train(lgbm_params, lgbm_train, \n",
    "                                                num_boost_round = num_trees,\n",
    "                                                fobj            = optimize_log_loss_uma_calza, # functools.partial(optimize_non_interferent_log_loss, alpha=alpha),\n",
    "                                                feval           = avg_log_loss_uma, #, functools.partial(avg_non_interferent_log_loss, alpha=alpha),\n",
    "                                                evals_result    = lgbm_info,\n",
    "                                                valid_sets      = [lgbm_train, lgbm_valid], \n",
    "                                                valid_names     = ['train', 'valid'],\n",
    "                                                verbose_eval    = 5)\n",
    "                    \n",
    "                    \n",
    "                    if np.min(lgbm_info['valid'][awesome_hack]) < best_loss:\n",
    "                        best_model = lgbm_model\n",
    "                        best_info = lgbm_info\n",
    "                        best_loss = np.min(lgbm_info['valid'][awesome_hack])\n",
    "                        best_info['num_trees'] = num_trees\n",
    "                        best_info['learning_rate'] = learning_rate\n",
    "                        best_info['num_leaves'] = num_leaves\n",
    "                \n",
    "\n",
    "                    # save file\n",
    "                    \n",
    "                    best_valid_iter = np.argmin(lgbm_info['valid'][awesome_hack])\n",
    "\n",
    "                    # update experimental results\n",
    "                    exp = exp.append({'num_trees': num_trees, \n",
    "                                      'learning_rate':learning_rate,\n",
    "                                      'num_leaves':num_leaves, \n",
    "                                      'alpha': alpha,\n",
    "                                      'best_round':best_valid_iter+1, \n",
    "                                      'avg_non_interferent_log_loss':lgbm_info['valid'][awesome_hack][best_valid_iter]},\n",
    "                                     ignore_index=True)\n",
    "            \n",
    "            best_valid_iter = np.argmin(best_info['valid'][awesome_hack])\n",
    "            \n",
    "            model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_A{:03d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                                    best_info['num_trees'],\n",
    "                                                                                    int(best_info['learning_rate']*1000),\n",
    "                                                                                    best_info['num_leaves'],\n",
    "                                                                                    int(alpha * 100),\n",
    "                                                                                    best_valid_iter + 1\n",
    "                                                                                   )\n",
    "            \n",
    "            \n",
    "            best_model.save_model(model_file_name)\n",
    "            print (\"Model saved to\", model_file_name)\n",
    "                    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing original files...\n",
      "Loading: ../data/census/train_B5.csv.bz2\n",
      "Loading: ../data/census/valid_B5.csv.bz2\n",
      "Loading: ../data/census/test_B5.csv.bz2\n",
      "Train/Valid/Test sizes: (206443, 15) (22601, 15) (114789, 15)\n",
      "Train/Valid/Test split: 0.60 0.07 0.33\n",
      "   ... with instance ids\n",
      "CatFX: ['workclass', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
      "Train/Valid/Test sizes: (206334, 15) (68498, 15) (69001, 15)\n",
      "Train/Valid/Test split: 0.60 0.20 0.20\n",
      "Saving processed files *.cat.bz2\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in method item of numpy.ndarray object at 0x7f2c907e6760> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;31mSystemError\u001b[0m: <built-in method item of numpy.ndarray object at 0x7f2c907e6760> returned a result with an error set",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-47460366ed92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                    \u001b[0;34m\"../data/census/valid_B{:d}.csv.bz2\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                    \u001b[0;34m\"../data/census/test_B{:d}.csv.bz2\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                                    \"../out/models/non_interferent_census_B{:d}\".format(B))  \n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mexperiments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../out/models/non_interferent_census_B{:d}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-140-f204a8620b0a>\u001b[0m in \u001b[0;36mtrain_non_interferent\u001b[0;34m(train_file, valid_file, test_file, output_model_file)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# load train/valid/test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_fx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_atk_train_valid_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'instance_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvalid_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'instance_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-109-c719ca8f50d4>\u001b[0m in \u001b[0;36mload_atk_train_valid_test\u001b[0;34m(atk_train_file, atk_valid_file, atk_test_file, train_split, valid_split, force)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtrain_cat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matk_train_file\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".cat.bz2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bz2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mvalid_cat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matk_valid_file\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".cat.bz2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bz2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mtest_cat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0matk_test_file\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".cat.bz2\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bz2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1743\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1745\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnicodeWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mwriter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    298\u001b[0m                                   \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                                   \u001b[0mdate_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                                   quoting=self.quoting)\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mto_native_types\u001b[0;34m(self, slicer, na_rep, quoting, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_object\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: <built-in method item of numpy.ndarray object at 0x7f2c907e6760> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "# enable/disable\n",
    "if True:\n",
    "    for B in [5]: #[5, 15, 150, 300]:\n",
    "\n",
    "        experiments = train_non_interferent(\"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/test_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../out/models/non_interferent_census_B{:d}\".format(B))  \n",
    "\n",
    "        experiments.to_csv('../out/models/non_interferent_census_B{:d}.csv'.format(B), index=False)\n",
    "\n",
    "        print (experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calza_loss(train, preds, alpha=0.5, groups=None):\n",
    "    \n",
    "    weights = np.full(len(train), 1.0 - alpha)\n",
    "    \n",
    "    if groups is None:\n",
    "        weights = np.full(len(train), 1.0)\n",
    "        \n",
    "    else:\n",
    "        offset = 0\n",
    "        for g in groups:\n",
    "            offset += g\n",
    "            if g == 2:\n",
    "                weights[offset-1] = alpha\n",
    "    \n",
    "    grad, hess = optimize_log_loss(train, preds)\n",
    "    \n",
    "    return weights * grad, weights * hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_calza_loss(preds, train_data, alpha=0.5, groups=None):\n",
    "    \n",
    "    weights = np.full(train_data.num_data(), 1.0 - alpha)\n",
    "    \n",
    "    if groups is None:\n",
    "        weights = np.full(train_data.num_data(), 1.0)\n",
    "    \n",
    "    else:\n",
    "        offset = 0\n",
    "        for g in groups:\n",
    "            offset += g\n",
    "            if g == 2:\n",
    "                weights[offset-1] = alpha\n",
    "    \n",
    "    labels = train_data.get_label()\n",
    "    losses = weights * np.log(1.0 + np.exp(-preds*labels))\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return 'avg_binary_log_loss', avg_loss, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_calza_model(train, train_groups, valid, valid_groups, cat_fx, input_model=None, num_trees=1, alpha=0.5, params=None):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    returns the new model (is model modified inplace?)\n",
    "    '''\n",
    "    \n",
    "    if cat_fx is None or len(cat_fx)==0:\n",
    "        cat_fx = \"auto\"\n",
    "        \n",
    "    assert train.shape[1]==valid.shape[1], \"Train/Valid Mismatch!\"\n",
    "\n",
    "    lgbm_train = lightgbm.Dataset(data=train[:,:-1], \n",
    "                                  label=train[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_valid = lightgbm.Dataset(data=valid[:,:-1], \n",
    "                                  label=valid[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_info = {}\n",
    "    lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                num_boost_round = num_trees, \n",
    "                                init_model = input_model,\n",
    "                                fobj = functools.partial(calza_loss, alpha=alpha, groups=train_groups), \n",
    "                                feval = functools.partial(avg_calza_loss, alpha=alpha, groups=valid_groups),\n",
    "                                evals_result = lgbm_info,\n",
    "                                valid_sets   = [lgbm_train, lgbm_valid], \n",
    "                                valid_names  = ['train', 'valid'],\n",
    "                                verbose_eval=5)\n",
    "\n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calza(atk_train, atk_valid, trees, \n",
    "                 cat_fx,\n",
    "                 params,\n",
    "                 output_model_file,\n",
    "                 partial_save=1000, \n",
    "                 adv_rounds=1,\n",
    "                 alpha=0.5):\n",
    "    ''' \n",
    "    atk_data: full dataset including all valid attacks\n",
    "    atk_groups: lenght of each attack set\n",
    "    trees: total number of trees to be produced\n",
    "    adv_rounds: adversarial instance injecting frequency\n",
    "    '''\n",
    "    # temp lgbm file\n",
    "    temp = output_model_file+\".tmp\"\n",
    "    \n",
    "    # get groups and remove instance ids\n",
    "    atk_groups = atk_train['instance_id'].value_counts().sort_index().values\n",
    "    atk_valid_groups = atk_valid['instance_id'].value_counts().sort_index().values\n",
    "    \n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(atk_train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])  \n",
    "    # print (\"CatFX:\", atk_train.columns.values[cat_fx])\n",
    "\n",
    "    # prepare data (avoiding pandas)\n",
    "    atk_data   = atk_train.iloc[:,1:].values\n",
    "    atk_valid  = atk_valid.iloc[:,1:].values\n",
    "    cat_fx = [x - 1 for x in cat_fx]\n",
    "\n",
    "    # train first trees\n",
    "    original_ids = np.cumsum(atk_groups[:-1])\n",
    "    original_ids = np.insert(original_ids, 0, 0)\n",
    "    \n",
    "    original_valid_ids = np.cumsum(atk_valid_groups[:-1])\n",
    "    original_valid_ids = np.insert(original_valid_ids, 0, 0)\n",
    "       \n",
    "    model, model_info = extend_calza_model(atk_data[original_ids, :], None, \n",
    "                                                  atk_valid[original_valid_ids, :], None,\n",
    "                                                  cat_fx=cat_fx,\n",
    "                                                  input_model=None, \n",
    "                                                  num_trees=adv_rounds, \n",
    "                                                  alpha=alpha,\n",
    "                                                  params=params)\n",
    "    \n",
    "    best_model = model\n",
    "    best_info = model_info\n",
    "    best_loss = np.min(model_info['valid']['avg_binary_log_loss'])\n",
    "    best_round = 1\n",
    "        \n",
    "    # train remaining trees\n",
    "    for t in range(adv_rounds+1, trees+1, adv_rounds):\n",
    "        # attack dataset\n",
    "        adv_data, adv_data_groups       = gen_adv_boosting_data(model, atk_data, atk_groups)\n",
    "        adv_valid_data, adv_valid_groups = gen_adv_boosting_data(model, atk_valid, atk_valid_groups)\n",
    "        \n",
    "        # train additional trees\n",
    "        model.save_model(temp)\n",
    "        model, model_info = extend_calza_model(adv_data, adv_data_groups,\n",
    "                                                      adv_valid_data, adv_valid_groups,\n",
    "                                                      cat_fx=cat_fx,\n",
    "                                                      input_model=temp, \n",
    "                                                      num_trees=adv_rounds, \n",
    "                                                      alpha=alpha,\n",
    "                                                      params=params)\n",
    "\n",
    "        if np.min(model_info['valid']['avg_binary_log_loss']) < best_loss:\n",
    "            best_model = model\n",
    "            best_info  = model_info\n",
    "            best_loss  = np.min(model_info['valid']['avg_binary_log_loss'])\n",
    "            best_round = t\n",
    "        \n",
    "        # save partial model\n",
    "        if t % partial_save == 0 and t != trees:\n",
    "            partial_filename = \"{:s}_T{:d}-of-{:d}_S{:04d}_L{:d}.model.tmp\".format(output_model_file, \n",
    "                                                                                   t, \n",
    "                                                                                   trees, \n",
    "                                                                                   int(params['learning_rate'] * 1000),\n",
    "                                                                                   params['num_leaves']\n",
    "                                                                                  )\n",
    "            \n",
    "            print(\"Save partial model to {}\".format(partial_filename))\n",
    "            model.save_model(filename=partial_filename)\n",
    "            \n",
    "    \n",
    "    return model, model_info, best_loss, best_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_calza(train_file, valid_file, test_file, output_model_file):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'best_round', 'avg_binary_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    assert \"instance_id\" in train.columns.values, \"Wrong training set file for GBDT\"\n",
    "    \n",
    "    alpha = 0.1\n",
    "\n",
    "    for num_trees in [500]:\n",
    "        for learning_rate in [0.1]: #[0.01, 0.05, 0.1]:\n",
    "            for num_leaves in [24]: #[8, 16, 24]:\n",
    "                      \n",
    "                lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                'num_leaves': num_leaves} \n",
    "                \n",
    "                lgbm_model, lgbm_info, best_loss, best_valid_iter = Calza(train,\n",
    "                                                    valid,\n",
    "                                                    trees=num_trees, \n",
    "                                                    cat_fx = cat_fx, \n",
    "                                                    output_model_file=output_model_file, \n",
    "                                                    adv_rounds=1,\n",
    "                                                    alpha=alpha,                      \n",
    "                                                    params=lgbm_params)\n",
    "                \n",
    "\n",
    "                # update experimental results\n",
    "                exp = exp.append({'num_trees': num_trees, \n",
    "                                  'learning_rate':learning_rate,\n",
    "                                  'num_leaves':num_leaves, \n",
    "                                  'best_round':best_valid_iter, \n",
    "                                  'avg_binary_log_loss':best_loss},\n",
    "                                 ignore_index=True)\n",
    "        \n",
    "        # save file\n",
    "        model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_A{:03d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                                num_trees,\n",
    "                                                                                int(learning_rate*1000),\n",
    "                                                                                num_leaves,\n",
    "                                                                                int(alpha*100),\n",
    "                                                                                best_valid_iter\n",
    "                                                                               )\n",
    "        lgbm_model.save_model(model_file_name)\n",
    "        print (\"Model saved to\", model_file_name)\n",
    "                \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing original files...\n",
      "Loading: ../data/census/train_B150.csv.bz2\n",
      "Loading: ../data/census/valid_B150.csv.bz2\n",
      "Loading: ../data/census/test_B150.csv.bz2\n",
      "Train/Valid/Test sizes: (1261180, 15) (137558, 15) (701292, 15)\n",
      "Train/Valid/Test split: 0.60 0.07 0.33\n",
      "   ... with instance ids\n",
      "CatFX: ['workclass', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
      "Train/Valid/Test sizes: (1260493, 15) (417914, 15) (421623, 15)\n",
      "Train/Valid/Test split: 0.60 0.20 0.20\n",
      "Saving processed files *.cat.bz2\n",
      "[5]\ttrain's avg_binary_log_loss: 0.352595\tvalid's avg_binary_log_loss: 0.28337\n",
      "[10]\ttrain's avg_binary_log_loss: 0.265723\tvalid's avg_binary_log_loss: 0.236901\n",
      "[15]\ttrain's avg_binary_log_loss: 0.220969\tvalid's avg_binary_log_loss: 0.214101\n",
      "[20]\ttrain's avg_binary_log_loss: 0.196331\tvalid's avg_binary_log_loss: 0.202158\n",
      "[25]\ttrain's avg_binary_log_loss: 0.181724\tvalid's avg_binary_log_loss: 0.196211\n",
      "[30]\ttrain's avg_binary_log_loss: 0.173047\tvalid's avg_binary_log_loss: 0.192582\n",
      "[35]\ttrain's avg_binary_log_loss: 0.167672\tvalid's avg_binary_log_loss: 0.190336\n",
      "[40]\ttrain's avg_binary_log_loss: 0.164306\tvalid's avg_binary_log_loss: 0.18889\n",
      "[45]\ttrain's avg_binary_log_loss: 0.158797\tvalid's avg_binary_log_loss: 0.184338\n",
      "[50]\ttrain's avg_binary_log_loss: 0.152657\tvalid's avg_binary_log_loss: 0.178347\n",
      "[55]\ttrain's avg_binary_log_loss: 0.14878\tvalid's avg_binary_log_loss: 0.174598\n",
      "[60]\ttrain's avg_binary_log_loss: 0.146891\tvalid's avg_binary_log_loss: 0.173025\n",
      "[65]\ttrain's avg_binary_log_loss: 0.146331\tvalid's avg_binary_log_loss: 0.17282\n",
      "[70]\ttrain's avg_binary_log_loss: 0.144824\tvalid's avg_binary_log_loss: 0.171532\n",
      "[75]\ttrain's avg_binary_log_loss: 0.143306\tvalid's avg_binary_log_loss: 0.170458\n",
      "[80]\ttrain's avg_binary_log_loss: 0.14204\tvalid's avg_binary_log_loss: 0.169432\n",
      "[85]\ttrain's avg_binary_log_loss: 0.141626\tvalid's avg_binary_log_loss: 0.169421\n",
      "[90]\ttrain's avg_binary_log_loss: 0.140626\tvalid's avg_binary_log_loss: 0.168522\n",
      "[95]\ttrain's avg_binary_log_loss: 0.14026\tvalid's avg_binary_log_loss: 0.168466\n",
      "[100]\ttrain's avg_binary_log_loss: 0.139385\tvalid's avg_binary_log_loss: 0.167711\n",
      "[105]\ttrain's avg_binary_log_loss: 0.138588\tvalid's avg_binary_log_loss: 0.167149\n",
      "[110]\ttrain's avg_binary_log_loss: 0.137866\tvalid's avg_binary_log_loss: 0.166459\n",
      "[115]\ttrain's avg_binary_log_loss: 0.137191\tvalid's avg_binary_log_loss: 0.165816\n",
      "[120]\ttrain's avg_binary_log_loss: 0.136897\tvalid's avg_binary_log_loss: 0.165765\n",
      "[125]\ttrain's avg_binary_log_loss: 0.136618\tvalid's avg_binary_log_loss: 0.165737\n",
      "[130]\ttrain's avg_binary_log_loss: 0.135994\tvalid's avg_binary_log_loss: 0.165091\n",
      "[135]\ttrain's avg_binary_log_loss: 0.135743\tvalid's avg_binary_log_loss: 0.165055\n",
      "[140]\ttrain's avg_binary_log_loss: 0.135481\tvalid's avg_binary_log_loss: 0.164963\n",
      "[145]\ttrain's avg_binary_log_loss: 0.135225\tvalid's avg_binary_log_loss: 0.164916\n",
      "[150]\ttrain's avg_binary_log_loss: 0.134609\tvalid's avg_binary_log_loss: 0.164449\n",
      "[155]\ttrain's avg_binary_log_loss: 0.134367\tvalid's avg_binary_log_loss: 0.164432\n",
      "[160]\ttrain's avg_binary_log_loss: 0.133858\tvalid's avg_binary_log_loss: 0.163913\n",
      "[165]\ttrain's avg_binary_log_loss: 0.133646\tvalid's avg_binary_log_loss: 0.163824\n",
      "[170]\ttrain's avg_binary_log_loss: 0.133115\tvalid's avg_binary_log_loss: 0.163393\n",
      "[175]\ttrain's avg_binary_log_loss: 0.132909\tvalid's avg_binary_log_loss: 0.163376\n",
      "[180]\ttrain's avg_binary_log_loss: 0.132438\tvalid's avg_binary_log_loss: 0.163089\n",
      "[185]\ttrain's avg_binary_log_loss: 0.132234\tvalid's avg_binary_log_loss: 0.163092\n",
      "[190]\ttrain's avg_binary_log_loss: 0.13207\tvalid's avg_binary_log_loss: 0.163087\n",
      "[195]\ttrain's avg_binary_log_loss: 0.131855\tvalid's avg_binary_log_loss: 0.163129\n",
      "[200]\ttrain's avg_binary_log_loss: 0.131677\tvalid's avg_binary_log_loss: 0.163137\n",
      "[205]\ttrain's avg_binary_log_loss: 0.131495\tvalid's avg_binary_log_loss: 0.163117\n",
      "[210]\ttrain's avg_binary_log_loss: 0.13134\tvalid's avg_binary_log_loss: 0.163109\n",
      "[215]\ttrain's avg_binary_log_loss: 0.131163\tvalid's avg_binary_log_loss: 0.163112\n",
      "[220]\ttrain's avg_binary_log_loss: 0.13071\tvalid's avg_binary_log_loss: 0.162769\n",
      "[225]\ttrain's avg_binary_log_loss: 0.130518\tvalid's avg_binary_log_loss: 0.162778\n",
      "[230]\ttrain's avg_binary_log_loss: 0.130358\tvalid's avg_binary_log_loss: 0.162806\n",
      "[235]\ttrain's avg_binary_log_loss: 0.130197\tvalid's avg_binary_log_loss: 0.162804\n",
      "[240]\ttrain's avg_binary_log_loss: 0.130043\tvalid's avg_binary_log_loss: 0.162821\n",
      "[245]\ttrain's avg_binary_log_loss: 0.129702\tvalid's avg_binary_log_loss: 0.16267\n",
      "[250]\ttrain's avg_binary_log_loss: 0.129528\tvalid's avg_binary_log_loss: 0.162703\n",
      "[255]\ttrain's avg_binary_log_loss: 0.129371\tvalid's avg_binary_log_loss: 0.162793\n",
      "[260]\ttrain's avg_binary_log_loss: 0.129233\tvalid's avg_binary_log_loss: 0.162745\n",
      "[265]\ttrain's avg_binary_log_loss: 0.128848\tvalid's avg_binary_log_loss: 0.16239\n",
      "[270]\ttrain's avg_binary_log_loss: 0.128708\tvalid's avg_binary_log_loss: 0.162422\n",
      "[275]\ttrain's avg_binary_log_loss: 0.128567\tvalid's avg_binary_log_loss: 0.162475\n",
      "[280]\ttrain's avg_binary_log_loss: 0.128409\tvalid's avg_binary_log_loss: 0.162551\n",
      "[285]\ttrain's avg_binary_log_loss: 0.128051\tvalid's avg_binary_log_loss: 0.162263\n",
      "[290]\ttrain's avg_binary_log_loss: 0.127913\tvalid's avg_binary_log_loss: 0.16226\n",
      "[295]\ttrain's avg_binary_log_loss: 0.127752\tvalid's avg_binary_log_loss: 0.162277\n",
      "[300]\ttrain's avg_binary_log_loss: 0.127612\tvalid's avg_binary_log_loss: 0.162252\n",
      "[305]\ttrain's avg_binary_log_loss: 0.127475\tvalid's avg_binary_log_loss: 0.162288\n",
      "[310]\ttrain's avg_binary_log_loss: 0.12732\tvalid's avg_binary_log_loss: 0.162307\n",
      "[315]\ttrain's avg_binary_log_loss: 0.127197\tvalid's avg_binary_log_loss: 0.162258\n",
      "[320]\ttrain's avg_binary_log_loss: 0.127076\tvalid's avg_binary_log_loss: 0.16228\n",
      "[325]\ttrain's avg_binary_log_loss: 0.126727\tvalid's avg_binary_log_loss: 0.16213\n",
      "[330]\ttrain's avg_binary_log_loss: 0.126606\tvalid's avg_binary_log_loss: 0.162107\n",
      "[335]\ttrain's avg_binary_log_loss: 0.126498\tvalid's avg_binary_log_loss: 0.16214\n",
      "[340]\ttrain's avg_binary_log_loss: 0.126376\tvalid's avg_binary_log_loss: 0.162204\n",
      "[345]\ttrain's avg_binary_log_loss: 0.126271\tvalid's avg_binary_log_loss: 0.162231\n",
      "[350]\ttrain's avg_binary_log_loss: 0.126161\tvalid's avg_binary_log_loss: 0.162274\n",
      "[355]\ttrain's avg_binary_log_loss: 0.12588\tvalid's avg_binary_log_loss: 0.162087\n",
      "[360]\ttrain's avg_binary_log_loss: 0.125576\tvalid's avg_binary_log_loss: 0.16184\n",
      "[365]\ttrain's avg_binary_log_loss: 0.125459\tvalid's avg_binary_log_loss: 0.161816\n",
      "[370]\ttrain's avg_binary_log_loss: 0.125339\tvalid's avg_binary_log_loss: 0.161747\n",
      "[375]\ttrain's avg_binary_log_loss: 0.125215\tvalid's avg_binary_log_loss: 0.161749\n",
      "[380]\ttrain's avg_binary_log_loss: 0.124933\tvalid's avg_binary_log_loss: 0.161636\n",
      "[385]\ttrain's avg_binary_log_loss: 0.124682\tvalid's avg_binary_log_loss: 0.161547\n",
      "[390]\ttrain's avg_binary_log_loss: 0.124561\tvalid's avg_binary_log_loss: 0.161548\n",
      "[395]\ttrain's avg_binary_log_loss: 0.124282\tvalid's avg_binary_log_loss: 0.161463\n",
      "[400]\ttrain's avg_binary_log_loss: 0.124182\tvalid's avg_binary_log_loss: 0.161422\n",
      "[405]\ttrain's avg_binary_log_loss: 0.124067\tvalid's avg_binary_log_loss: 0.161472\n",
      "[410]\ttrain's avg_binary_log_loss: 0.123956\tvalid's avg_binary_log_loss: 0.161514\n",
      "[415]\ttrain's avg_binary_log_loss: 0.123832\tvalid's avg_binary_log_loss: 0.161582\n",
      "[420]\ttrain's avg_binary_log_loss: 0.123706\tvalid's avg_binary_log_loss: 0.161553\n",
      "[425]\ttrain's avg_binary_log_loss: 0.123601\tvalid's avg_binary_log_loss: 0.161512\n",
      "[430]\ttrain's avg_binary_log_loss: 0.123338\tvalid's avg_binary_log_loss: 0.161421\n",
      "[435]\ttrain's avg_binary_log_loss: 0.123091\tvalid's avg_binary_log_loss: 0.161297\n",
      "[440]\ttrain's avg_binary_log_loss: 0.122973\tvalid's avg_binary_log_loss: 0.161256\n",
      "[445]\ttrain's avg_binary_log_loss: 0.122872\tvalid's avg_binary_log_loss: 0.161313\n",
      "[450]\ttrain's avg_binary_log_loss: 0.122614\tvalid's avg_binary_log_loss: 0.161133\n",
      "[455]\ttrain's avg_binary_log_loss: 0.122508\tvalid's avg_binary_log_loss: 0.161233\n",
      "[460]\ttrain's avg_binary_log_loss: 0.122411\tvalid's avg_binary_log_loss: 0.16126\n",
      "[465]\ttrain's avg_binary_log_loss: 0.122311\tvalid's avg_binary_log_loss: 0.161262\n",
      "[470]\ttrain's avg_binary_log_loss: 0.122204\tvalid's avg_binary_log_loss: 0.16124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[475]\ttrain's avg_binary_log_loss: 0.122107\tvalid's avg_binary_log_loss: 0.161301\n",
      "[480]\ttrain's avg_binary_log_loss: 0.121994\tvalid's avg_binary_log_loss: 0.161306\n",
      "[485]\ttrain's avg_binary_log_loss: 0.121895\tvalid's avg_binary_log_loss: 0.161288\n",
      "[490]\ttrain's avg_binary_log_loss: 0.121783\tvalid's avg_binary_log_loss: 0.161305\n",
      "[495]\ttrain's avg_binary_log_loss: 0.121693\tvalid's avg_binary_log_loss: 0.161376\n",
      "[500]\ttrain's avg_binary_log_loss: 0.12159\tvalid's avg_binary_log_loss: 0.161416\n",
      "Model saved to ../out/models/calza_census_B150_T500_S0100_L24_A010_R451.model\n",
      "   num_trees  learning_rate  num_leaves  best_round  avg_binary_log_loss\n",
      "0      500.0            0.1        24.0       451.0             0.161131\n"
     ]
    }
   ],
   "source": [
    "# enable/disable\n",
    "if True:\n",
    "    for B in [150]: #[5, 15, 150, 300]:\n",
    "\n",
    "        experiments = train_calza(\"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/test_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../out/models/calza_census_B{:d}\".format(B))  \n",
    "\n",
    "        experiments.to_csv('../out/models/calza_census_B{:d}.csv'.format(B), index=False)\n",
    "\n",
    "        print (experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check groups are the same after re-splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def check_groups(atk_train_file, atk_valid_file, atk_test_file):\n",
    "    # Load post-resplitting data\n",
    "    post_train, post_valid, post_test, cat_fx = load_atk_train_valid_test(atk_train_file, atk_valid_file, atk_test_file)\n",
    "    post_train_groups = post_train['instance_id'].value_counts().sort_index().values\n",
    "    post_valid_groups = post_valid['instance_id'].value_counts().sort_index().values\n",
    "    post_test_groups  = post_test['instance_id'].value_counts().sort_index().values\n",
    "    \n",
    "    # load pre-re-splitting data\n",
    "    pre_train = pd.read_csv(atk_train_file)\n",
    "    pre_valid = pd.read_csv(atk_valid_file)\n",
    "    pre_test  = pd.read_csv(atk_test_file)\n",
    "    pre_train_groups = pre_train['instance_id'].value_counts().sort_index().values\n",
    "    pre_valid_groups = pre_valid['instance_id'].value_counts().sort_index().values\n",
    "    pre_test_groups  = pre_test['instance_id'].value_counts().sort_index().values\n",
    "\n",
    "    # check global lenght\n",
    "    print (\"PRE  TOTAL instances:\", len(pre_train)+ len(pre_valid)+ len(pre_test))\n",
    "    print (\"POST TOTAL instances:\", len(post_train)+ len(post_valid)+ len(post_test))\n",
    "    \n",
    "    assert len(pre_train)+ len(pre_valid)+ len(pre_test) == \\\n",
    "            len(post_train)+ len(post_valid)+ len(post_test),\\\n",
    "            \"Different number of instances !\"\n",
    "            \n",
    "    # check groups\n",
    "    print (\"PRE  lengths in groups:\", len(pre_train_groups), len(pre_valid_groups), len(pre_test_groups))\n",
    "    print (\"POST lengths in groups:\", len(post_train_groups), len(post_valid_groups), len(post_test_groups))\n",
    "    pre_all_groups  = np.concatenate([pre_train_groups, pre_valid_groups, pre_test_groups])\n",
    "    post_all_groups = np.concatenate([post_train_groups, post_valid_groups, post_test_groups])\n",
    "    print (\"PRE  TOTAL groups:\", len(pre_all_groups))\n",
    "    print (\"POST TOTAL groups:\", len(post_all_groups))\n",
    "    \n",
    "    assert len(pre_all_groups)==len(post_all_groups),\\\n",
    "        \"Different number of groups!\"\n",
    "    \n",
    "    # compare values\n",
    "    assert (pre_all_groups==post_all_groups).all(),\\\n",
    "        \"Groups have different sizes!\"\n",
    "\n",
    "B=5\n",
    "check_groups(\"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                   \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                   \"../data/census/test_B{:d}.csv.bz2\".format(B)) \n",
    "B=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check extraction of non-attacked instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nonatk_ids(atk_data):\n",
    "    # get groups and remove instance ids\n",
    "    atk_groups = atk_data['instance_id'].value_counts().sort_index().values\n",
    "\n",
    "    original_ids = np.cumsum(atk_groups[:-1])\n",
    "    original_ids = np.insert(original_ids, 0, 0)\n",
    "\n",
    "    return original_ids\n",
    "    \n",
    "def check_nonatk_filter(atk_train_file, atk_valid_file, atk_test_file,\n",
    "                        train_file, valid_file, test_file):\n",
    "    # Load post-resplitting data\n",
    "    atk_train, atk_valid, atk_test, cat_fx = load_atk_train_valid_test(atk_train_file, atk_valid_file, atk_test_file)\n",
    "    \n",
    "    # Load non attacked instances\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "\n",
    "    # Filter atks and skip instance id\n",
    "    filtered_train = atk_train.iloc[get_nonatk_ids(atk_train),1:]\n",
    "    filtered_valid = atk_valid.iloc[get_nonatk_ids(atk_valid),1:]\n",
    "    filtered_test  = atk_test.iloc[get_nonatk_ids(atk_test),1:]\n",
    "\n",
    "    print (\"Attacked shapes\", atk_train.shape, atk_valid.shape, atk_test.shape)\n",
    "    print (\"Filteres shapes\", filtered_train.shape, filtered_valid.shape, filtered_test.shape)\n",
    "    print (\"Original shapes\", train.shape, valid.shape, test.shape)\n",
    "    \n",
    "    assert np.array_equal(train.values,filtered_train.values), \"Different Data !\"\n",
    "    assert np.array_equal(valid.values,filtered_valid.values), \"Different Data !\"\n",
    "    assert np.array_equal(test.values,filtered_test.values), \"Different Data !\"\n",
    "\n",
    "\n",
    "    \n",
    "B=5\n",
    "check_nonatk_filter(\"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                       \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                       \"../data/census/test_B{:d}.csv.bz2\".format(B),\n",
    "                   \"../data/census/train_ori.csv.bz2\".format(B),\n",
    "                       \"../data/census/valid_ori.csv.bz2\".format(B),\n",
    "                       \"../data/census/test_ori.csv.bz2\".format(B) ) \n",
    "B=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(train_file, valid_file, test_file, output_model_file):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['C', 'avg_binary_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    X_train = train.iloc[:,:-1].values\n",
    "    y_train = train.iloc[:,-1].values\n",
    "    y_train[y_train == -1] = 0\n",
    "    \n",
    "    X_valid = valid.iloc[:,:-1].values\n",
    "    y_valid = valid.iloc[:,-1].values\n",
    "    \n",
    "    for c in [0.001, 0.01, 0.1, 1.0]:\n",
    "        \n",
    "        model = SVC(kernel='rbf', probability=True)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_preds = model.predict_proba(X_valid)[:,0]\n",
    "        cur_avg_binary_log_loss = np.mean(binary_log_loss(y_preds, y_valid))\n",
    "        \n",
    "        model_file_name = \"{:s}_C{:04d}.model\".format(output_model_file, int(c * 1000))\n",
    "        \n",
    "        with open(model_file_name, 'wb') as fout:\n",
    "            pickle.dump(model, fout)\n",
    "        \n",
    "        print (\"Model saved to\", model_file_name)\n",
    "        \n",
    "        # update experimental results\n",
    "        exp = exp.append({'C': c, \n",
    "                          'avg_binary_log_loss':cur_avg_binary_log_loss},\n",
    "                         ignore_index=True)\n",
    "    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable/disable LGBM Baseline\n",
    "if True:\n",
    "    experiments = train_svm ( \"../data/census/train_ori.csv.bz2\",\n",
    "                                                     \"../data/census/valid_ori.csv.bz2\",\n",
    "                                                     \"../data/census/test_ori.csv.bz2\",\n",
    "                                                     \"../out/models/svm_census\")  \n",
    "\n",
    "    experiments.to_csv('../out/models/svm_census.csv', index=False)\n",
    "\n",
    "    print (experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
