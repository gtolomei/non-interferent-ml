{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIF\n",
    "## our proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    " - http://lightgbm.readthedocs.io/en/latest/\n",
    " - http://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
    " - https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "import pickle\n",
    "import json\n",
    "import functools\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "from NILib import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Non-Interferent GBDT\n",
    "\n",
    "Sort of parameters:\n",
    " - `get_ni_w` functions used to define the weights of the instances at training time. Those weights implement that max loss per groups coefficient\n",
    " - `fobj` objective parameter corresponding to the objective function to be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_non_interf_model(atk_train, atk_valid, cat_fx, \n",
    "                            alpha=1.0, num_trees=1, params=None):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    returns the new model (is model modified inplace?)\n",
    "    '''\n",
    "    \n",
    "    if cat_fx is None or len(cat_fx)==0:\n",
    "        cat_fx = \"auto\"\n",
    "        \n",
    "    assert atk_train.shape[1]==atk_valid.shape[1], \"Train/Valid Mismatch!\"\n",
    "    \n",
    "    train_groups = atk_train['instance_id'].value_counts().sort_index().values\n",
    "    valid_groups = atk_valid['instance_id'].value_counts().sort_index().values\n",
    "\n",
    "    original_train_ids = np.cumsum(train_groups[:-1])\n",
    "    original_train_ids = np.insert(original_train_ids, 0, 0)\n",
    "    \n",
    "    original_valid_ids = np.cumsum(valid_groups[:-1])\n",
    "    original_valid_ids = np.insert(original_valid_ids, 0, 0)\n",
    "\n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(atk_train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])\n",
    "    print (\"CatFX:\", atk_train.columns.values[cat_fx])\n",
    "\n",
    "    # remove instance id\n",
    "    atk_train = atk_train.iloc[:,1:].values\n",
    "    atk_valid = atk_valid.iloc[:,1:].values\n",
    "    cat_fx = [x - 1 for x in cat_fx]\n",
    "        \n",
    "    unatk_train = atk_train[original_train_ids,:]\n",
    "    unatk_valid = atk_valid[original_valid_ids,:]\n",
    "\n",
    "    \n",
    "    # -------------------------\n",
    "    # train first iteration\n",
    "    lgbm_train = lightgbm.Dataset(data=unatk_train[:,:-1], \n",
    "                                  label=unatk_train[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_valid = lightgbm.Dataset(data=unatk_valid[:,:-1], \n",
    "                                  label=unatk_valid[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "\n",
    "    lgbm_info = {}\n",
    "    lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                num_boost_round = 1,\n",
    "                                fobj  = optimize_log_loss, \n",
    "                                feval = avg_log_loss,\n",
    "                                evals_result = lgbm_info,\n",
    "                                valid_sets   = [lgbm_train, lgbm_valid], \n",
    "                                valid_names  = ['train', 'valid'],\n",
    "                                verbose_eval=5)\n",
    "\n",
    "    # -------------------------\n",
    "    # train other iteration\n",
    "    def get_ni_w_old(preds, labels, groups):\n",
    "        # \"weights\"\n",
    "        w = np.ones(len(groups))\n",
    "\n",
    "        offset = 0\n",
    "        for instance_id, g in enumerate(groups):\n",
    "            exp_pl = np.exp(- preds[offset:offset+g] * labels[offset:offset+g])\n",
    "            w[instance_id] = 1.0 / np.max(1.0 + exp_pl)\n",
    "            offset += g  \n",
    "\n",
    "        return w\n",
    "    \n",
    "    def get_ni_w(preds, labels, groups):\n",
    "        # \"weights\"\n",
    "        w = np.ones(len(groups))\n",
    "\n",
    "        offset = 0\n",
    "        for instance_id, g in enumerate(groups):\n",
    "            exp_pl = np.exp(- preds[offset:offset+g] * labels[offset:offset+g])\n",
    "            # can we replace with e^max\n",
    "            w[instance_id] = 1.0 / np.max(1.0 + exp_pl)\n",
    "            offset += g  \n",
    "\n",
    "        return w\n",
    "    \n",
    "    def get_ni_w_num(preds, labels, groups):\n",
    "        # \"weights\"\n",
    "        w = np.ones(len(groups))\n",
    "\n",
    "        offset = 0\n",
    "        for instance_id, g in enumerate(groups):\n",
    "            exp_pl = np.exp(- preds[offset:offset+g] * labels[offset:offset+g])\n",
    "            w[instance_id] = 1.0 / np.sum(1.0 + exp_pl)\n",
    "            w[instance_id] *= np.exp(-2.0) \n",
    "            offset += g  \n",
    "\n",
    "        return w\n",
    "\n",
    "    for t in range (1, num_trees):\n",
    "    \n",
    "        # get predictions on atk instances\n",
    "        train_preds  = lgbm_model.predict(atk_train[:,:-1])\n",
    "        train_labels = atk_train[:,-1]\n",
    "        train_weights = get_ni_w(train_preds, train_labels, train_groups)\n",
    "                \n",
    "        # repeat for validation\n",
    "        valid_preds  = lgbm_model.predict(atk_valid[:,:-1])\n",
    "        valid_labels = atk_valid[:,-1]\n",
    "        valid_weights = get_ni_w(valid_preds, valid_labels, valid_groups)\n",
    "        \n",
    "        # prepare data and train\n",
    "        lgbm_train = lightgbm.Dataset(data=unatk_train[:,:-1], \n",
    "                                      label=unatk_train[:,-1],\n",
    "                                      weight=train_weights,\n",
    "                                      categorical_feature = cat_fx)\n",
    "\n",
    "        lgbm_valid = lightgbm.Dataset(data=unatk_valid[:,:-1], \n",
    "                                      label=unatk_valid[:,-1],\n",
    "                                      weight=valid_weights,\n",
    "                                      categorical_feature = cat_fx)\n",
    "\n",
    "        new_lgbm_info = {}\n",
    "        lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                    num_boost_round = 1, \n",
    "                                    init_model = lgbm_model,\n",
    "                                    fobj  = functools.partial(optimize_non_interferent_log_loss, alpha=alpha), \n",
    "                                    feval = avg_log_loss,# functools.partial(avg_non_interferent_log_loss, alpha=alpha),\n",
    "                                    evals_result = new_lgbm_info,\n",
    "                                    valid_sets   = [lgbm_train, lgbm_valid], \n",
    "                                    valid_names  = ['train', 'valid'],\n",
    "                                    verbose_eval=5)\n",
    "        \n",
    "        awesome_hack = \"avg_binary_log_loss\"\n",
    "        lgbm_info['train'][awesome_hack] += new_lgbm_info['train'][awesome_hack]\n",
    "        lgbm_info['valid'][awesome_hack] += new_lgbm_info['valid'][awesome_hack]\n",
    "\n",
    "\n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_non_interferent(train_file, valid_file, test_file, output_model_file):\n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'alpha', 'best_round', 'avg_non_interferent_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    atk_train, atk_valid, atk_test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    \n",
    "    for num_trees in [200]:\n",
    "        for alpha in [0.5]: #[0.25, 0.50, 0.75, 1.00]:\n",
    "            best_model = None\n",
    "            best_info = None\n",
    "            best_loss = np.inf\n",
    "            awesome_hack = \"avg_non_interferent_log_loss\" + \" [alpha={:.2f}]\".format(alpha)\n",
    "            awesome_hack = \"avg_binary_log_loss\"\n",
    "            \n",
    "            for learning_rate in [0.1]: #[0.01, 0.05, 0.1]:\n",
    "                for num_leaves in [24]: #[8, 16, 24, 32]:\n",
    "                    \n",
    "                    \n",
    "                    lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                    'num_leaves': num_leaves} \n",
    "                    lgbm_model, lgbm_info = extend_non_interf_model(atk_train, atk_valid, cat_fx, \n",
    "                                alpha=alpha, num_trees=num_trees, params=lgbm_params)\n",
    "                    \n",
    "                    if np.min(lgbm_info['valid'][awesome_hack]) < best_loss:\n",
    "                        best_model = lgbm_model\n",
    "                        best_info = lgbm_info\n",
    "                        best_loss = np.min(lgbm_info['valid'][awesome_hack])\n",
    "                        best_info['num_trees'] = num_trees\n",
    "                        best_info['learning_rate'] = learning_rate\n",
    "                        best_info['num_leaves'] = num_leaves\n",
    "                \n",
    "\n",
    "                    # save file\n",
    "\n",
    "                    best_valid_iter = np.argmin(lgbm_info['valid'][awesome_hack])\n",
    "\n",
    "                    # update experimental results\n",
    "                    exp = exp.append({'num_trees': num_trees, \n",
    "                                  'learning_rate':learning_rate,\n",
    "                                  'num_leaves':num_leaves, \n",
    "                                  'alpha': alpha,\n",
    "                                  'best_round':best_valid_iter+1, \n",
    "                                  'avg_non_interferent_log_loss':lgbm_info['valid'][awesome_hack][best_valid_iter]},\n",
    "                                 ignore_index=True)\n",
    "            \n",
    "                best_valid_iter = np.argmin(best_info['valid'][awesome_hack])\n",
    "            \n",
    "                model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_A{:03d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                                    best_info['num_trees'],\n",
    "                                                                                    int(best_info['learning_rate']*1000),\n",
    "                                                                                    best_info['num_leaves'],\n",
    "                                                                                    int(alpha * 100),\n",
    "                                                                                    best_valid_iter + 1\n",
    "                                                                                   )\n",
    "            \n",
    "            \n",
    "                best_model.save_model(model_file_name)\n",
    "                print (\"Model saved to\", model_file_name)\n",
    "                    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n",
      "CatFX: ['workclass' 'marital_status' 'occupation' 'relationship' 'race' 'sex'\n",
      " 'native_country']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucchese/.local/lib/python3.6/site-packages/lightgbm/basic.py:1205: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/lucchese/.local/lib/python3.6/site-packages/lightgbm/basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\ttrain's avg_binary_log_loss: 0.516512\tvalid's avg_binary_log_loss: 0.517631\n",
      "[10]\ttrain's avg_binary_log_loss: 0.438974\tvalid's avg_binary_log_loss: 0.44067\n",
      "[15]\ttrain's avg_binary_log_loss: 0.400717\tvalid's avg_binary_log_loss: 0.402585\n",
      "[20]\ttrain's avg_binary_log_loss: 0.380127\tvalid's avg_binary_log_loss: 0.382371\n",
      "[25]\ttrain's avg_binary_log_loss: 0.367717\tvalid's avg_binary_log_loss: 0.370707\n",
      "[30]\ttrain's avg_binary_log_loss: 0.359085\tvalid's avg_binary_log_loss: 0.36275\n",
      "[35]\ttrain's avg_binary_log_loss: 0.354346\tvalid's avg_binary_log_loss: 0.358746\n",
      "[40]\ttrain's avg_binary_log_loss: 0.337099\tvalid's avg_binary_log_loss: 0.343025\n",
      "[45]\ttrain's avg_binary_log_loss: 0.334643\tvalid's avg_binary_log_loss: 0.341765\n",
      "[50]\ttrain's avg_binary_log_loss: 0.32006\tvalid's avg_binary_log_loss: 0.328638\n",
      "[55]\ttrain's avg_binary_log_loss: 0.315671\tvalid's avg_binary_log_loss: 0.325411\n",
      "[60]\ttrain's avg_binary_log_loss: 0.310002\tvalid's avg_binary_log_loss: 0.320871\n",
      "[65]\ttrain's avg_binary_log_loss: 0.307152\tvalid's avg_binary_log_loss: 0.318886\n",
      "[70]\ttrain's avg_binary_log_loss: 0.303395\tvalid's avg_binary_log_loss: 0.316126\n",
      "[75]\ttrain's avg_binary_log_loss: 0.300309\tvalid's avg_binary_log_loss: 0.313921\n",
      "[80]\ttrain's avg_binary_log_loss: 0.297601\tvalid's avg_binary_log_loss: 0.312038\n",
      "[85]\ttrain's avg_binary_log_loss: 0.29667\tvalid's avg_binary_log_loss: 0.311895\n",
      "[90]\ttrain's avg_binary_log_loss: 0.293191\tvalid's avg_binary_log_loss: 0.309004\n",
      "[95]\ttrain's avg_binary_log_loss: 0.292339\tvalid's avg_binary_log_loss: 0.308971\n",
      "[100]\ttrain's avg_binary_log_loss: 0.291014\tvalid's avg_binary_log_loss: 0.308274\n",
      "[105]\ttrain's avg_binary_log_loss: 0.29021\tvalid's avg_binary_log_loss: 0.308181\n",
      "[110]\ttrain's avg_binary_log_loss: 0.289088\tvalid's avg_binary_log_loss: 0.307615\n",
      "[115]\ttrain's avg_binary_log_loss: 0.286681\tvalid's avg_binary_log_loss: 0.305561\n",
      "[120]\ttrain's avg_binary_log_loss: 0.285594\tvalid's avg_binary_log_loss: 0.305281\n",
      "[125]\ttrain's avg_binary_log_loss: 0.284608\tvalid's avg_binary_log_loss: 0.304856\n",
      "[130]\ttrain's avg_binary_log_loss: 0.28311\tvalid's avg_binary_log_loss: 0.304058\n",
      "[135]\ttrain's avg_binary_log_loss: 0.282449\tvalid's avg_binary_log_loss: 0.303932\n",
      "[140]\ttrain's avg_binary_log_loss: 0.281463\tvalid's avg_binary_log_loss: 0.303468\n",
      "[145]\ttrain's avg_binary_log_loss: 0.280263\tvalid's avg_binary_log_loss: 0.30306\n",
      "[150]\ttrain's avg_binary_log_loss: 0.279698\tvalid's avg_binary_log_loss: 0.302868\n",
      "[155]\ttrain's avg_binary_log_loss: 0.279133\tvalid's avg_binary_log_loss: 0.302896\n",
      "[160]\ttrain's avg_binary_log_loss: 0.278564\tvalid's avg_binary_log_loss: 0.302836\n",
      "[165]\ttrain's avg_binary_log_loss: 0.27779\tvalid's avg_binary_log_loss: 0.302604\n",
      "[170]\ttrain's avg_binary_log_loss: 0.276953\tvalid's avg_binary_log_loss: 0.302354\n",
      "[175]\ttrain's avg_binary_log_loss: 0.276473\tvalid's avg_binary_log_loss: 0.302305\n",
      "[180]\ttrain's avg_binary_log_loss: 0.275981\tvalid's avg_binary_log_loss: 0.302163\n",
      "[185]\ttrain's avg_binary_log_loss: 0.275251\tvalid's avg_binary_log_loss: 0.301934\n",
      "[190]\ttrain's avg_binary_log_loss: 0.274667\tvalid's avg_binary_log_loss: 0.301955\n",
      "[195]\ttrain's avg_binary_log_loss: 0.274185\tvalid's avg_binary_log_loss: 0.301919\n",
      "[200]\ttrain's avg_binary_log_loss: 0.273806\tvalid's avg_binary_log_loss: 0.301905\n",
      "Model saved to ../out/models/non_interferent_census_B150_T200_S0100_L24_A050_R193.model\n",
      "   num_trees  learning_rate  num_leaves  alpha  best_round  \\\n",
      "0      200.0            0.1        24.0    0.5       193.0   \n",
      "\n",
      "   avg_non_interferent_log_loss  \n",
      "0                      0.301842  \n"
     ]
    }
   ],
   "source": [
    "# enable/disable\n",
    "if True:\n",
    "    for B in [150]: #[5, 15, 150, 300]:\n",
    "\n",
    "        experiments = train_non_interferent(\"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/test_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../out/models/non_interferent_census_B{:d}\".format(B))  \n",
    "\n",
    "        experiments.to_csv('../out/models/non_interferent_census_B{:d}.csv'.format(B), index=False)\n",
    "\n",
    "        print (experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
