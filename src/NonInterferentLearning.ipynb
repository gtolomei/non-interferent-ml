{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    " - http://lightgbm.readthedocs.io/en/latest/\n",
    " - http://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
    " - https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "# Adding the following line, allows Jupyter Notebook to visualize plots\n",
    "# produced by matplotlib directly below the code cell which generated those.\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load 6 different datasets:\n",
    "-  Training set (original)\n",
    "-  Training set (_attacked_)\n",
    "-  Validation set (original)\n",
    "-  Validation set (_attacked_)\n",
    "-  Test set (original)\n",
    "-  Test set (_attacked_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_PATH = \"../data/census\"\n",
    "MODELS_PATH = \"../out/models\"\n",
    "ATTACKER = \"weak\" # strong\n",
    "TRAINING_SET=\"train_ori.csv.bz2\" # original training set\n",
    "TRAINING_SET_ATT=\"train_\"+ATTACKER+\"_att.csv.bz2\" # perturbed training set\n",
    "VALIDATION_SET=\"valid_ori.csv.bz2\" # original validation set\n",
    "VALIDATION_SET_ATT=\"valid_\"+ATTACKER+\"_att.csv.bz2\" # perturbed validation set\n",
    "TEST_SET=\"test_ori.csv.bz2\" # original test set\n",
    "TEST_SET_ATT=\"test_\"+ATTACKER+\"_att.csv.bz2\" # perturbed test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, dataset_filename, sep=\",\"):\n",
    "    return pd.read_csv(path+\"/\"+dataset_filename, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_categorical_features(dataset):\n",
    "    categorical_features = []\n",
    "    for column in dataset.columns:\n",
    "        if dataset[column].dtype == 'object':\n",
    "            categorical_features.append(column)\n",
    "    return categorical_features\n",
    "            \n",
    "def label_encode(dataset, categorical_features):\n",
    "    dataset_le = dataset.copy()\n",
    "    for column in dataset_le.columns:\n",
    "        if column in categorical_features:\n",
    "            dataset_le[column] = dataset_le[column].astype('category')\n",
    "            dataset_le[column] = dataset_le[column].cat.codes\n",
    "    return dataset_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(dataset, label):\n",
    "    dataset_oh = pd.get_dummies(dataset)\n",
    "    columns = dataset_oh.columns.tolist()\n",
    "    columns.insert(len(columns), columns.pop(columns.index(label)))\n",
    "    dataset_oh = dataset_oh.loc[:,columns]\n",
    "    dataset_oh.columns = columns\n",
    "    \n",
    "    return dataset_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = load_dataset(DATASETS_PATH, TRAINING_SET)\n",
    "TRAIN_ATT = load_dataset(DATASETS_PATH, TRAINING_SET_ATT)\n",
    "\n",
    "VALID = load_dataset(DATASETS_PATH, VALIDATION_SET)\n",
    "VALID_ATT = load_dataset(DATASETS_PATH, VALIDATION_SET_ATT)\n",
    "\n",
    "TEST = load_dataset(DATASETS_PATH, TEST_SET)\n",
    "TEST_ATT = load_dataset(DATASETS_PATH, TEST_SET_ATT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute group lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ATT_OFFSETS = TRAIN_ATT['instance_id'].value_counts().sort_index().values\n",
    "VALID_ATT_OFFSETS = VALID_ATT['instance_id'].value_counts().sort_index().values\n",
    "TEST_ATT_OFFSETS = TEST_ATT['instance_id'].value_counts().sort_index().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer _categorical_ features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES = infer_categorical_features(TRAIN)\n",
    "print(\"List of categorical features: [{}]\"\n",
    "      .format(\", \".join([cf for cf in CATEGORICAL_FEATURES])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform _categorical_ features to _numeric_ (label encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = label_encode(TRAIN, set(CATEGORICAL_FEATURES))\n",
    "TRAIN_ATT = label_encode(TRAIN_ATT.iloc[:,1:], set(CATEGORICAL_FEATURES))\n",
    "\n",
    "VALID = label_encode(VALID, set(CATEGORICAL_FEATURES))\n",
    "VALID_ATT = label_encode(VALID_ATT.iloc[:,1:], set(CATEGORICAL_FEATURES))\n",
    "\n",
    "TEST = label_encode(TEST, set(CATEGORICAL_FEATURES))\n",
    "TEST_ATT = label_encode(TEST_ATT.iloc[:,1:], set(CATEGORICAL_FEATURES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform _categorical_ features to _numeric_ (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN = one_hot_encode(TRAIN, \"income_greater_than_50k\")\n",
    "# TRAIN_ATT = one_hot_encode(TRAIN_ATT.iloc[:,1:], \"income_greater_than_50k\")\n",
    "\n",
    "# VALID = one_hot_encode(VALID, \"income_greater_than_50k\")\n",
    "# VALID_ATT = one_hot_encode(VALID_ATT.iloc[:,1:], \"income_greater_than_50k\")\n",
    "\n",
    "# TEST = one_hot_encode(TEST, \"income_greater_than_50k\")\n",
    "# TEST_ATT = one_hot_encode(TEST_ATT.iloc[:,1:], \"income_greater_than_50k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(TRAIN.shape)\n",
    "print(TRAIN_ATT.shape)\n",
    "print(VALID.shape)\n",
    "print(VALID_ATT.shape)\n",
    "print(TEST.shape)\n",
    "print(TEST_ATT.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters used for _standard_ and _baseline_ learning\n",
    "\n",
    "-  Training is done by optimizing (i.e., minimizing) standard **binary log loss** (<code>fobj=optimize_binary_logloss</code>)\n",
    "-  Evaluation is measured using standard **binary log loss** (<code>feval=avg_binary_logloss</code>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please, refer to https://slundberg.github.io/shap/notebooks/Census%20income%20classification%20with%20LightGBM.html for any further detail\n",
    "# or\n",
    "# https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n",
    "std_params = {\n",
    "    \"max_bin\": 511,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"boosting_type\": \"gbdt\",#\"rf\"\n",
    "    \"objective\": \"regression_l2\", #\"binary\",\n",
    "    \"metric\": [\"None\"], # We use our own implementation of binary log loss (i.e., optimize_binary_logloss) \n",
    "                        # instead of the default one (i.e., \"binary_logloss\"), which may be in fact cross-entropy\n",
    "    \"num_leaves\": 15,\n",
    "    \"verbose\": 1,\n",
    "    \"min_data_in_leaf\": 20,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"boost_from_average\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters used for _non-interferent_ learning\n",
    "\n",
    "-  Training is done by optimizing (i.e., minimizing) our custom **binary log loss under max attack** (<code>fobj=optimize_binary_logloss_under_max_attack</code>)\n",
    "-  Evaluation is measured using our custom **binary log loss under max attack** (<code>feval=avg_binary_logloss_under_max_attack</code>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please, refer to https://slundberg.github.io/shap/notebooks/Census%20income%20classification%20with%20LightGBM.html for any further detail\n",
    "# or\n",
    "# https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n",
    "non_interferent_params = {\n",
    "    \"max_bin\": 511,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"boosting_type\": \"gbdt\",#\"rf\",\n",
    "    \"objective\": \"regression_l2\",\n",
    "    \"metric\": [\"None\"], # We will specify our own custom objective function (i.e., optimize_binary_logloss_under_max_attack)\n",
    "    \"num_leaves\": 15,\n",
    "    \"verbose\": 1,\n",
    "    \"min_data_in_leaf\": 20,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"boost_from_average\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_BOOST_ROUNDS = 200\n",
    "MIN_BOOST_ROUNDS = 200\n",
    "STEP_BOOST_ROUNDS = 50\n",
    "BOOST_ROUNDS = [br for br in range(MIN_BOOST_ROUNDS, MAX_BOOST_ROUNDS+1, STEP_BOOST_ROUNDS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard objective function\n",
    "\n",
    "The following function, called <code>optimize_binary_logloss</code>, is the one optimized (i.e., minimized) for learning _standard_ and _baseline_ approaches. More specifically, this is the standard binary log loss which is used to train any _standard_ or _baseline_ model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>fobj=optimize_binary_logloss</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined objective function\n",
    "# f(preds: array, train_data: Dataset) -> grad: array, hess: array\n",
    "\n",
    "# To be used with a regression task\n",
    "def optimize_binary_logloss(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    exp_pl = np.exp(preds * labels)\n",
    "    # http://www.wolframalpha.com/input/?i=differentiate+log(1+%2B+exp(-kx)+)\n",
    "    grads = -labels / (1.0 +  exp_pl)  \n",
    "    # http://www.wolframalpha.com/input/?i=d%5E2%2Fdx%5E2+log(1+%2B+exp(-kx)+)\n",
    "    hess = labels**2 * exp_pl / (1.0 + exp_pl)**2 \n",
    "\n",
    "    # this is to optimize average logloss\n",
    "    norm = 1.0/len(preds)\n",
    "    grads *= norm\n",
    "    hess *= norm\n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom objective function\n",
    "\n",
    "In addition to the standard binary log loss used to train a model, we introduce another custom objective function called <code>optimize_binary_logloss_under_max_attack</code>. This function is used to train a _non-interferent_ model; in other words, non-interferent models are learned by optimizing (i.e., minimizing) the following function.\n",
    "\n",
    "$$\n",
    "\\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}} \\log  \\left( \\sum_{\\mathbf{x}' \\in \\mathit{MaxAtk}({\\mathbf{x}},{A})} e^{\\ell(h(\\mathbf{x}'), y)} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>fobj=optimize_binary_logloss_under_max_attack</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined objective function\n",
    "# f(preds: array, train_data: Dataset) -> grad: array, hess: array\n",
    "\n",
    "def optimize_binary_logloss_under_max_attack(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    grads = np.zeros_like(labels, dtype=np.float64)\n",
    "    hess = np.zeros_like(grads)\n",
    "\n",
    "    norm = 1.0 / float(len(attack_lens))\n",
    "    \n",
    "    offset = 0\n",
    "    for atk in attack_lens:\n",
    "        exp_pl = np.exp(- preds[offset:offset+atk] * labels[offset:offset+atk])\n",
    "        \n",
    "        inv_sum = 1.0 / np.sum(1.0 + exp_pl)\n",
    "\n",
    "        x_grad = inv_sum * exp_pl\n",
    "            \n",
    "        grads[offset:offset+atk] = norm * x_grad * (- labels[offset:offset+atk])\n",
    "        hess[offset:offset+atk]  = norm * x_grad * (1.0 - x_grad)\n",
    "        \n",
    "        offset += atk    \n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard evaluation metric\n",
    "\n",
    "The following function is the one used for evaluating the quality of the learned model (either _standard_, _baseline_, or _non-interferent_). This is the standard <code>avg_binary_logloss</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>feval=avg_binary_logloss</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_binary_logloss(preds, train_data):\n",
    "    \n",
    "    labels = train_data.get_label()\n",
    "    losses = np.log(1.0 + np.exp(-preds*labels))\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return 'binary_logloss', avg_loss, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_binary_logloss(model, boost_round, test, test_groups=None):\n",
    "    \n",
    "    lgbm_test = lightgbm.Dataset(data=test.iloc[:,:-1].values, \n",
    "                                 label=test.iloc[:,-1].values,\n",
    "                                 free_raw_data=False)\n",
    "    \n",
    "    return avg_binary_logloss(model.predict(test.iloc[:,:-1].values, num_iteration=boost_round), lgbm_test)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom evaluation metric\n",
    "\n",
    "In addition to the standard <code>avg_binary_logloss</code>, we provide an additional evaluation metric called <code>avg_binary_logloss_under_max_attack</code>, which is used for measuring the validity of any model (either _standard_, _baseline_, or _non-interferent_). More precisely, <code>avg_binary_logloss_under_max_attack</code> is the binary logloss modified to operate on groups of perturbed instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>feval=avg_binary_logloss_under_max_attack</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(p):\n",
    "    return 1.0/(1.0 + np.exp(-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom metric\n",
    "\n",
    "def binary_logloss(pred, true_label):\n",
    "\n",
    "    return np.log(1.0 + np.exp(-pred * true_label))\n",
    "\n",
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_binary_logloss_under_max_attack(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    offset = 0\n",
    "    max_logloss = []\n",
    "    \n",
    "    for atk in attack_lens:\n",
    "        losses = [binary_logloss(h,t) for h,t in zip(preds[offset:offset+atk], labels[offset:offset+atk])]\n",
    "        max_logloss.append(max(losses))\n",
    "        \n",
    "        offset += atk\n",
    "    \n",
    "    return 'binary_logloss_under_max_attack', np.mean(max_logloss), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_binary_logloss_under_max_attack(model, boost_round, test, test_groups=None):\n",
    "    \n",
    "    lgbm_test = lightgbm.Dataset(data=test.iloc[:,:-1].values, \n",
    "                                 label=test.iloc[:,-1].values,\n",
    "                                 group=test_groups,\n",
    "                                 free_raw_data=False)\n",
    "    \n",
    "    return avg_binary_logloss_under_max_attack(model.predict(test.iloc[:,:-1].values, num_iteration=boost_round), \n",
    "                                               lgbm_test)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional validity measures\n",
    "\n",
    "In addition to the evaluation metrics defined above (used for training), we also consider the following **4** measures of validity to compare the performance of each learned model:\n",
    "\n",
    "-  <code>eval_binary_err_rate</code>: This is the traditional binary error rate (1-accuracy);\n",
    "-  <code>eval_binary_err_rate_under_max_attack</code>: This is the binary error rate modified to operate on groups of perturbed instances.\n",
    "-  <code>eval_roc_auc</code>: This is the classical ROC AUC score;\n",
    "-  <code>eval_roc_auc_under_max_attack</code>: This is the ROC AUC score modified to operate on groups of perturbed instances.\n",
    "\n",
    "Again, note that those are **not** metrics used at training time (i.e., they do not define any <code>feval</code>), rather they are used to assess the (offline) quality of each learned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_binary_err_rate</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_binary_err_rate(model, boost_round, test_set, test_groups=None):\n",
    "    X = test_set.iloc[:,:-1].values\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "    predictions = [1 if p > 0 else -1 for p in model.predict(X, num_iteration=boost_round)]\n",
    "    \n",
    "    errs = 0\n",
    "    for p,l in zip(predictions,labels):\n",
    "        if p != l:\n",
    "            errs += 1\n",
    "    return errs/len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_binary_err_rate_under_max_attack</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_binary_err_rate_under_max_attack(model, boost_round, test_set, test_groups=None):\n",
    "    X = test_set.iloc[:,:-1].values\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "    predictions = [1 if p > 0 else -1 for p in model.predict(X, num_iteration=boost_round)]\n",
    "    \n",
    "    offset = 0\n",
    "    errs = 0\n",
    "\n",
    "    for g in test_groups:\n",
    "        predictions_att = predictions[offset:offset+g]\n",
    "        true_label = labels[offset]\n",
    "        if np.any([p != true_label for p in predictions_att]):\n",
    "            errs += 1\n",
    "        offset += g\n",
    "\n",
    "    return errs/len(test_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_roc_auc</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_roc_auc(model, boost_round, test_set, test_groups=None):\n",
    "    X = test_set.iloc[:,:-1].values\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "    predictions = logistic(model.predict(X, num_iteration=boost_round))\n",
    "    \n",
    "    return roc_auc_score(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_roc_auc_under_max_attack</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_roc_auc_under_max_attack(model, boost_round, test_set, test_groups=None):\n",
    "    X = test_set.iloc[:,:-1].values\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "    predictions = logistic(model.predict(X, num_iteration=boost_round))\n",
    "    \n",
    "    offset = 0\n",
    "    true_labels = []\n",
    "    worst_predictions = []\n",
    "    \n",
    "    for g in test_groups:\n",
    "        true_label = labels[offset]\n",
    "        true_labels.append(true_label)\n",
    "        predictions_att = predictions[offset:offset+g]\n",
    "        prediction_distances = np.abs(predictions_att - true_label)\n",
    "        worst_predictions.append(predictions_att[np.argmax(prediction_distances)])\n",
    "    \n",
    "        offset += g\n",
    "        \n",
    "    return roc_auc_score(true_labels, worst_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_f1</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_f1(model, boost_round, test_set, test_groups=None):\n",
    "    X = test_set.iloc[:,:-1].values\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "    predictions = [1 if p > 0 else -1 for p in logistic(model.predict(X, num_iteration=boost_round))]\n",
    "    \n",
    "    return f1_score(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_f1_under_max_attack</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_f1_under_max_attack(model, boost_round, test_set, test_groups=None):\n",
    "    X = test_set.iloc[:,:-1].values\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "    predictions = [1 if p > 0 else -1 for p in logistic(model.predict(X, num_iteration=boost_round))]\n",
    "    \n",
    "    offset = 0\n",
    "    true_labels = []\n",
    "    worst_predictions = []\n",
    "    \n",
    "    for g in test_groups:\n",
    "        true_label = labels[offset]\n",
    "        true_labels.append(true_label)\n",
    "        predictions_att = predictions[offset:offset+g]\n",
    "        prediction_distances = np.abs(predictions_att - true_label)\n",
    "        worst_predictions.append(predictions_att[np.argmax(prediction_distances)])\n",
    "    \n",
    "        offset += g\n",
    "        \n",
    "    return f1_score(true_labels, worst_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_filename, model):\n",
    "    with open(model_filename, 'wb') as fout:\n",
    "        pickle.dump(model, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_filename):\n",
    "    with open(model_filename, 'rb') as fin:\n",
    "        return pickle.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_METRICS = [eval_binary_logloss, \n",
    "                eval_binary_err_rate, \n",
    "                eval_roc_auc\n",
    "               ]\n",
    "\n",
    "EVAL_METRICS_UNDER_MAX_ATTACK = [eval_binary_logloss_under_max_attack, \n",
    "                                 eval_binary_err_rate_under_max_attack, \n",
    "                                 eval_roc_auc_under_max_attack\n",
    "                                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate each model w.r.t. _all_ evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_learned_model(model, boost_round, eval_metric, test, test_groups=None):\n",
    "    return eval_metric(model, boost_round, test, test_groups=test_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_learned_models(model, model_type, boost_round, test, test_groups=None):\n",
    "\n",
    "    eval_metrics = EVAL_METRICS\n",
    "    d_test = \"D_test\"\n",
    "    if test_groups is not None:\n",
    "        eval_metrics = EVAL_METRICS_UNDER_MAX_ATTACK\n",
    "        d_test = \"D_test_att\"\n",
    "    \n",
    "    header = ['Model','N. of Trees'] + [m.__name__.replace('eval_','').replace('_',' ').title() for m in eval_metrics]\n",
    "    df = pd.DataFrame(columns=header)\n",
    "    first_row = [model_type, boost_round] + [None for m in eval_metrics]\n",
    "    df.loc[0] = first_row\n",
    "\n",
    "    for eval_metric in eval_metrics:\n",
    "        res = eval_learned_model(model, boost_round, eval_metric, test, test_groups=test_groups)\n",
    "        print(\"{} learning - {} on {} [boost rounds={}] = {:.5f}\"\n",
    "                  .format(model_type, eval_metric.__name__, d_test, boost_round, res))\n",
    "        df[eval_metric.__name__.replace('eval_','').replace('_',' ').title()] = res\n",
    "    print(\"******************************************************************************************************\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. _Standard_ Learning: Models are trained on the original dataset $\\mathcal{D}_{train}$ using _standard_ binary log loss\n",
    "\n",
    "-  This model is trained on the original training set by minimizing standard **binary log loss** (i.e., <code>fobj=optimize_binary_logloss</code>)\n",
    "\n",
    "-  Its performance is assessed by means of <code>feval=avg_binary_logloss</code> (i.e., the metric optimized during training) both on training and validation set.\n",
    "\n",
    "-  The model leading to the lowest **binary log loss** on the validation test is the one returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_learning(train, \n",
    "                 valid,\n",
    "                 params=std_params, \n",
    "                 fobj=optimize_binary_logloss, \n",
    "                 feval=avg_binary_logloss,\n",
    "                 num_boost_round=MIN_BOOST_ROUNDS):\n",
    "    \n",
    "    lgbm_info = {}\n",
    "    \n",
    "    lgbm_train = lightgbm.Dataset(data=train.iloc[:,:-1].values, \n",
    "                                  label=train.iloc[:,-1].values\n",
    "                                 )\n",
    "    \n",
    "    lgbm_valid = lightgbm.Dataset(data=valid.iloc[:,:-1].values, \n",
    "                                  label=valid.iloc[:,-1].values,\n",
    "                                  reference=lgbm_train, \n",
    "                                  free_raw_data=False)\n",
    "    \n",
    "    lgbm_model = lightgbm.train(params=params, \n",
    "                                train_set=lgbm_train, \n",
    "                                num_boost_round=num_boost_round,\n",
    "                                valid_sets = [lgbm_valid],\n",
    "                                valid_names  = [\"validation\"],\n",
    "                                fobj = fobj,\n",
    "                                feval = feval,\n",
    "                                evals_result = lgbm_info,\n",
    "                                early_stopping_rounds=50,\n",
    "                                verbose_eval=20)\n",
    "    \n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_learning_runs(train, valid, boost_rounds=BOOST_ROUNDS):\n",
    "    \n",
    "    std_runs = {}\n",
    "    std_runs['type'] = 'Standard'\n",
    "    std_runs['run'] = {}\n",
    "    for br in boost_rounds:\n",
    "        print(\"***** {} learning - Optimizing standard binary log loss on the original D_train [boost rounds={}] *****\"\n",
    "              .format(std_runs['type'], br))\n",
    "        std_model, std_res = std_learning(train, valid, num_boost_round=br)\n",
    "        std_runs['run'][br] = {}\n",
    "        std_runs['run'][br]['model'] = std_model\n",
    "        std_runs['run'][br]['results'] = std_res\n",
    "\n",
    "    return std_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "std_runs = std_learning_runs(TRAIN, VALID, boost_rounds=BOOST_ROUNDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STD_MODEL_FILENAME = MODELS_PATH+\"/std_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\".pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persist _standard_ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(STD_MODEL_FILENAME, std_runs['run'][MAX_BOOST_ROUNDS]['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. _Baseline_: Learning models trained on the attacked $\\mathcal{D}_{train\\_att}$ using _standard_ binary log loss\n",
    "\n",
    "-  This model is trained on the original training set by minimizing standard **binary log loss** (i.e., <code>fobj=optimize_binary_logloss</code>)\n",
    "\n",
    "-  Its performance is assessed by means of <code>feval=avg_binary_logloss</code> (i.e., the metric optimized during training) both on training and validation set.\n",
    "\n",
    "-  The model leading to the lowest **binary log loss** on the validation test is the one returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_select_instances(groups, p_attacked_inst, n_attacks_per_inst):\n",
    "    \n",
    "    i = 0\n",
    "    selected_instances = []\n",
    "    for g in groups:\n",
    "        selected_instances.append(i)\n",
    "        if n_attacks_per_inst > 0:\n",
    "            if g > n_attacks_per_inst:\n",
    "                if np.random.random_sample() <= p_attacked_inst: # the instance is going to be attacked\n",
    "                    selected = np.random.choice(g-1, n_attacks_per_inst, replace=False) + i + 1\n",
    "                    selected_instances.extend(sorted(selected))\n",
    "            else:\n",
    "                selected_instances.extend([x for x in range(i+1,i+g)])\n",
    "        i += g\n",
    "    \n",
    "    return selected_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_learning(train, \n",
    "                      valid, \n",
    "                      p_attacked_inst, \n",
    "                      n_attacks_per_inst, \n",
    "                      params=std_params, \n",
    "                      fobj=optimize_binary_logloss, \n",
    "                      feval=avg_binary_logloss,\n",
    "                      num_boost_round=MIN_BOOST_ROUNDS, \n",
    "                      train_group=TRAIN_ATT_OFFSETS):\n",
    "\n",
    "    \n",
    "    selected_instances = random_select_instances(train_group, p_attacked_inst, n_attacks_per_inst)\n",
    "    train = train.loc[selected_instances]\n",
    "    \n",
    "    lgbm_info = {}\n",
    "    \n",
    "    lgbm_train = lightgbm.Dataset(data=train.iloc[:,:-1].values, \n",
    "                                  label=train.iloc[:,-1].values\n",
    "                                 )\n",
    "    \n",
    "    lgbm_valid = lightgbm.Dataset(data=valid.iloc[:,:-1].values, \n",
    "                                  label=valid.iloc[:,-1].values,\n",
    "                                  reference=lgbm_train, \n",
    "                                  free_raw_data=False)\n",
    "    \n",
    "    lgbm_model = lightgbm.train(params=params, \n",
    "                                train_set=lgbm_train, \n",
    "                                num_boost_round=num_boost_round,\n",
    "                                valid_sets = [lgbm_valid],\n",
    "                                valid_names  = [\"validation\"],\n",
    "                                fobj = fobj,\n",
    "                                feval = feval,\n",
    "                                evals_result = lgbm_info,\n",
    "                                early_stopping_rounds=50,\n",
    "                                verbose_eval=20)\n",
    "    \n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_learning_runs(train, \n",
    "                           valid, \n",
    "                           p_attacked_inst=1.0,\n",
    "                           n_attacks_per_inst=0,\n",
    "                           boost_rounds=BOOST_ROUNDS\n",
    "                          ):\n",
    "    \n",
    "    baseline_runs = {}\n",
    "    baseline_runs['type'] = 'Baseline' \n",
    "    baseline_runs['run'] = {}\n",
    "    \n",
    "    for br in boost_rounds:\n",
    "        print(\"***** {} learning - Optimizing standard binary log loss on D_train_att [boost rounds={}; p_attacked_inst={:.2f}; n_attacks_per_inst={}] *****\"\n",
    "              .format(baseline_runs['type'], br, p_attacked_inst, n_attacks_per_inst))\n",
    "        baseline_model, baseline_res = baseline_learning(train, \n",
    "                                                         valid, \n",
    "                                                         p_attacked_inst, \n",
    "                                                         n_attacks_per_inst, \n",
    "                                                         num_boost_round=br)\n",
    "        baseline_runs['run'][br] = {}\n",
    "        baseline_runs['run'][br]['model'] = baseline_model\n",
    "        baseline_runs['run'][br]['results'] = baseline_res\n",
    "\n",
    "    return baseline_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_ATTACKED_INSTANCE = [1.0]\n",
    "N_ATTACKS_PER_INSTANCE = [1, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Persist _baseline_ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for pa in P_ATTACKED_INSTANCE:\n",
    "    for na in N_ATTACKS_PER_INSTANCE:\n",
    "        baseline_runs = baseline_learning_runs(TRAIN_ATT, VALID_ATT, \n",
    "                                               p_attacked_inst=pa, \n",
    "                                               n_attacks_per_inst=na,\n",
    "                                               boost_rounds=BOOST_ROUNDS\n",
    "                                              )\n",
    "\n",
    "        BASELINE_MODEL_FILENAME = MODELS_PATH+\"/baseline_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\"_p-{}_n-{}.pkl\".format(int(pa*100), na)\n",
    "        save_model(BASELINE_MODEL_FILENAME, baseline_runs['run'][MAX_BOOST_ROUNDS]['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. _Non-Interferent_: Learn _non-interferent_ models trained on the original $\\mathcal{D}_{train}$ using custom cost function (binary log loss under max attack)\n",
    "\n",
    "-  This model is trained on the original training set by minimizing our custom objective function, i.e., a modified **binary log loss** called <code>optimize_binary_logloss_under_max_attack</code>.\n",
    "\n",
    "-  Its performance is assessed by means of <code>avg_binary_logloss_under_max_attack</code> both on training and validation set.\n",
    "\n",
    "-  The model leading to the lowest <code>avg_binary_logloss_under_max_attack</code> on the validation test is the one returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_interferent_learning(train, \n",
    "                             valid, \n",
    "                             params=non_interferent_params, \n",
    "                             num_boost_round=MIN_BOOST_ROUNDS, \n",
    "                             fobj=optimize_binary_logloss_under_max_attack, \n",
    "                             feval=avg_binary_logloss_under_max_attack,\n",
    "                             train_group=TRAIN_ATT_OFFSETS,\n",
    "                             valid_group=VALID_ATT_OFFSETS):\n",
    "    \n",
    "    lgbm_info = {}\n",
    "    \n",
    "    lgbm_train = lightgbm.Dataset(data=train.iloc[:,:-1].values, \n",
    "                                  label=train.iloc[:,-1].values, \n",
    "                                  group=train_group\n",
    "                                 )\n",
    "    \n",
    "    lgbm_valid = lightgbm.Dataset(data=valid.iloc[:,:-1].values, \n",
    "                                  label=valid.iloc[:,-1].values, \n",
    "                                  group=valid_group,\n",
    "                                  reference=lgbm_train, \n",
    "                                  free_raw_data=False)\n",
    "    \n",
    "    lgbm_model = lightgbm.train(params=params, \n",
    "                                train_set=lgbm_train, \n",
    "                                num_boost_round=num_boost_round, \n",
    "                                valid_sets = [lgbm_valid],\n",
    "                                valid_names  = [\"validation\"], \n",
    "                                evals_result = lgbm_info,\n",
    "                                fobj = fobj,\n",
    "                                feval = feval,\n",
    "                                early_stopping_rounds=50,\n",
    "                                verbose_eval=20)\n",
    "    \n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_interferent_learning_runs(train, \n",
    "                                  valid, \n",
    "                                  boost_rounds=BOOST_ROUNDS\n",
    "                                 ):\n",
    "    \n",
    "    non_interferent_runs = {}\n",
    "    non_interferent_runs['type'] = 'Non-Interferent'\n",
    "    non_interferent_runs['run'] = {}\n",
    "    \n",
    "    for br in boost_rounds:\n",
    "        print(\"***** {} learning - Optimizing custom binary log loss under max attack on D_train_att [boost rounds={}] *****\"\n",
    "              .format(non_interferent_runs['type'], br))\n",
    "        ni_model, ni_res = non_interferent_learning(train, \n",
    "                                                    valid, \n",
    "                                                    num_boost_round=br)\n",
    "        non_interferent_runs['run'][br] = {}\n",
    "        non_interferent_runs['run'][br]['model'] = ni_model\n",
    "        non_interferent_runs['run'][br]['results'] = ni_res\n",
    "\n",
    "    return non_interferent_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "non_interferent_runs = non_interferent_learning_runs(TRAIN_ATT, VALID_ATT, \n",
    "                                                     boost_rounds=BOOST_ROUNDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_INTERFERENT_MODEL_FILENAME = MODELS_PATH+\"/non-interferent_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\".pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persist _non-interferent_ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(NON_INTERFERENT_MODEL_FILENAME, non_interferent_runs['run'][MAX_BOOST_ROUNDS]['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EVAL_TREES = 200\n",
    "MIN_EVAL_TREES = 10\n",
    "STEP_EVAL_TREES = 10\n",
    "EVAL_TREES = sorted(list(set([t for t in range(MIN_EVAL_TREES, MAX_EVAL_TREES, STEP_EVAL_TREES)] + [MAX_EVAL_TREES])))\n",
    "# The following adds the \"best_iteration\" learned on the validation set\n",
    "EVAL_TREES = [0] + EVAL_TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_runs(model, model_type, test, eval_trees=EVAL_TREES, test_groups=None):\n",
    "    eval_results = []\n",
    "    for t in eval_trees:\n",
    "        eval_results.append(eval_learned_models(model, model_type, t, test, test_groups=test_groups))\n",
    "        \n",
    "    eval_df = pd.concat(eval_results, axis=0)\n",
    "    eval_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve all model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_filenames():\n",
    "    return sorted([f for f in listdir(MODELS_PATH) if f != '.gitignore' and isfile(join(MODELS_PATH, f))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_filenames = get_model_filenames()\n",
    "print(\"\\n\".join([mf for mf in all_model_filenames]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load _standard_ models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STD_MODEL_FILENAME = MODELS_PATH+\"/std_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\".pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_model = load_model(STD_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate _standard_ models on $D_{test}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "std_df = eval_runs(std_model, \"Standard\", TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate _standard_ models on $D_{test\\_att}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_att_df = eval_runs(std_model, \"Standard\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge both _standard_ evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_std_df = pd.merge(std_df, std_att_df, on=['Model', 'N. of Trees'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_std_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load _baseline_ models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_MODEL_100_1_FILENAME = MODELS_PATH+\"/baseline_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\"_p-100_n-1.pkl\"\n",
    "#BASELINE_MODEL_100_4_FILENAME = MODELS_PATH+\"/baseline_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\"_p-100_n-4.pkl\"\n",
    "BASELINE_MODEL_100_MAX_FILENAME = MODELS_PATH+\"/baseline_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\"_p-100_n-1000.pkl\"\n",
    "#BASELINE_MODEL_50_1_FILENAME = MODELS_PATH+\"/baseline_\"+ATTACKER+\"+\"_\"+str(MAX_BOOST_ROUNDS)+\"_p-50_n-1.pkl\"\n",
    "#BASELINE_MODEL_50_4_FILENAME = MODELS_PATH+\"/baseline_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\"_p-50_n-4.pkl\"\n",
    "#BASELINE_MODEL_50_MAX_FILENAME = MODELS_PATH+\"/baseline_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\"_p-50_n-1000.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_100_1 = load_model(BASELINE_MODEL_100_1_FILENAME)\n",
    "#baseline_model_100_4 = load_model(BASELINE_MODEL_100_4_FILENAME)\n",
    "baseline_model_100_MAX = load_model(BASELINE_MODEL_100_MAX_FILENAME)\n",
    "# baseline_model_50_1 = load_model(BASELINE_MODEL_50_1_FILENAME)\n",
    "# baseline_model_50_4 = load_model(BASELINE_MODEL_50_4_FILENAME)\n",
    "# baseline_model_50_MAX = load_model(BASELINE_MODEL_50_MAX_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate _baseline_ models on $D_{test}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_100_1_df = eval_runs(baseline_model_100_1, \"Baseline (p=1.0; n=1)\", TEST)\n",
    "# baseline_100_4_df = eval_runs(baseline_model_100_4, \"Baseline (p=1.0; n=4)\", TEST)\n",
    "baseline_100_max_df = eval_runs(baseline_model_100_MAX, \"Baseline (p=1.0; n=max)\", TEST)\n",
    "# baseline_50_1_df = eval_runs(baseline_model_50_1, \"Baseline (p=0.5; n=1)\", TEST)\n",
    "# baseline_50_4_df = eval_runs(baseline_model_50_4, \"Baseline (p=0.5; n=4)\", TEST)\n",
    "# baseline_50_max_df = eval_runs(baseline_model_50_MAX, \"Baseline (p=0.5; n=max)\", TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = pd.concat(#[baseline_100_1_df, baseline_100_4_df, baseline_100_max_df, baseline_50_1_df, baseline_50_4_df, baseline_50_max_df], \n",
    "                        [baseline_100_1_df, baseline_100_max_df], \n",
    "                        axis=0)\n",
    "baseline_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate _baseline_ model on $D_{test\\_att}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baseline_att_100_1_df = eval_runs(baseline_model_100_1, \"Baseline (p=1.0; n=1)\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)\n",
    "#baseline_att_100_4_df = eval_runs(baseline_model_100_4, \"Baseline (p=1.0; n=4)\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)\n",
    "baseline_att_100_max_df = eval_runs(baseline_model_100_MAX, \"Baseline (p=1.0; n=max)\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)\n",
    "#baseline_att_50_1_df = eval_runs(baseline_model_50_1, \"Baseline (p=0.5; n=1)\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)\n",
    "#baseline_att_50_4_df = eval_runs(baseline_model_50_4, \"Baseline (p=0.5; n=4)\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)\n",
    "#baseline_att_50_max_df = eval_runs(baseline_model_50_MAX, \"Baseline (p=0.5; n=max)\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_att_df = pd.concat(# [baseline_att_100_1_df, baseline_att_100_4_df, baseline_att_100_max_df, baseline_att_50_1_df, baseline_att_50_4_df, baseline_att_50_max_df]\n",
    "    [baseline_att_100_1_df, baseline_att_100_max_df], axis=0)\n",
    "baseline_att_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_att_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge both _baseline_ evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_baseline_df = pd.merge(baseline_df, baseline_att_df, on=[\"Model\", \"N. of Trees\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_baseline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load _non-interferent_ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_INTERFERENT_MODEL_FILENAME = MODELS_PATH+\"/non-interferent_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\".pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_interferent_model = load_model(NON_INTERFERENT_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate _non-interferent_ model on $D_{test}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_interferent_df = eval_runs(non_interferent_model, \"Non-Interferent\", TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate _non-interferent_ model on $D_{test\\_att}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "non_interferent_att_df = eval_runs(non_interferent_model, \"Non-Interferent\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge both _non-interferent_ evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_non_interferent_df = pd.merge(non_interferent_df, non_interferent_att_df, on=['Model', 'N. of Trees'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack _all_ evaluations one on top of each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df = pd.concat([overall_non_interferent_df, overall_baseline_df, overall_std_df], axis=0)\n",
    "overall_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the DataFrame containing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df.to_csv(\"../plots/\"+ATTACKER+\".csv\", sep=\",\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
