{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- True implementation\n",
    "  - LightGBM (scikit learn)\n",
    "  - Loop: attack & train\n",
    "    - annotate full-attacks dataset with costs\n",
    "    - generate attacked datasets\n",
    "    - continue training from previous forest with new data\n",
    "    \n",
    "    \n",
    "validation split is a bit strange ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_categorical_features(dataset):\n",
    "    categorical_features = set([])\n",
    "    for column in dataset.columns:\n",
    "        if dataset[column].dtype == 'object':\n",
    "            categorical_features.add(column)\n",
    "    return categorical_features\n",
    "            \n",
    "def label_encode(dataset, categorical_features):\n",
    "    dataset_le = dataset.copy()\n",
    "    for column in dataset_le.columns:\n",
    "        if column in categorical_features:\n",
    "            dataset_le[column] = dataset_le[column].astype('category')\n",
    "            dataset_le[column] = dataset_le[column].cat.codes\n",
    "    return dataset_le\n",
    "\n",
    "def load_atk_train_valid_test( atk_train_file, atk_valid_file, atk_test_file, force=False):\n",
    "    \n",
    "    \n",
    "    if  ( force or \n",
    "          not os.path.exists(atk_train_file+\".cat.bz2\") or\n",
    "          not os.path.exists(atk_valid_file+\".cat.bz2\") or\n",
    "          not os.path.exists(atk_test_file+\".cat.bz2\") ):\n",
    "    \n",
    "        print (\"Pre-processing original files...\")\n",
    "\n",
    "        print (\"Loading:\", atk_train_file)\n",
    "        print (\"Loading:\", atk_valid_file)\n",
    "        print (\"Loading:\", atk_test_file)\n",
    "\n",
    "        train = pd.read_csv(atk_train_file)\n",
    "        valid = pd.read_csv(atk_valid_file)\n",
    "        test  = pd.read_csv(atk_test_file)\n",
    "        \n",
    "        print (\"Train/Valid/Test sizes:\", train.shape, valid.shape, test.shape)\n",
    "        print (\"Train/Valid/Test split: {:.2f} {:.2f} {:.2f}\"\n",
    "                   .format( train.shape[0]/(train.shape[0]+valid.shape[0]+test.shape[0]),\n",
    "                            valid.shape[0]/(train.shape[0]+valid.shape[0]+test.shape[0]),\n",
    "                            test.shape[0] /(train.shape[0]+valid.shape[0]+test.shape[0]) ) )\n",
    "\n",
    "        # concat to process correctly label encoding\n",
    "        full = pd.concat( [train, valid, test] )\n",
    "\n",
    "        fx = infer_categorical_features(full)\n",
    "        print(\"List of categorical features: [{}]\".format(\", \".join([cf for cf in fx])))\n",
    "\n",
    "        full = label_encode(full, fx)\n",
    "\n",
    "        # split-back into train valid test\n",
    "        train_cat = full.iloc[0:train.shape[0],:]\n",
    "        valid_cat = full.iloc[train.shape[0]:train.shape[0]+valid.shape[0],:]\n",
    "        test_cat  = full.iloc[train.shape[0]+valid.shape[0]:,:]    \n",
    "\n",
    "        # save to file\n",
    "        print (\"Saving processed files *.cat.bz2\")\n",
    "        train_cat.to_csv(atk_train_file+\".cat.bz2\", compression=\"bz2\", index=False)\n",
    "        valid_cat.to_csv(atk_valid_file+\".cat.bz2\", compression=\"bz2\", index=False)\n",
    "        test_cat.to_csv (atk_test_file+\".cat.bz2\",  compression=\"bz2\", index=False)\n",
    "        \n",
    "    else:\n",
    "        print (\"Loading pre-processed files...\")\n",
    "\n",
    "        train_cat = pd.read_csv(atk_train_file+\".cat.bz2\")\n",
    "        valid_cat = pd.read_csv(atk_valid_file+\".cat.bz2\")\n",
    "        test_cat  = pd.read_csv(atk_test_file+\".cat.bz2\")\n",
    "    \n",
    "    # return data\n",
    "    return train_cat, valid_cat, test_cat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdvBoosting_gen_data(model, data, groups):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    groups : grouping of same attacked instance \n",
    "    returns the new data matrix and new groups\n",
    "    \n",
    "    WARNING: currently works only for binary classification\n",
    "    '''\n",
    "    # score the datataset\n",
    "    labels = data[:,-1]\n",
    "    \n",
    "    predictions = model.predict(data[:,:-1]) # exclude labels\n",
    "    # binarize\n",
    "    predictions = (predictions>0).astype(np.float)\n",
    "    predictions = 2*predictions - 1\n",
    "    \n",
    "    # check mispredictions\n",
    "    matchings = labels * predictions\n",
    "    \n",
    "    # select original data + attacked instances\n",
    "    new_selected = [] # id of selected instances\n",
    "    new_groups   = []\n",
    "    \n",
    "    offset = 0\n",
    "    for g in groups:\n",
    "        if g==1:\n",
    "            # there are not attacks, just add original\n",
    "            new_selected += [offset]\n",
    "            new_groups   += [1]\n",
    "        else:\n",
    "            # get a slice of the matching scores\n",
    "            g_matchings = matchings[offset:offset+g]\n",
    "\n",
    "            # most misclassified (smallest margin)\n",
    "            # skip original\n",
    "            adv_instance = np.argmin(g_matchings[1:])+1\n",
    "\n",
    "            # add original and adversarial\n",
    "            new_selected += [offset, adv_instance]\n",
    "            new_groups   += [2]\n",
    "        \n",
    "        offset += g\n",
    "    \n",
    "    new_dataset = data[new_selected,:]\n",
    "    \n",
    "    return new_dataset, new_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom metrics\n",
    "def binary_log_loss(pred, true_label):\n",
    "\n",
    "    return np.log(1.0 + np.exp(-pred * true_label))\n",
    "\n",
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss(preds, train_data):\n",
    "    \n",
    "    labels = train_data.get_label()\n",
    "    losses = np.log(1.0 + np.exp(-preds*labels))\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return 'avg_binary_log_loss', avg_loss, False\n",
    "\n",
    "\n",
    "def optimize_log_loss(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    exp_pl = np.exp(preds * labels)\n",
    "    # http://www.wolframalpha.com/input/?i=differentiate+log(1+%2B+exp(-kx)+)\n",
    "    grads = -labels / (1.0 +  exp_pl)  \n",
    "    # http://www.wolframalpha.com/input/?i=d%5E2%2Fdx%5E2+log(1+%2B+exp(-kx)+)\n",
    "    hess = labels**2 * exp_pl / (1.0 + exp_pl)**2 \n",
    "\n",
    "    # this is to optimize average logloss\n",
    "    norm = 1.0/len(preds)\n",
    "    grads *= norm\n",
    "    hess *= norm\n",
    "    \n",
    "    return grads, hess\n",
    "\n",
    "\n",
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss_uma(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    offset = 0\n",
    "    max_logloss = []\n",
    "    avg_max_logloss = 0.0\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "    \n",
    "        for atk in attack_lens:\n",
    "            losses = [binary_log_loss(h,t) for h,t in zip(preds[offset:offset+atk], labels[offset:offset+atk])]\n",
    "            max_logloss.append(max(losses))\n",
    "\n",
    "            offset += atk\n",
    "        \n",
    "        avg_max_logloss = np.mean(max_logloss)  \n",
    "\n",
    "    return 'avg_binary_log_loss_under_max_attack', avg_max_logloss, False\n",
    "\n",
    "def avg_non_interferent_log_loss(preds, train_data, alpha=1.0):\n",
    "    \n",
    "    # binary logloss under maximal attack\n",
    "    _, loss_uma, _    = avg_log_loss_uma(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    _, loss_plain, _  = avg_log_loss(preds, train_data)\n",
    "    \n",
    "    # combine the above two losses together\n",
    "    weighted_loss = alpha*loss_uma + (1.0-alpha)*loss_plain\n",
    "\n",
    "    return 'avg_non_interferent_log_loss [alpha={}]'.format(alpha), weighted_loss, False\n",
    "\n",
    "def optimize_log_loss_uma(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    grads = np.zeros_like(labels, dtype=np.float64)\n",
    "    hess = np.zeros_like(grads)\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "\n",
    "        norm = 1.0 / float(len(attack_lens))\n",
    "\n",
    "        offset = 0\n",
    "        for atk in attack_lens:\n",
    "            exp_pl = np.exp(- preds[offset:offset+atk] * labels[offset:offset+atk])\n",
    "\n",
    "            inv_sum = 1.0 / np.sum(1.0 + exp_pl)\n",
    "\n",
    "            x_grad = inv_sum * exp_pl\n",
    "\n",
    "            grads[offset:offset+atk] = norm * x_grad * (- labels[offset:offset+atk])\n",
    "            hess[offset:offset+atk]  = norm * x_grad * (1.0 - x_grad)\n",
    "\n",
    "            offset += atk    \n",
    "    \n",
    "    return grads, hess\n",
    "\n",
    "def optimize_non_interferent_log_loss(preds, train_data, alpha=1.0):\n",
    "    # binary logloss under maximal attack\n",
    "    grads_uma, hess_uma = optimize_log_loss_uma(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    grads_plain, hess_plain = optimize_log_loss(preds, train_data)\n",
    "    \n",
    "    # combine the above two losses together\n",
    "    grads = alpha*grads_uma + (1.0-alpha)*grads_plain\n",
    "    hess  = alpha*hess_uma  + (1.0-alpha)*hess_plain\n",
    "    \n",
    "    return grads, hess\n",
    "\n",
    "\n",
    "\n",
    "def AdvBoosting_extend_model(data, input_model=None, num_trees=1, params=None):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    returns the new model (is model modified inplace?)\n",
    "    '''\n",
    "    \n",
    "    if params is None:\n",
    "        params = {\n",
    "            'learning_rate': 0.1,\n",
    "            'num_leaves': 16,\n",
    "            'min_data_in_leaf': 20, #[1, 20]\n",
    "            'verbose': 0\n",
    "        }  \n",
    "\n",
    "    lgbm_train = lightgbm.Dataset(data=data[:,:-1], \n",
    "                                  label=data[:,-1])\n",
    "    \n",
    "    lgbm_info = {}\n",
    "    lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                num_boost_round = num_trees, \n",
    "                                init_model = input_model,\n",
    "                                fobj = optimize_log_loss, \n",
    "                                feval = avg_log_loss,\n",
    "                                evals_result = lgbm_info,\n",
    "                                valid_sets   = [lgbm_train], \n",
    "                                valid_names  = ['adv-train'],\n",
    "                                verbose_eval=10)\n",
    "\n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdvBoosting( atk_train, trees, \n",
    "                 output_model_file,\n",
    "                 partial_save=10, \n",
    "                 adv_rounds=1):\n",
    "    ''' \n",
    "    atk_data: full dataset including all valid attacks\n",
    "    atk_groups: lenght of each attack set\n",
    "    trees: total number of trees to be produced\n",
    "    adv_rounds: adversarial instance injecting frequency\n",
    "    '''\n",
    "    # get groups and remove instance ids\n",
    "    atk_groups = atk_train['instance_id'].value_counts().sort_index().values\n",
    "    atk_data   = atk_train.iloc[:,1:].values\n",
    "    \n",
    "    # train first trees\n",
    "    original_ids = np.cumsum(atk_groups[:-1])\n",
    "    original_ids = np.insert(original_ids, 0, 0)\n",
    "    \n",
    "    model, model_info = AdvBoosting_extend_model( atk_data[original_ids, :], \n",
    "                                                  input_model=None, \n",
    "                                                  num_trees=adv_rounds, \n",
    "                                                  params=None )\n",
    "    \n",
    "    # train remaining trees\n",
    "    for t in range(2, trees+1, adv_rounds):\n",
    "        # attack dataset\n",
    "        adv_data, adv_offsets = AdvBoosting_gen_data( model, atk_data, atk_groups )\n",
    "        \n",
    "        # train additional trees\n",
    "        model, model_info = AdvBoosting_extend_model( adv_data, \n",
    "                                                      input_model=model, \n",
    "                                                      num_trees=adv_rounds, \n",
    "                                                      params=None)\n",
    "        # save partial model\n",
    "        if t%partial_save==0 and t!=trees:\n",
    "            partial_filename = \"{}.T{:03d}.lgbm\".format(output_model_file, t)\n",
    "            if os.path.exists(partial_filename):\n",
    "                os.remove(partial_filename)\n",
    "            model.save_model( filename=partial_filename )\n",
    "            \n",
    "    if os.path.exists(output_model_file):\n",
    "        os.remove(output_model_file)  \n",
    "    model.save_model(filename=output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_PATH = \"../data/census\"\n",
    "MODELS_PATH = \"../out/models\"\n",
    "ATTACKER = \"strong\" # weak\n",
    "\n",
    "\n",
    "TRAINING_SET=\"train_ori.csv.bz2\" # original training set\n",
    "TRAINING_SET_ATT=\"train_\"+ATTACKER+\"_att.csv.bz2\" # perturbed training set\n",
    "VALIDATION_SET=\"valid_ori.csv.bz2\" # original validation set\n",
    "VALIDATION_SET_ATT=\"valid_\"+ATTACKER+\"_att.csv.bz2\" # perturbed validation set\n",
    "TEST_SET=\"test_ori.csv.bz2\" # original test set\n",
    "TEST_SET_ATT=\"test_\"+ATTACKER+\"_att.csv.bz2\" # perturbed test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n"
     ]
    }
   ],
   "source": [
    "train, valid, test = load_atk_train_valid_test(  \n",
    "                                os.path.join(DATASETS_PATH, TRAINING_SET_ATT),\n",
    "                                os.path.join(DATASETS_PATH, VALIDATION_SET_ATT),\n",
    "                                os.path.join(DATASETS_PATH, TEST_SET_ATT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tadv-train's avg_binary_log_loss: 0.414062\n",
      "[20]\tadv-train's avg_binary_log_loss: 0.343473\n",
      "[30]\tadv-train's avg_binary_log_loss: 0.317184\n",
      "[40]\tadv-train's avg_binary_log_loss: 0.304094\n",
      "[50]\tadv-train's avg_binary_log_loss: 0.297048\n",
      "[60]\tadv-train's avg_binary_log_loss: 0.292382\n",
      "[70]\tadv-train's avg_binary_log_loss: 0.289018\n",
      "[80]\tadv-train's avg_binary_log_loss: 0.286211\n",
      "[90]\tadv-train's avg_binary_log_loss: 0.283709\n",
      "[100]\tadv-train's avg_binary_log_loss: 0.281838\n",
      "[110]\tadv-train's avg_binary_log_loss: 0.143443\n",
      "[120]\tadv-train's avg_binary_log_loss: 0.140765\n",
      "[130]\tadv-train's avg_binary_log_loss: 0.139608\n",
      "[140]\tadv-train's avg_binary_log_loss: 0.139014\n",
      "[150]\tadv-train's avg_binary_log_loss: 0.138433\n",
      "[160]\tadv-train's avg_binary_log_loss: 0.137954\n",
      "[170]\tadv-train's avg_binary_log_loss: 0.137515\n",
      "[180]\tadv-train's avg_binary_log_loss: 0.137123\n",
      "[190]\tadv-train's avg_binary_log_loss: 0.136724\n",
      "[200]\tadv-train's avg_binary_log_loss: 0.136319\n"
     ]
    }
   ],
   "source": [
    "adv_model = \"../out/models/adv_boosting.lgbm\"\n",
    "\n",
    "AdvBoosting(train, trees=100, output_model_file=adv_model, adv_rounds=100, partial_save=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 684K\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 283K Feb 16 09:52 adv_boosting.lgbm\r\n",
      "-rw-rw-r-- 1 lucchese lucchese  13K Feb 16 09:51 adv_boosting.lgbm.T009.lgbm\r\n",
      "-rw-rw-r-- 1 lucchese lucchese  12K Feb 16 09:51 adv_boosting.lgbm.T008.lgbm\r\n",
      "-rw-rw-r-- 1 lucchese lucchese  11K Feb 16 09:51 adv_boosting.lgbm.T007.lgbm\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 8.8K Feb 16 09:51 adv_boosting.lgbm.T006.lgbm\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 7.4K Feb 16 09:51 adv_boosting.lgbm.T005.lgbm\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 6.0K Feb 16 09:51 adv_boosting.lgbm.T004.lgbm\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 4.6K Feb 16 09:51 adv_boosting.lgbm.T003.lgbm\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 3.3K Feb 16 09:51 adv_boosting.lgbm.T002.lgbm\r\n",
      "-rw-rw-r-- 1 lucchese lucchese  85K Feb 16 09:50 adv_boosting.lgbm.T060.lgbm\r\n",
      "-rw-rw-r-- 1 lucchese lucchese  57K Feb 16 09:50 adv_boosting.lgbm.T040.lgbm\r\n",
      "-rw-rw-r-- 1 lucchese lucchese  29K Feb 16 09:50 adv_boosting.lgbm.T020.lgbm\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 139K Feb 12 09:41 std_strong_200.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lht ../out/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug running time\n",
    "#%load_ext line_profiler\n",
    "# %lprun -f AdvBoosting  AdvBoosting(train, trees=3)\n",
    "# %lprun -f AdvBoosting_gen_data  AdvBoosting(train, trees=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm ../out/models/*.lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
