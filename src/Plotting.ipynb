{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAMES = [\"census\", \"wine\", \"credit\"]\n",
    "EVAL_MODELS = dict([(dn, pd.read_csv(\"../out/eval/{}.csv\".format(dn), sep=\",\")) for dn in DATASET_NAMES])\n",
    "EVAL_METRICS = [\"Accuracy\", \"F1 Macro\", \"ROC AUC\"]\n",
    "\n",
    "DATASET_CLEAN_NAMES = {\"census\": \"census\", \"wine\":\"wine\", \"credit\": \"credit\"}\n",
    "\n",
    "STD_MODELS = [\"GBDT\", \"Random Forest\"]\n",
    "ROBUST_MODELS = [\"Adv Boosting\", \"Robust Trees\", \"RF-Treant\"]\n",
    "ALL_MODELS = STD_MODELS + ROBUST_MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add <code>Accuracy</code> column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in EVAL_MODELS:\n",
    "    eval_df = EVAL_MODELS[dataset]\n",
    "    eval_df['Accuracy'] = 1 - eval_df['Binary Err Rate']\n",
    "    colnames = eval_df.columns.tolist()\n",
    "    new_colnames = colnames[:4] + [colnames[-1]] + colnames[4:-1]\n",
    "    EVAL_MODELS[dataset] = eval_df[new_colnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize <code>Budget</code> values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in EVAL_MODELS:\n",
    "    eval_df = EVAL_MODELS[dataset]\n",
    "    eval_df['Training Budget Norm'] = round(eval_df['Training Budget']/eval_df['Training Budget'].max(), 2)\n",
    "    eval_df['Test Budget Norm'] = round(eval_df['Test Budget']/eval_df['Test Budget'].max(), 2)\n",
    "    colnames = eval_df.columns.tolist()\n",
    "    new_colnames = colnames[:2] + [colnames[-2]] + [colnames[2]] + [colnames[-1]] + colnames[3:-2]\n",
    "    EVAL_MODELS[dataset] = eval_df[new_colnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing performance deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_performance_dataset(eval_df, metrics):\n",
    "    \n",
    "    data = eval_df[(eval_df[\"Model\"].isin(ROBUST_MODELS)) &\n",
    "                   (eval_df[\"Training Budget\"] == eval_df[\"Test Budget\"]) &\n",
    "                   (eval_df[\"Training Budget\"] > 0)][[\"Model\", \"Training Budget\", \"Accuracy\", \"F1 Macro\", \"ROC AUC\"]]\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for budget in data[\"Training Budget\"].unique():\n",
    "        print(\"Considering budget B = {}\".format(budget))\n",
    "        data_b = data[data['Training Budget'] == budget]\n",
    "        for metric in metrics:\n",
    "            print(\"Evaluation metric = {}\".format(metric))\n",
    "            ab_metric = np.round(data_b[(data_b[\"Model\"] == \"Adv Boosting\")][metric].iloc[0], 3)\n",
    "            data_b[\"Delta {}\".format(metric)] = np.round(((np.round(data_b[metric], 3) - ab_metric) * 100) / ab_metric, 1)\n",
    "        all_data.append(data_b)\n",
    "\n",
    "    return pd.concat(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_performance(metrics):\n",
    "    \n",
    "    delta_performance_dfs = {}\n",
    "    \n",
    "    for dataset in EVAL_MODELS:\n",
    "        print(\"***** Examining dataset `{}` *****\".format(dataset))\n",
    "        delta_performance_dfs[dataset] = delta_performance_dataset(EVAL_MODELS[dataset], metrics)\n",
    "    \n",
    "    return delta_performance_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_performance_dfs = delta_performance(EVAL_METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_performance_dfs['credit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Security Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data, ax, x, metric, y_lims=None, show_y_label=True):\n",
    "    \n",
    "    #palette = [\"#f37736\", \"#3385c6\", \"#ee2e31\", \"#ffc425\", \"#009688\"]\n",
    "    palette = [\"#ee2e31\", \"#ffc425\", \"#009688\"]\n",
    "    \n",
    "    if x == \"Test Budget Norm\":\n",
    "        x_label = \"Test Attacker Budget\"\n",
    "    \n",
    "    if x == \"Training Budget Norm\":\n",
    "        x_label = \"Training Attacker Budget\"\n",
    "        #_ = ax.axhline(y=data[data[\"Model\"] == \"Random Forest\"][metric].values[0], ls=\"-\", lw=.75, color=\"#3385c6\")\n",
    "        #_ = ax.axhline(y=data[data[\"Model\"] == \"GBDT\"][metric].values[0], ls=\"--\", lw=.75, color=\"#f37736\")\n",
    "    \n",
    "    _ = sns.lineplot(x=x, \n",
    "                     y= metric, \n",
    "                     hue=\"Model\",\n",
    "                     #markers=[\"o\", \"X\", \"s\", \"P\", \"D\"],\n",
    "                     markers=[\"s\", \"P\", \"D\"],\n",
    "                     style=\"Model\",\n",
    "                     #style_order=[\"Random Forest\", \"GBDT\", \"RF-Treant\", \"Adv Boosting\", \"Robust Trees\"],\n",
    "                     style_order=[\"RF-Treant\", \"Adv Boosting\", \"Robust Trees\"],\n",
    "                     data=data,\n",
    "                     markersize=14,\n",
    "                     palette=palette,\n",
    "                     ax=ax\n",
    "                    )\n",
    "    \n",
    "    _ = ax.tick_params(axis = 'both', which = 'major', labelsize = 16)\n",
    "    _ = ax.set_xlabel(x_label, fontsize=18, labelpad=12)\n",
    "    \n",
    "    if show_y_label: \n",
    "        _ = ax.set_ylabel(metric, fontsize=18, labelpad=12)\n",
    "    else: \n",
    "        _ = ax.set_ylabel('')\n",
    "    \n",
    "    if y_lims:\n",
    "        _ = ax.set_ylim(y_lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_dataset(df, metrics, models, axes, x, max_budget, y_lims=None, show_y_label=True):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for m_i, m in enumerate(metrics):\n",
    "        for model in models:\n",
    "            # get the slice of the dataframe corresponding to the current model (e.g., \"GBDT\")\n",
    "            model_df = df.loc[df[\"Model\"] == model]\n",
    "            \n",
    "            data.append(model_df[model_df[max_budget] == model_df[max_budget].max()])\n",
    "        if y_lims:    \n",
    "            plot_data(pd.concat(data, axis=0), axes[m_i], x, m, y_lims[m_i], show_y_label)\n",
    "        else:\n",
    "            plot_data(pd.concat(data, axis=0), axes[m_i], x, m)\n",
    "    \n",
    "    plt.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(eval_models, eval_metrics, models, dataset_names, x, max_budget, y_lims=None):\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=3, \n",
    "                             ncols=3, \n",
    "                             sharex=True, \n",
    "                             sharey=y_lims==None, \n",
    "                             figsize=(18, 12))\n",
    "    \n",
    "    for j, dataset in enumerate(eval_models):\n",
    "        print(\"Generating evaluation subplot for dataset `{}`...\".format(dataset))\n",
    "        if y_lims:\n",
    "            plot_all_dataset(eval_models[dataset], eval_metrics, models, axes[:, j], x, max_budget, y_lims[j], j==0)\n",
    "        else:\n",
    "            plot_all_dataset(eval_models[dataset], eval_metrics, models, axes[:, j], x, max_budget)\n",
    "            \n",
    "        _ = axes[0, j].set_title(dataset_names[dataset], fontsize=20, fontname=\"Courier New\", pad=12)\n",
    "    \n",
    "    flat_axes = axes.flatten()\n",
    "    handles, labels = flat_axes[0].get_legend_handles_labels()\n",
    "    # remove legend title\n",
    "    handles = handles[1:]\n",
    "    labels = labels[1:]\n",
    "    \n",
    "    \n",
    "    for i, ax in enumerate(flat_axes):\n",
    "        ax.get_legend().remove()\n",
    "        \n",
    "    fig.legend(handles=[handles[2], handles[0], handles[1]], #[handles[-1], handles[2], handles[3], handles[1], handles[0]],\n",
    "               labels=[labels[2], labels[0], labels[1]], #[labels[-1], labels[2], labels[3], labels[1], labels[0]],\n",
    "               loc='upper center', \n",
    "               bbox_to_anchor=(0.5, 1.06), \n",
    "               fontsize=18,\n",
    "               fancybox=True, \n",
    "               shadow=True,\n",
    "               ncol=3,\n",
    "               markerscale=2.\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Plot max _training_ budget vs. many _test_ budgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing max _training_ budget, plot GBDT, RF, AB, RT, and RF-Treant when attacked with different _test_ budgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lims = [ [(0.55,.9),(0.45,.85),(.2,.95) ] ,\n",
    "           [(.5,.8),(.3,.8),(.25,.9) ] ,\n",
    "           [(.75,.84),(.42,.7),(.5,.8) ] ]\n",
    "y_lims = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(EVAL_MODELS, \n",
    "         EVAL_METRICS, \n",
    "         ROBUST_MODELS, #ALL_MODELS\n",
    "         DATASET_CLEAN_NAMES, \n",
    "         x=\"Test Budget Norm\", \n",
    "         max_budget=\"Training Budget\",\n",
    "         y_lims=y_lims\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Plot max _test_ budget vs. many _training_ budgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing max _test_ budget, plot GBDT, RF, AB, RT, and RF-Treant when attacked with different _test_ budgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lims = [ [(0.55,.9),(0.45,.8),(.2,.92) ] ,\n",
    "           [(.5,.75),(.3,.75),(.25,.85) ] ,\n",
    "           [(.7,.82),(.4,.65),(.5,.8) ] ]\n",
    "\n",
    "y_lims = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(EVAL_MODELS, \n",
    "         EVAL_METRICS, \n",
    "         ROBUST_MODELS, #ALL_MODELS \n",
    "         DATASET_CLEAN_NAMES, \n",
    "         x=\"Training Budget Norm\", \n",
    "         max_budget=\"Test Budget\",\n",
    "         y_lims=y_lims\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Adversarial Boosting vs. Robust Trees vs. RF-Treant vs. GBDT vs. RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_atk(data, ax, metric):\n",
    "    \n",
    "    palette = [\"#f37736\", \"#3385c6\", \"#ee2e31\", \"#ffc425\", \"#009688\"]\n",
    "    \n",
    "    _ = sns.lineplot(x=\"Test Budget Norm\", \n",
    "                     y= metric, \n",
    "                     hue=\"Model\",\n",
    "                     markers=[\"o\", \"X\", \"s\", \"P\", \"D\"],\n",
    "                     style=\"Model\",\n",
    "                     style_order=[\"Random Forest\", \"GBDT\", \"RF-Treant\", \"Adv Boosting\", \"Robust Trees\"],\n",
    "                     data=data,\n",
    "                     markersize=14,\n",
    "                     palette=palette,\n",
    "                     ax=ax\n",
    "                    )\n",
    "    \n",
    "    _ = ax.tick_params(axis = 'both', which = 'major', labelsize = 16)\n",
    "    _ = ax.set_xlabel(\"Test Attacker Budget\", fontsize=18, labelpad=12)\n",
    "    _ = ax.set_ylabel(metric, fontsize=18, labelpad=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_atk_dataset(df, metrics, models, axes):\n",
    "    \n",
    "    test_budgets = df[\"Test Budget\"].unique()\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for m_i, m in enumerate(metrics):\n",
    "        for b_i, b in enumerate(test_budgets):\n",
    "            \n",
    "            data.append(df.loc[(df[\"Model\"].isin(models)) & \n",
    "                               (df[\"Training Budget\"] == b) & \n",
    "                               (df[\"Test Budget\"] == b) |\n",
    "                               (df[\"Model\"] == \"GBDT\") |\n",
    "                               (df[\"Model\"] == \"Random Forest\") |\n",
    "                               (df[\"Test Budget\"] == 0)\n",
    "                              ]) # & (df[\"Test Budget\"] > 0) & (df[\"Test Budget\"] == b)])\n",
    "            \n",
    "        plot_atk(pd.concat(data, axis=0), axes[m_i], m)\n",
    "    \n",
    "    plt.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_atk_datasets(eval_models, eval_metrics, robust_models, dataset_names):\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=3, \n",
    "                             ncols=3, \n",
    "                             sharex=True, \n",
    "                             sharey=True, \n",
    "                             figsize=(18, 12))\n",
    "    \n",
    "    for j, dataset in enumerate(eval_models):\n",
    "        print(\"Generating evaluation subplot for dataset `{}`...\".format(dataset))\n",
    "        plot_atk_dataset(eval_models[dataset], eval_metrics, robust_models, axes[:, j])\n",
    "        _ = axes[0, j].set_title(dataset_names[dataset], fontsize=20, fontname=\"Courier New\")\n",
    "    \n",
    "    flat_axes = axes.flatten()\n",
    "    handles, labels = flat_axes[0].get_legend_handles_labels()\n",
    "    # remove legend title\n",
    "    handles = handles[1:]\n",
    "    labels = labels[1:]\n",
    "    \n",
    "    \n",
    "    for i, ax in enumerate(flat_axes):\n",
    "        ax.get_legend().remove()\n",
    "        \n",
    "    fig.legend(handles=[handles[-1], handles[2], handles[3], handles[1], handles[0]],\n",
    "               labels=[labels[-1], labels[2], labels[3], labels[1], labels[0]],\n",
    "               loc='upper center', \n",
    "               bbox_to_anchor=(0.5, 1.06), \n",
    "               fontsize=18,\n",
    "               fancybox=True, \n",
    "               shadow=True,\n",
    "               ncol=5,\n",
    "               markerscale=2.\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_atk_datasets(EVAL_MODELS, EVAL_METRICS, ALL_MODELS, DATASET_CLEAN_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Plot Standard GBDT vs. RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_std(data, ax, metric, y_lims=None, show_y_label=True):\n",
    "    \n",
    "    palette = [\"#f37736\", \"#3385c6\"] \n",
    "    \n",
    "    _ = sns.lineplot(x=\"Test Budget Norm\", \n",
    "                     y= metric, \n",
    "                     hue=\"Model\",\n",
    "                     markers=True,\n",
    "                     style=\"Model\",\n",
    "                     style_order=[\"Random Forest\", \"GBDT\"],\n",
    "                     data=data,\n",
    "                     markersize=18,\n",
    "                     palette=palette,\n",
    "                     ax=ax\n",
    "                    )\n",
    "    \n",
    "    _ = ax.tick_params(axis = 'both', which = 'major', labelsize = 22)\n",
    "    _ = ax.set_xlabel(\"Test Attacker Budget\", fontsize=26, labelpad=12)\n",
    "    \n",
    "    if show_y_label: \n",
    "        _ = ax.set_ylabel(metric, fontsize=26, labelpad=12)\n",
    "    else: \n",
    "        _ = ax.set_ylabel('')\n",
    "    \n",
    "    if y_lims:\n",
    "        _ = ax.set_ylim(y_lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_std_dataset(df, metrics, std_models, axes, y_lims=None, show_y_label=True):\n",
    "    \n",
    "    for m_i, m in enumerate(metrics):\n",
    "        if y_lims:\n",
    "            plot_std(df[(df['Model']=='GBDT') | (df['Model']=='Random Forest')], axes[m_i], m, y_lims[m_i], show_y_label)\n",
    "        else:\n",
    "            plot_std(df[(df['Model']=='GBDT') | (df['Model']=='Random Forest')], axes[m_i], m)\n",
    "    \n",
    "    plt.tight_layout(pad=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_std_datasets(eval_models, eval_metrics, std_models, dataset_names, y_lims=None):\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=3, \n",
    "                             ncols=3, \n",
    "                             sharex=True, \n",
    "                             sharey=y_lims==None, \n",
    "                             figsize=(18, 8))\n",
    "    \n",
    "    for j, dataset in enumerate(eval_models):\n",
    "        print(\"Generating evaluation subplot for dataset `{}`...\".format(dataset))\n",
    "        if y_lims:\n",
    "            plot_std_dataset(eval_models[dataset], eval_metrics, std_models, axes[:, j], y_lims[j], j==0)\n",
    "        else:\n",
    "            plot_std_dataset(eval_models[dataset], eval_metrics, std_models, axes[:, j])\n",
    "            \n",
    "        _ = axes[0, j].set_title(dataset_names[dataset], fontsize=28, fontname='Courier New', pad=12)\n",
    "    \n",
    "    flat_axes = axes.flatten()\n",
    "    handles, labels = flat_axes[0].get_legend_handles_labels()\n",
    "    # remove legend title\n",
    "    handles = handles[1:]\n",
    "    labels = labels[1:]\n",
    "    \n",
    "    for ax in flat_axes:\n",
    "        ax.get_legend().remove()  \n",
    "    \n",
    "    fig.legend(handles=handles[::-1], \n",
    "               labels=labels[::-1],\n",
    "               loc='upper center', \n",
    "               bbox_to_anchor=(0.5, 1.15), \n",
    "               fontsize=28,\n",
    "               fancybox=True, \n",
    "               shadow=True,\n",
    "               ncol=2,\n",
    "               markerscale=2.\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lims = [ [(0.5,.9),(0.4,.85),(.2,1) ] ,\n",
    "           [(.45,.8),(.25,.8),(.25,.85) ] ,\n",
    "           [(.7,.85),(.4,.7),(.5,.85) ] ]\n",
    "\n",
    "y_lims = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_std_datasets(EVAL_MODELS, EVAL_METRICS, STD_MODELS, DATASET_CLEAN_NAMES, y_lims=y_lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Plot Adversarial Boosting vs. Robust Trees vs. RF-Treant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_atk(data, ax, metric):\n",
    "    \n",
    "    palette = [\"#ee2e31\", \"#ffc425\", \"#009688\"] #ee4035 \"#4281a4\" \"#edc951\"\n",
    "    \n",
    "    _ = sns.lineplot(x=\"Test Budget Norm\", \n",
    "                     y= metric, \n",
    "                     hue=\"Model\",\n",
    "                     markers=[\"s\", \"P\", \"D\"],\n",
    "                     style=\"Model\",\n",
    "                     style_order=[\"RF-Treant\", \"Adv Boosting\", \"Robust Trees\"],\n",
    "                     data=data,\n",
    "                     markersize=14,\n",
    "                     palette=palette,\n",
    "                     ax=ax\n",
    "                    )\n",
    "    \n",
    "    _ = ax.tick_params(axis = 'both', which = 'major', labelsize = 16)\n",
    "    _ = ax.set_xlabel(\"Test Attacker Budget\", fontsize=18, labelpad=12)\n",
    "    _ = ax.set_ylabel(metric, fontsize=18, labelpad=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_atk_dataset(df, metrics, models, axes):\n",
    "    \n",
    "    test_budgets = df[\"Test Budget\"].unique() # [1:]\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for m_i, m in enumerate(metrics):\n",
    "        for b_i, b in enumerate(test_budgets):\n",
    "            # mask = []\n",
    "            # for rm in robust_models:\n",
    "                # mask.append(\"{} [train budget={}]\".format(rm, b))\n",
    "            # data.append(df.loc[(df[\"Model\"].isin(mask)) & (df[\"Budget\"] == b)].replace(regex=r' \\[train.*\\]', value=''))\n",
    "            data.append(df.loc[(df[\"Model\"].isin(models)) & (df[\"Training Budget\"] == b) & (df[\"Test Budget\"] > 0)]) # & (df[\"Test Budget\"] == b)])\n",
    "            \n",
    "        plot_atk(pd.concat(data, axis=0), axes[m_i], m)\n",
    "    \n",
    "    plt.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_atk_datasets(eval_models, eval_metrics, robust_models, dataset_names):\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=3, \n",
    "                             ncols=3, \n",
    "                             sharex=True, \n",
    "                             sharey=True, \n",
    "                             figsize=(18, 12))\n",
    "    \n",
    "    for j, dataset in enumerate(eval_models):\n",
    "        print(\"Generating evaluation subplot for dataset `{}`...\".format(dataset))\n",
    "        plot_atk_dataset(eval_models[dataset], eval_metrics, robust_models, axes[:, j])\n",
    "        _ = axes[0, j].set_title(dataset_names[dataset], fontsize=20, fontname=\"Courier New\")\n",
    "    \n",
    "    flat_axes = axes.flatten()\n",
    "    handles, labels = flat_axes[0].get_legend_handles_labels()\n",
    "    # remove legend title\n",
    "    handles = handles[1:]\n",
    "    labels = labels[1:]\n",
    "    \n",
    "    \n",
    "    for i, ax in enumerate(flat_axes):\n",
    "        ax.get_legend().remove()\n",
    "#         I was trying to adjust x_lim for the `census` dataset but set_xlim/set_ylim has a \"global\" effect\n",
    "#         since we are using sharex=True and sharey=True\n",
    "#         print(ax.get_xlim())\n",
    "#         if i % 3 == 0:\n",
    "#             ax.set_xlim(0.2, 1.05)\n",
    "#         print(ax.get_xlim())\n",
    "        \n",
    "    fig.legend(handles=[handles[2], handles[0], handles[1]], \n",
    "               labels=[labels[2], labels[0], labels[1]],\n",
    "               loc='upper center', \n",
    "               bbox_to_anchor=(0.5, 1.06), \n",
    "               fontsize=18,\n",
    "               fancybox=True, \n",
    "               shadow=True,\n",
    "               ncol=3,\n",
    "               markerscale=2.\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_atk_datasets(EVAL_MODELS, EVAL_METRICS, ROBUST_MODELS, DATASET_CLEAN_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Efficiency-Robustness Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tradeoff(data, ax, metric):\n",
    "    \n",
    "    palette = [\"#f37736\", \"#3385c6\", \"#ee2e31\", \"#ffc425\", \"#009688\"]\n",
    "    #palette = [\"#f37736\", \"#3385c6\", \"#009688\"] \n",
    "    \n",
    "    _ = sns.lineplot(x=\"Training Budget Norm\", \n",
    "                     y= metric, \n",
    "                     hue=\"Model\",\n",
    "                     markers=True,\n",
    "                     style=\"Model\",\n",
    "                     #style_order=[\"Random Forest\", \"GBDT\", \"RF-Treant\"],\n",
    "                     style_order=[\"Random Forest\", \"GBDT\", \"RF-Treant\", \"Adv Boosting\", \"Robust Trees\"],\n",
    "                     data=data,\n",
    "                     markersize=14,\n",
    "                     palette=palette,\n",
    "                     ax=ax\n",
    "                    )\n",
    "    \n",
    "    _ = ax.tick_params(axis = 'both', which = 'major', labelsize = 16)\n",
    "    _ = ax.set_xlabel(\"Train Attacker Budget\", fontsize=18, labelpad=12)\n",
    "    _ = ax.set_ylabel(metric, fontsize=18, labelpad=12)\n",
    "    # draw two horizontal lines for standard models (GBDT and RF)\n",
    "    _ = ax.axhline(y=data[data[\"Model\"] == \"Random Forest\"][metric].values[0], ls=\"-\", lw=.5, color=\"#3385c6\")\n",
    "    _ = ax.axhline(y=data[data[\"Model\"] == \"GBDT\"][metric].values[0], ls=\"--\", lw=.5, color=\"#f37736\")\n",
    "    _ = ax.set_ylim(0.6, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tradeoff_dataset(df, metrics, models, axes):\n",
    "    \n",
    "    for m_i, m in enumerate(metrics):   \n",
    "        plot_tradeoff(df[(df[\"Model\"].isin(models)) & (df['Test Budget']==0)], axes[m_i], m)\n",
    "    \n",
    "    plt.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_tradeoff_datasets(eval_models, eval_metrics, models, dataset_names):\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=3, \n",
    "                             ncols=3, \n",
    "                             sharex=True, \n",
    "                             sharey=True, \n",
    "                             figsize=(18, 10))\n",
    "    \n",
    "    for j, dataset in enumerate(eval_models):\n",
    "        print(\"Generating evaluation subplot for dataset `{}`...\".format(dataset))\n",
    "        plot_tradeoff_dataset(eval_models[dataset], eval_metrics, models, axes[:, j])\n",
    "        _ = axes[0, j].set_title(dataset_names[dataset], fontsize=20, fontname=\"Courier New\")\n",
    "        \n",
    "    flat_axes = axes.flatten()\n",
    "    handles, labels = flat_axes[0].get_legend_handles_labels()\n",
    "    # remove legend title\n",
    "    handles = handles[1:]\n",
    "    labels = labels[1:]\n",
    "    \n",
    "    \n",
    "    for ax in flat_axes:\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "    fig.legend(handles=handles,#[handles[1], handles[0], handles[-1], handles[2], handles[3]], #handles,\n",
    "               labels=labels,#[labels[1], labels[0], labels[-1], labels[2], labels[3]], # labels,\n",
    "               loc='upper center', \n",
    "               bbox_to_anchor=(0.5, 1.06), \n",
    "               fontsize=18,\n",
    "               fancybox=True, \n",
    "               shadow=True,\n",
    "               ncol=5,\n",
    "               markerscale=2.\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = STD_MODELS + [\"RF-Treant\"]\n",
    "MODELS = ALL_MODELS\n",
    "\n",
    "plot_all_tradeoff_datasets(EVAL_MODELS, EVAL_METRICS, MODELS, DATASET_CLEAN_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD VERSION!!! Plot GBDT vs. RF under attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #d11141 #00b159 #00aedb #f37735 #ffc425\n",
    "\n",
    "def plot_standard(eval_df, dataset_name, eval_metrics, ax):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    atk_budgets = eval_df['Budget Norm'].unique()\n",
    "    #fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "    palette = ['#d11141', '#00b159'] #, '#00aedb', '#f37735', '#ffc425', '#8874a3']\n",
    "    palette = ['#00aedb', '#d11141']\n",
    "    ls = ['-', '--']\n",
    "    markers=['^', 's','o']\n",
    "    for i, metric in enumerate(eval_metrics):\n",
    "        _ = sns.lineplot(x=\"Budget Norm\", \n",
    "                        y= metric, \n",
    "                        hue=\"Model\",\n",
    "                        markers=[markers[i], markers[i]],\n",
    "                        style=\"Model\",\n",
    "                        style_order=[\"GBDT\", \"Random Forest\"],\n",
    "                        palette=palette, #[i*2:i*2+2],\n",
    "                        data=eval_df[(eval_df['Model']=='GBDT') | (eval_df['Model']=='Random Forest')],\n",
    "                        markersize=12,\n",
    "                        ax=ax)\n",
    "        #_ = ax.set_xlabel(\"attacker's budget\", fontsize=18, labelpad=12)\n",
    "        #_ = ax.set_ylabel(\"metric\", fontsize=18, labelpad=12)\n",
    "        _ = ax.tick_params(axis = 'both', which = 'major', labelsize = 16)\n",
    "        _ = ax.set_title(dataset_name, fontsize=22)\n",
    "        legend_elements = [Line2D([0], [0], color='#00aedb', lw=2),\n",
    "                           Line2D([0], [0], color='#d11141', lw=2, ls=\"--\"),\n",
    "                           Line2D([0], [0], color='black', lw=1, marker=\"^\", markersize=8), #, mew=3),\n",
    "                           Line2D([0], [0], color='black', lw=1, marker=\"s\", markersize=8),\n",
    "                           Line2D([0], [0], color='black', lw=1, marker=\"o\", markersize=8)\n",
    "                          ]\n",
    "        \n",
    "        leg = ax.legend(title='Models', \n",
    "                      loc='best', \n",
    "                      handles=legend_elements, \n",
    "                      labels=['GBDT', 'Random Forest', 'Accuracy', 'F1 Macro', 'ROC AUC'],\n",
    "                      borderpad=1,\n",
    "                      fontsize=16)\n",
    "        leg.get_title().set_fontsize(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_standard(eval_dict, eval_metrics):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, sharex=True, sharey=True, figsize=(18,8))\n",
    "    i = 0\n",
    "    for dataset_name in eval_dict:\n",
    "        print(\"Plotting GBDT vs. RF under attack for dataset `{}`...\".format(dataset_name))\n",
    "        plot_standard(eval_dict[dataset_name], dataset_name, eval_metrics, axes[i])\n",
    "        axes[i].set_xlabel(\"attacker's budget\", fontsize=20, labelpad=12)          \n",
    "        if i == 0:\n",
    "            axes[i].set_ylabel(\"metric\", fontsize=20, labelpad=12)\n",
    "        i += 1\n",
    "    plt.tight_layout(pad=2.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_all_standard(EVAL_MODELS, EVAL_METRICS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
