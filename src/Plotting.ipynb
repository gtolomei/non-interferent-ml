{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAMES = [\"census\", \"wine\", \"credit\"]\n",
    "EVAL_MODELS = dict([(dn, pd.read_csv(\"../out/eval/{}.csv\".format(dn), sep=\",\")) for dn in DATASET_NAMES])\n",
    "EVAL_METRICS = [\"Accuracy\", \"F1 Macro\", \"ROC AUC\"]\n",
    "\n",
    "DATASET_CLEAN_NAMES = {\"census\": \"Census Dataset\", \"wine\":\"Wine Dataset\", \"credit\": \"Credit Dataset\"}\n",
    "\n",
    "STD_MODELS = [\"GBDT\", \"Random Forest\"]\n",
    "ROBUST_MODELS = [\"Adv Boosting\", \"Robust Trees\", \"RF-Treant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add <code>Accuracy</code> column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in EVAL_MODELS:\n",
    "    eval_df = EVAL_MODELS[dataset]\n",
    "    eval_df['Accuracy'] = 1 - eval_df['Binary Err Rate']\n",
    "    colnames = eval_df.columns.tolist()\n",
    "    new_colnames = colnames[:3] + [colnames[-1]] + colnames[3:-1]\n",
    "    EVAL_MODELS[dataset] = eval_df[new_colnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize <code>Budget</code> values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in EVAL_MODELS:\n",
    "    eval_df = EVAL_MODELS[dataset]\n",
    "    eval_df['Budget Norm'] = round(eval_df['Budget']/eval_df['Budget'].max(), 2)\n",
    "    colnames = eval_df.columns.tolist()\n",
    "    new_colnames = colnames[:2] + [colnames[-1]] + colnames[2:-1]\n",
    "    EVAL_MODELS[dataset] = eval_df[new_colnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Plot Standard GBDT vs. RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_std(data, ax, metric):\n",
    "    \n",
    "    palette = [\"#f37736\", \"#3385c6\"] \n",
    "    \n",
    "    _ = sns.lineplot(x=\"Budget Norm\", \n",
    "                     y= metric, \n",
    "                     hue=\"Model\",\n",
    "                     markers=True,\n",
    "                     style=\"Model\",\n",
    "                     style_order=[\"Random Forest\", \"GBDT\"],\n",
    "                     data=data,\n",
    "                     markersize=14,\n",
    "                     palette=palette,\n",
    "                     ax=ax\n",
    "                    )\n",
    "    \n",
    "    _ = ax.tick_params(axis = 'both', which = 'major', labelsize = 16)\n",
    "    _ = ax.set_xlabel(\"Attacker Budget\", fontsize=18, labelpad=12)\n",
    "    _ = ax.set_ylabel(metric, fontsize=18, labelpad=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_std_dataset(df, metrics, std_models, axes):\n",
    "    \n",
    "    for m_i, m in enumerate(metrics):   \n",
    "        plot_std(df[(df['Model']=='GBDT') | (df['Model']=='Random Forest')], axes[m_i], m)\n",
    "    \n",
    "    plt.tight_layout(pad=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_std_datasets(eval_models, eval_metrics, std_models, dataset_names):\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=3, \n",
    "                             ncols=3, \n",
    "                             sharex=True, \n",
    "                             sharey=True, \n",
    "                             figsize=(18, 8))\n",
    "    \n",
    "    for j, dataset in enumerate(eval_models):\n",
    "        print(\"Generating evaluation subplot for dataset `{}`...\".format(dataset))\n",
    "        plot_std_dataset(eval_models[dataset], eval_metrics, std_models, axes[:, j])\n",
    "        _ = axes[0, j].set_title(dataset_names[dataset], fontsize=20)\n",
    "    \n",
    "    flat_axes = axes.flatten()\n",
    "    handles, labels = flat_axes[0].get_legend_handles_labels()\n",
    "    # remove legend title\n",
    "    handles = handles[1:]\n",
    "    labels = labels[1:]\n",
    "    \n",
    "    for ax in flat_axes:\n",
    "        ax.get_legend().remove()  \n",
    "    \n",
    "    fig.legend(handles=handles[::-1], \n",
    "               labels=labels[::-1],\n",
    "               loc='upper center', \n",
    "               bbox_to_anchor=(0.5, 1.07), \n",
    "               fontsize=18,\n",
    "               fancybox=True, \n",
    "               shadow=True,\n",
    "               ncol=3,\n",
    "               markerscale=2.\n",
    "              )\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_std_datasets(EVAL_MODELS, EVAL_METRICS, STD_MODELS, DATASET_CLEAN_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Plot Adversarial Boosting vs. Robust Trees vs. RF-Treant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_atk(data, ax, metric):\n",
    "    \n",
    "    palette = [\"#ee2e31\", \"#ffc425\", \"#009688\"] #ee4035 \"#4281a4\" \"#edc951\"\n",
    "    \n",
    "    _ = sns.lineplot(x=\"Budget Norm\", \n",
    "                     y= metric, \n",
    "                     hue=\"Model\",\n",
    "                     markers=True,\n",
    "                     style=\"Model\",\n",
    "                     style_order=[\"RF-Treant\", \"Adv Boosting\", \"Robust Trees\"],\n",
    "                     data=data,\n",
    "                     markersize=14,\n",
    "                     palette=palette,\n",
    "                     ax=ax\n",
    "                    )\n",
    "    \n",
    "    _ = ax.tick_params(axis = 'both', which = 'major', labelsize = 16)\n",
    "    _ = ax.set_xlabel(\"Attacker Budget\", fontsize=18, labelpad=12)\n",
    "    _ = ax.set_ylabel(metric, fontsize=18, labelpad=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_atk_dataset(df, metrics, robust_models, axes):\n",
    "    \n",
    "    atk_budgets = df[\"Budget\"].unique()[1:]\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for m_i, m in enumerate(metrics):\n",
    "        for b_i, b in enumerate(atk_budgets):\n",
    "            mask = []\n",
    "            for rm in robust_models:\n",
    "                mask.append(\"{} [train budget={}]\".format(rm, b))\n",
    "            data.append(df.loc[(df[\"Model\"].isin(mask)) & (df[\"Budget\"] == b)].replace(regex=r' \\[train.*\\]', value=''))\n",
    "            \n",
    "        plot_atk(pd.concat(data, axis=0), axes[m_i], m)\n",
    "    \n",
    "    plt.tight_layout(pad=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_atk_datasets(eval_models, eval_metrics, robust_models, dataset_names):\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=3, \n",
    "                             ncols=3, \n",
    "                             sharex=True, \n",
    "                             sharey=True, \n",
    "                             figsize=(18, 10))\n",
    "    \n",
    "    for j, dataset in enumerate(eval_models):\n",
    "        print(\"Generating evaluation subplot for dataset `{}`...\".format(dataset))\n",
    "        plot_atk_dataset(eval_models[dataset], eval_metrics, robust_models, axes[:, j])\n",
    "        _ = axes[0, j].set_title(dataset_names[dataset], fontsize=20)\n",
    "    \n",
    "    flat_axes = axes.flatten()\n",
    "    handles, labels = flat_axes[0].get_legend_handles_labels()\n",
    "    # remove legend title\n",
    "    handles = handles[1:]\n",
    "    labels = labels[1:]\n",
    "    \n",
    "    for ax in flat_axes:\n",
    "        ax.get_legend().remove()  \n",
    "    \n",
    "    fig.legend(handles=[handles[2], handles[0], handles[1]], \n",
    "               labels=[labels[2], labels[0], labels[1]],\n",
    "               loc='upper center', \n",
    "               bbox_to_anchor=(0.5, 1.06), \n",
    "               fontsize=18,\n",
    "               fancybox=True, \n",
    "               shadow=True,\n",
    "               ncol=3,\n",
    "               markerscale=2.\n",
    "              )\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_atk_datasets(EVAL_MODELS, EVAL_METRICS, ROBUST_MODELS, DATASET_CLEAN_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD VERSION!!! Plot GBDT vs. RF under attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #d11141 #00b159 #00aedb #f37735 #ffc425\n",
    "\n",
    "def plot_standard(eval_df, dataset_name, eval_metrics, ax):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    atk_budgets = eval_df['Budget Norm'].unique()\n",
    "    #fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "    palette = ['#d11141', '#00b159'] #, '#00aedb', '#f37735', '#ffc425', '#8874a3']\n",
    "    palette = ['#00aedb', '#d11141']\n",
    "    ls = ['-', '--']\n",
    "    markers=['^', 's','o']\n",
    "    for i, metric in enumerate(eval_metrics):\n",
    "        _ = sns.lineplot(x=\"Budget Norm\", \n",
    "                        y= metric, \n",
    "                        hue=\"Model\",\n",
    "                        markers=[markers[i], markers[i]],\n",
    "                        style=\"Model\",\n",
    "                        style_order=[\"GBDT\", \"Random Forest\"],\n",
    "                        palette=palette, #[i*2:i*2+2],\n",
    "                        data=eval_df[(eval_df['Model']=='GBDT') | (eval_df['Model']=='Random Forest')],\n",
    "                        markersize=12,\n",
    "                        ax=ax)\n",
    "        #_ = ax.set_xlabel(\"attacker's budget\", fontsize=18, labelpad=12)\n",
    "        #_ = ax.set_ylabel(\"metric\", fontsize=18, labelpad=12)\n",
    "        _ = ax.tick_params(axis = 'both', which = 'major', labelsize = 16)\n",
    "        _ = ax.set_title(dataset_name, fontsize=22)\n",
    "        legend_elements = [Line2D([0], [0], color='#00aedb', lw=2),\n",
    "                           Line2D([0], [0], color='#d11141', lw=2, ls=\"--\"),\n",
    "                           Line2D([0], [0], color='black', lw=1, marker=\"^\", markersize=8), #, mew=3),\n",
    "                           Line2D([0], [0], color='black', lw=1, marker=\"s\", markersize=8),\n",
    "                           Line2D([0], [0], color='black', lw=1, marker=\"o\", markersize=8)\n",
    "                          ]\n",
    "        \n",
    "        leg = ax.legend(title='Models', \n",
    "                      loc='best', \n",
    "                      handles=legend_elements, \n",
    "                      labels=['GBDT', 'Random Forest', 'Accuracy', 'F1 Macro', 'ROC AUC'],\n",
    "                      borderpad=1,\n",
    "                      fontsize=16)\n",
    "        leg.get_title().set_fontsize(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_standard(eval_dict, eval_metrics):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, sharex=True, sharey=True, figsize=(18,8))\n",
    "    i = 0\n",
    "    for dataset_name in eval_dict:\n",
    "        print(\"Plotting GBDT vs. RF under attack for dataset `{}`...\".format(dataset_name))\n",
    "        plot_standard(eval_dict[dataset_name], dataset_name, eval_metrics, axes[i])\n",
    "        axes[i].set_xlabel(\"attacker's budget\", fontsize=20, labelpad=12)          \n",
    "        if i == 0:\n",
    "            axes[i].set_ylabel(\"metric\", fontsize=20, labelpad=12)\n",
    "        i += 1\n",
    "    plt.tight_layout(pad=2.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_all_standard(EVAL_MODELS, EVAL_METRICS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
