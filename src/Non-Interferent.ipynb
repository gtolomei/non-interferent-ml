{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Interferent Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    " - http://lightgbm.readthedocs.io/en/latest/\n",
    " - http://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
    " - https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "import pickle\n",
    "import json\n",
    "import functools\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nilib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path to dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"wine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR=\"../data/{}\".format(DATASET_NAME)\n",
    "MODELS_DIR=\"../out/models/{}\".format(DATASET_NAME)\n",
    "TRAINING_FILENAME=DATASET_DIR + \"/\" + \"train_ori.csv.bz2\"\n",
    "TRAINING_FILENAME_ATT=DATASET_DIR + \"/\" + \"train_B{}.csv.bz2\"\n",
    "VALIDATION_FILENAME=DATASET_DIR + \"/\" + \"valid_ori.csv.bz2\"\n",
    "VALIDATION_FILENAME_ATT=DATASET_DIR + \"/\" + \"valid_B{}.csv.bz2\"\n",
    "TEST_FILENAME=DATASET_DIR + \"/\" + \"test_ori.csv.bz2\"\n",
    "TEST_FILENAME_ATT=DATASET_DIR + \"/\" + \"test_B{}.csv.bz2\"\n",
    "MODEL_FILENAME=MODELS_DIR + \"/non-interferent_{}_B{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_BUDGETS=[20] #, 30, 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Interferent GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_non_interf_model(atk_train, atk_valid, cat_fx, \n",
    "                            alpha=1.0, num_trees=1, params=None):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    returns the new model (is model modified inplace?)\n",
    "    '''\n",
    "    \n",
    "    assert atk_train.shape[1]==atk_valid.shape[1], \"Train/Valid Mismatch!\"\n",
    "    \n",
    "    train_groups = atk_train['instance_id'].value_counts().sort_index().values\n",
    "    valid_groups = atk_valid['instance_id'].value_counts().sort_index().values\n",
    "\n",
    "    original_train_ids = np.cumsum(train_groups[:-1])\n",
    "    original_train_ids = np.insert(original_train_ids, 0, 0)\n",
    "    \n",
    "    original_valid_ids = np.cumsum(valid_groups[:-1])\n",
    "    original_valid_ids = np.insert(original_valid_ids, 0, 0)\n",
    "\n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(atk_train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])\n",
    "    print (\"CatFX:\", atk_train.columns.values[cat_fx])\n",
    "\n",
    "    # remove instance id\n",
    "    atk_train = atk_train.iloc[:,1:].values\n",
    "    atk_valid = atk_valid.iloc[:,1:].values\n",
    "    cat_fx = [x - 1 for x in cat_fx]\n",
    "        \n",
    "    unatk_train = atk_train[original_train_ids,:]\n",
    "    unatk_valid = atk_valid[original_valid_ids,:]\n",
    "\n",
    "    if cat_fx is None or len(cat_fx)==0:\n",
    "        cat_fx = \"auto\"\n",
    "\n",
    "    # -------------------------\n",
    "    # train first iteration\n",
    "    lgbm_train = lightgbm.Dataset(data=unatk_train[:,:-1], \n",
    "                                  label=unatk_train[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_valid = lightgbm.Dataset(data=atk_valid[:,:-1], \n",
    "                                  label=atk_valid[:,-1],\n",
    "                                  group=valid_groups,\n",
    "                                  categorical_feature = cat_fx)\n",
    "\n",
    "    lgbm_info = {}\n",
    "    lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                num_boost_round = 1,\n",
    "                                fobj  = optimize_log_loss, \n",
    "                                feval = functools.partial(avg_non_interferent_log_loss, alpha=alpha), #avg_log_loss\n",
    "                                evals_result = lgbm_info,\n",
    "                                valid_sets   = [lgbm_valid], \n",
    "                                valid_names  = ['valid'],\n",
    "                                verbose_eval=5)\n",
    "\n",
    "    # -------------------------\n",
    "    # train other iteration\n",
    "    def get_ni_w_old(preds, labels, groups):\n",
    "        # \"weights\"\n",
    "        w = np.ones(len(groups))\n",
    "\n",
    "        offset = 0\n",
    "        for instance_id, g in enumerate(groups):\n",
    "            exp_pl = np.exp(- preds[offset:offset+g] * labels[offset:offset+g])\n",
    "            w[instance_id] = 1.0 / np.max(1.0 + exp_pl)\n",
    "            offset += g  \n",
    "\n",
    "        return w\n",
    "    \n",
    "    def get_ni_w(preds, labels, groups):\n",
    "        # \"weights\"\n",
    "        w = np.ones(len(groups))\n",
    "\n",
    "        offset = 0\n",
    "        for instance_id, g in enumerate(groups):\n",
    "            exp_pl = np.exp(- preds[offset:offset+g] * labels[offset:offset+g])\n",
    "            # can we replace with e^max\n",
    "            w[instance_id] = 1.0 / np.max(1.0 + exp_pl)\n",
    "            offset += g  \n",
    "\n",
    "        return w\n",
    "    \n",
    "    def get_ni_w_num(preds, labels, groups):\n",
    "        # \"weights\"\n",
    "        w = np.ones(len(groups))\n",
    "\n",
    "        offset = 0\n",
    "        for instance_id, g in enumerate(groups):\n",
    "            exp_pl = np.exp(- preds[offset:offset+g] * labels[offset:offset+g])\n",
    "            w[instance_id] = 1.0 / np.sum(1.0 + exp_pl)\n",
    "            w[instance_id] *= np.exp(-2.0) \n",
    "            offset += g  \n",
    "\n",
    "        return w\n",
    "\n",
    "    for t in range (1, num_trees):\n",
    "    \n",
    "        # get predictions on atk instances\n",
    "        train_preds  = lgbm_model.predict(atk_train[:,:-1])\n",
    "        train_labels = atk_train[:,-1]\n",
    "        train_weights = get_ni_w(train_preds, train_labels, train_groups)\n",
    "                \n",
    "        # repeat for validation\n",
    "        valid_preds  = lgbm_model.predict(atk_valid[:,:-1])\n",
    "        valid_labels = atk_valid[:,-1]\n",
    "        valid_weights = get_ni_w(valid_preds, valid_labels, valid_groups)\n",
    "        \n",
    "        # prepare data and train\n",
    "        lgbm_train = lightgbm.Dataset(data=unatk_train[:,:-1], \n",
    "                                      label=unatk_train[:,-1],\n",
    "                                      weight=train_weights,\n",
    "                                      categorical_feature = cat_fx)\n",
    "\n",
    "        lgbm_valid = lightgbm.Dataset(data=atk_valid[:,:-1], \n",
    "                                      label=atk_valid[:,-1],\n",
    "                                      group=valid_groups,\n",
    "                                      categorical_feature = cat_fx)\n",
    "\n",
    "        new_lgbm_info = {}\n",
    "        lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                    num_boost_round = 1, \n",
    "                                    init_model = lgbm_model,\n",
    "                                    fobj  = functools.partial(optimize_non_interferent_log_loss, alpha=alpha), \n",
    "                                    feval = functools.partial(avg_non_interferent_log_loss, alpha=alpha), #avg_log_loss\n",
    "                                    evals_result = new_lgbm_info,\n",
    "                                    valid_sets   = [lgbm_valid], #[lgbm_train, lgbm_valid], \n",
    "                                    valid_names  = ['valid'],    #['train', 'valid'],\n",
    "                                    verbose_eval=5)\n",
    "        \n",
    "        awesome_hack = 'avg_non_interferent_log_loss [alpha={:.2f}]'.format(alpha)\n",
    "        lgbm_info['valid'][awesome_hack] += new_lgbm_info['valid'][awesome_hack]\n",
    "\n",
    "\n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_non_interferent(train_file, valid_file, test_file, output_model_file, alpha=1.0):\n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'alpha', 'best_round', 'avg_non_interferent_log_loss', 'filename'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    atk_train, atk_valid, atk_test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    \n",
    "    for num_trees in [10]:\n",
    "        best_model = None\n",
    "        best_info = None\n",
    "        best_loss = np.inf\n",
    "        awesome_hack = 'avg_non_interferent_log_loss [alpha={:.2f}]'.format(alpha)\n",
    "\n",
    "        for learning_rate in [0.01, 0.05, 0.1]:\n",
    "            for num_leaves in [8, 16, 24]:\n",
    "\n",
    "                lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                'num_leaves': num_leaves} \n",
    "                lgbm_model, lgbm_info = extend_non_interf_model(atk_train, atk_valid, cat_fx, \n",
    "                            alpha=alpha, num_trees=num_trees, params=lgbm_params)\n",
    "\n",
    "                # save file\n",
    "                print (lgbm_info)\n",
    "                best_valid_iter = np.argmin(lgbm_info['valid'][awesome_hack])\n",
    "\n",
    "                model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_A{:03d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                                    num_trees,\n",
    "                                                                                    int(learning_rate*1000),\n",
    "                                                                                    num_leaves,\n",
    "                                                                                    int(alpha * 100),\n",
    "                                                                                    best_valid_iter + 1\n",
    "                                                                                   )\n",
    "                # update experimental results\n",
    "                exp = exp.append({'num_trees': num_trees, \n",
    "                              'learning_rate':learning_rate,\n",
    "                              'num_leaves':num_leaves, \n",
    "                              'alpha': alpha,\n",
    "                              'best_round':best_valid_iter+1, \n",
    "                              'avg_non_interferent_log_loss':lgbm_info['valid'][awesome_hack][best_valid_iter],\n",
    "                              'filename':model_file_name},\n",
    "                             ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                lgbm_model.save_model(model_file_name)\n",
    "                print (\"Model saved to\", model_file_name)\n",
    "                    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n",
      "CatFX: []\n",
      "[5]\tvalid's avg_non_interferent_log_loss [alpha=1.00]: 0.686278\n",
      "[10]\tvalid's avg_non_interferent_log_loss [alpha=1.00]: 0.680404\n",
      "{'valid': defaultdict(<class 'list'>, {'avg_non_interferent_log_loss [alpha=1.00]': [0.6916419191447511, 0.6901792175736522, 0.68875790257787, 0.6873768403015927, 0.686278368017648, 0.6850437367050667, 0.6840346738286972, 0.6827921699583906, 0.6815782731292056, 0.6804036993210465]})}\n",
      "Model saved to ../out/models/wine/non-interferent_wine_B20_T10_S0010_L8_A100_R10.model\n",
      "CatFX: []\n",
      "[5]\tvalid's avg_non_interferent_log_loss [alpha=1.00]: 0.685034\n",
      "[10]\tvalid's avg_non_interferent_log_loss [alpha=1.00]: 0.67761\n",
      "{'valid': defaultdict(<class 'list'>, {'avg_non_interferent_log_loss [alpha=1.00]': [0.6916203519761026, 0.6900965677138187, 0.6883933065443634, 0.6867299428167076, 0.6850341793172037, 0.6833890788437711, 0.6821026724235246, 0.6805196358209405, 0.6790001401877684, 0.677610322379812]})}\n",
      "Model saved to ../out/models/wine/non-interferent_wine_B20_T10_S0010_L16_A100_R10.model\n",
      "CatFX: []\n",
      "[5]\tvalid's avg_non_interferent_log_loss [alpha=1.00]: 0.684562\n",
      "[10]\tvalid's avg_non_interferent_log_loss [alpha=1.00]: 0.676806\n",
      "{'valid': defaultdict(<class 'list'>, {'avg_non_interferent_log_loss [alpha=1.00]': [0.6913465714551082, 0.68955597263683, 0.6878514711031933, 0.6861929558739105, 0.6845623583956932, 0.6829431255132661, 0.6813717843658383, 0.6797407341876494, 0.6783538335452862, 0.6768058968588487]})}\n",
      "Model saved to ../out/models/wine/non-interferent_wine_B20_T10_S0010_L24_A100_R10.model\n",
      "CatFX: []\n",
      "[5]\tvalid's avg_non_interferent_log_loss [alpha=1.00]: 0.661505\n",
      "[10]\tvalid's avg_non_interferent_log_loss [alpha=1.00]: 0.633659\n",
      "{'valid': defaultdict(<class 'list'>, {'avg_non_interferent_log_loss [alpha=1.00]': [0.6859328890024912, 0.6804497507441006, 0.6746270119726914, 0.6675425354145242, 0.6615049485929587, 0.6560275217052017, 0.6506442113512626, 0.6434224643927343, 0.6388025700563773, 0.6336586283852547]})}\n",
      "Model saved to ../out/models/wine/non-interferent_wine_B20_T10_S0050_L8_A100_R10.model\n",
      "CatFX: []\n",
      "[5]\tvalid's avg_non_interferent_log_loss [alpha=1.00]: 0.658543\n",
      "[10]\tvalid's avg_non_interferent_log_loss [alpha=1.00]: 0.629355\n",
      "{'valid': defaultdict(<class 'list'>, {'avg_non_interferent_log_loss [alpha=1.00]': [0.6858458324829143, 0.6773620862557829, 0.6708113858661917, 0.6625640070006402, 0.6585429998003529, 0.6522737232461501, 0.6460108351166091, 0.6395592187759581, 0.6352213064550787, 0.6293550075652261]})}\n",
      "Model saved to ../out/models/wine/non-interferent_wine_B20_T10_S0050_L16_A100_R10.model\n",
      "CatFX: []\n",
      "[5]\tvalid's avg_non_interferent_log_loss [alpha=1.00]: 0.654189\n",
      "[10]\tvalid's avg_non_interferent_log_loss [alpha=1.00]: 0.625417\n",
      "{'valid': defaultdict(<class 'list'>, {'avg_non_interferent_log_loss [alpha=1.00]': [0.6844891020538066, 0.6767829171672595, 0.6693077653847289, 0.6606849036250595, 0.6541891325487537, 0.6471817315859422, 0.6405831110270555, 0.6345437259260476, 0.6293985637795116, 0.6254166180852644]})}\n",
      "Model saved to ../out/models/wine/non-interferent_wine_B20_T10_S0050_L24_A100_R10.model\n",
      "CatFX: []\n",
      "[5]\tvalid's avg_non_interferent_log_loss [alpha=1.00]: 0.635411\n",
      "[10]\tvalid's avg_non_interferent_log_loss [alpha=1.00]: 0.603417\n",
      "{'valid': defaultdict(<class 'list'>, {'avg_non_interferent_log_loss [alpha=1.00]': [0.6794977187645488, 0.6695949480815145, 0.6580460249045411, 0.6475121179756901, 0.6354113803214734, 0.6276231090313326, 0.6172727517659878, 0.6155489282567786, 0.6078551224058675, 0.6034169030739405]})}\n",
      "Model saved to ../out/models/wine/non-interferent_wine_B20_T10_S0100_L8_A100_R10.model\n",
      "CatFX: []\n",
      "[5]\tvalid's avg_non_interferent_log_loss [alpha=1.00]: 0.630365\n",
      "[10]\tvalid's avg_non_interferent_log_loss [alpha=1.00]: 0.593031\n",
      "{'valid': defaultdict(<class 'list'>, {'avg_non_interferent_log_loss [alpha=1.00]': [0.6793754340644556, 0.6659755744616802, 0.6514915246443921, 0.6416044788904058, 0.6303646694107107, 0.617744430119946, 0.6118990470760989, 0.604032046479056, 0.5980108522324792, 0.5930307490521711]})}\n",
      "Model saved to ../out/models/wine/non-interferent_wine_B20_T10_S0100_L16_A100_R10.model\n",
      "CatFX: []\n",
      "[5]\tvalid's avg_non_interferent_log_loss [alpha=1.00]: 0.625923\n",
      "[10]\tvalid's avg_non_interferent_log_loss [alpha=1.00]: 0.591373\n",
      "{'valid': defaultdict(<class 'list'>, {'avg_non_interferent_log_loss [alpha=1.00]': [0.6766923545210857, 0.6620271967098637, 0.6470754413623857, 0.6362813907208087, 0.6259225720273496, 0.6171289156056097, 0.6093404667031893, 0.6013705915894425, 0.5969228857368728, 0.5913730806363569]})}\n",
      "Model saved to ../out/models/wine/non-interferent_wine_B20_T10_S0100_L24_A100_R10.model\n",
      "  num_trees  learning_rate num_leaves  alpha best_round  \\\n",
      "0        10           0.01          8    1.0         10   \n",
      "1        10           0.01         16    1.0         10   \n",
      "2        10           0.01         24    1.0         10   \n",
      "3        10           0.05          8    1.0         10   \n",
      "4        10           0.05         16    1.0         10   \n",
      "5        10           0.05         24    1.0         10   \n",
      "6        10           0.10          8    1.0         10   \n",
      "7        10           0.10         16    1.0         10   \n",
      "8        10           0.10         24    1.0         10   \n",
      "\n",
      "   avg_non_interferent_log_loss  \\\n",
      "0                      0.680404   \n",
      "1                      0.677610   \n",
      "2                      0.676806   \n",
      "3                      0.633659   \n",
      "4                      0.629355   \n",
      "5                      0.625417   \n",
      "6                      0.603417   \n",
      "7                      0.593031   \n",
      "8                      0.591373   \n",
      "\n",
      "                                            filename  \n",
      "0  ../out/models/wine/non-interferent_wine_B20_T1...  \n",
      "1  ../out/models/wine/non-interferent_wine_B20_T1...  \n",
      "2  ../out/models/wine/non-interferent_wine_B20_T1...  \n",
      "3  ../out/models/wine/non-interferent_wine_B20_T1...  \n",
      "4  ../out/models/wine/non-interferent_wine_B20_T1...  \n",
      "5  ../out/models/wine/non-interferent_wine_B20_T1...  \n",
      "6  ../out/models/wine/non-interferent_wine_B20_T1...  \n",
      "7  ../out/models/wine/non-interferent_wine_B20_T1...  \n",
      "8  ../out/models/wine/non-interferent_wine_B20_T1...  \n",
      "best model is: ../out/models/wine/non-interferent_wine_B20_T10_S0100_L24_A100_R10.model\n"
     ]
    }
   ],
   "source": [
    "for B in TRAINING_BUDGETS:\n",
    "\n",
    "        experiments = train_non_interferent(TRAINING_FILENAME_ATT.format(B),\n",
    "                                            VALIDATION_FILENAME_ATT.format(B),\n",
    "                                            TEST_FILENAME_ATT.format(B),\n",
    "                                            MODEL_FILENAME.format(DATASET_NAME, B))  \n",
    "\n",
    "        experiments.to_csv(MODEL_FILENAME.format(DATASET_NAME, B) + \".csv\", index=False)\n",
    "\n",
    "        print(experiments)\n",
    "        print ('best model is:', experiments.sort_values('avg_non_interferent_log_loss').iloc[0]['filename'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
