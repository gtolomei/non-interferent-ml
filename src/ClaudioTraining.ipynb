{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models\n",
    "\n",
    "This notebook contains the code used for training the following learning models:\n",
    "\n",
    "-  **Standard GBDT** (_baseline 1_)\n",
    "-  **Adversarial Boosting** (_baseline 2_)\n",
    "-  **Non-Interferent GBDT** (our proposal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    " - http://lightgbm.readthedocs.io/en/latest/\n",
    " - http://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
    " - https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "import pickle\n",
    "import json\n",
    "import functools\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(dataset, categorical_features):\n",
    "    dataset_le = dataset.copy()\n",
    "    for column in dataset_le.columns:\n",
    "        if column in categorical_features:\n",
    "            dataset_le[column] = dataset_le[column].astype('category')\n",
    "            dataset_le[column] = dataset_le[column].cat.codes.astype(np.int32)\n",
    "    return dataset_le\n",
    "\n",
    "def load_atk_train_valid_test(atk_train_file, atk_valid_file, atk_test_file, \n",
    "                              train_split=0.6, valid_split=0.2, force=False):\n",
    "    \n",
    "    \n",
    "    if  (force or \n",
    "          not os.path.exists(atk_train_file+\".cat.bz2\") or\n",
    "          not os.path.exists(atk_valid_file+\".cat.bz2\") or\n",
    "          not os.path.exists(atk_test_file+\".cat.bz2\") or \n",
    "          not os.path.exists(atk_train_file+\".cat.json\") ):\n",
    "    \n",
    "        print (\"Pre-processing original files...\")\n",
    "\n",
    "        print (\"Loading:\", atk_train_file)\n",
    "        print (\"Loading:\", atk_valid_file)\n",
    "        print (\"Loading:\", atk_test_file)\n",
    "\n",
    "        train = pd.read_csv(atk_train_file)\n",
    "        valid = pd.read_csv(atk_valid_file)\n",
    "        test  = pd.read_csv(atk_test_file)\n",
    "        \n",
    "        print (\"Train/Valid/Test sizes:\", train.shape, valid.shape, test.shape)\n",
    "        print (\"Train/Valid/Test split: {:.2f} {:.2f} {:.2f}\"\n",
    "                   .format( train.shape[0]/(train.shape[0]+valid.shape[0]+test.shape[0]),\n",
    "                            valid.shape[0]/(train.shape[0]+valid.shape[0]+test.shape[0]),\n",
    "                            test.shape[0] /(train.shape[0]+valid.shape[0]+test.shape[0]) ) )\n",
    "\n",
    "\n",
    "        # split-back into train valid test\n",
    "        if 'instance_id' in train.columns.values:\n",
    "            print ('   ... with instance ids')\n",
    "            valid['instance_id'] += train.iloc[-1,0]\n",
    "            test['instance_id']  += valid.iloc[-1,0]\n",
    "            assert max(train['instance_id'])<min(valid['instance_id']), \"Instance ID mismatch\"\n",
    "            assert max(valid['instance_id'])<min(test['instance_id']), \"Instance ID mismatch\"\n",
    "            \n",
    "            groups = np.concatenate( [ train['instance_id'].value_counts().sort_index().values,\n",
    "                                       valid['instance_id'].value_counts().sort_index().values,\n",
    "                                       test['instance_id'].value_counts().sort_index().values ] )\n",
    "            \n",
    "            num_train_groups = int( len(groups)*train_split )\n",
    "            train_size = sum(groups[:num_train_groups])\n",
    "            num_valid_groups = int( len(groups)*valid_split )\n",
    "            valid_size = sum(groups[num_train_groups:num_train_groups+num_valid_groups])\n",
    "        else:\n",
    "            full_size = len(train) + len(valid) + len(test)\n",
    "            train_size = int( full_size*train_split )\n",
    "            valid_size = int( full_size*valid_split )\n",
    "        \n",
    "        # concat to process correctly label encoding\n",
    "        full = pd.concat( [train, valid, test] )\n",
    "\n",
    "        # get index of categorical features (-1 because of instance_id)\n",
    "        cat_fx = full.columns.values[np.where(full.dtypes=='object')[0]]\n",
    "        cat_fx = list(cat_fx)    \n",
    "        full = label_encode(full, cat_fx)\n",
    "        with open(atk_train_file+\".cat.json\", 'w') as fp:\n",
    "            json.dump(cat_fx, fp)\n",
    "        print (\"CatFX:\", cat_fx)\n",
    "\n",
    "        train_cat = full.iloc[0:train_size,:]\n",
    "        valid_cat = full.iloc[train_size:train_size+valid_size,:]\n",
    "        test_cat  = full.iloc[train_size+valid_size:,:]\n",
    "        \n",
    "        assert len(train_cat)+len(valid_cat)+len(test_cat)==len(full), \"Split sizes mismatch\"\n",
    "        \n",
    "\n",
    "        print (\"Train/Valid/Test sizes:\", train_cat.shape, valid_cat.shape, test_cat.shape)\n",
    "        print (\"Train/Valid/Test split: {:.2f} {:.2f} {:.2f}\"\n",
    "                   .format( train_cat.shape[0]/(train_cat.shape[0]+valid_cat.shape[0]+test_cat.shape[0]),\n",
    "                            valid_cat.shape[0]/(train_cat.shape[0]+valid_cat.shape[0]+test_cat.shape[0]),\n",
    "                            test_cat.shape[0] /(train_cat.shape[0]+valid_cat.shape[0]+test_cat.shape[0]) ) )\n",
    "\n",
    "        # save to file\n",
    "        print (\"Saving processed files *.cat.bz2\")\n",
    "        train_cat.to_csv(atk_train_file+\".cat.bz2\", compression=\"bz2\", index=False)\n",
    "        valid_cat.to_csv(atk_valid_file+\".cat.bz2\", compression=\"bz2\", index=False)\n",
    "        test_cat.to_csv (atk_test_file+\".cat.bz2\",  compression=\"bz2\", index=False)\n",
    "        \n",
    "    else:\n",
    "        print (\"Loading pre-processed files...\")\n",
    "\n",
    "        train_cat = pd.read_csv(atk_train_file+\".cat.bz2\")\n",
    "        valid_cat = pd.read_csv(atk_valid_file+\".cat.bz2\")\n",
    "        test_cat  = pd.read_csv(atk_test_file+\".cat.bz2\")\n",
    "        \n",
    "        with open(atk_train_file+\".cat.json\", 'r') as fp:\n",
    "            cat_fx = json.load(fp)\n",
    "    \n",
    "    # return data\n",
    "    return train_cat, valid_cat, test_cat, cat_fx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard\n",
    "\n",
    "The following function, called <code>optimize_log_loss</code>, is the one that should be optimized (i.e., minimized) for learning _standard_ and _baseline_ approaches. More specifically, this is the standard binary log loss which is used to train any _standard_ or _baseline_ model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $L$ = <code>optimize_log_loss</code>\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}}\\ell(h(\\mathbf{x}), y)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\ell(h(\\mathbf{x}), y) = log(1+e^{(-yh(\\mathbf{x}))})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_log_loss(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    exp_pl = np.exp(preds * labels)\n",
    "    # http://www.wolframalpha.com/input/?i=differentiate+log(1+%2B+exp(-kx)+)\n",
    "    grads = -labels / (1.0 +  exp_pl)  \n",
    "    # http://www.wolframalpha.com/input/?i=d%5E2%2Fdx%5E2+log(1+%2B+exp(-kx)+)\n",
    "    hess = labels**2 * exp_pl / (1.0 + exp_pl)**2 \n",
    "\n",
    "    # this is to optimize average logloss\n",
    "    norm = 1.0/len(preds)\n",
    "    grads *= norm\n",
    "    hess *= norm\n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom\n",
    "\n",
    "In addition to the standard binary log loss used to train a model, we introduce our custom <code>optimize_non_interferent_log_loss</code>, which is computed as the weighted combination of two objective functions, as follows:\n",
    "\n",
    "-  $L$ = <code>optimize_log_loss</code> (standard, already seen above);\n",
    "-  $L^A$ = <code>optimize_log_loss_uma</code> (custom, defined below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $L^A$ = <code>optimize_log_loss_uma</code>\n",
    "\n",
    "This function is used to train a **full** _non-interferent_ model; in other words, full non-interferent models are learned by optimizing (i.e., minimizing) the function which measures the binary log loss **under the maximal attack** possible.\n",
    "\n",
    "$$\n",
    "L^A = \\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}} \\log  \\left( \\sum_{\\mathbf{x}' \\in \\mathit{MaxAtk}({\\mathbf{x}},{A})} e^{\\ell(h(\\mathbf{x}'), y)} \\right).\n",
    "$$\n",
    "\n",
    "where still:\n",
    "\n",
    "$$\n",
    "\\ell(h(\\mathbf{x}), y) = log(1+e^{(-yh(\\mathbf{x}))})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_log_loss_uma(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    grads = np.zeros_like(labels, dtype=np.float64)\n",
    "    hess = np.zeros_like(grads)\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "\n",
    "        norm = 1.0 / float(len(attack_lens))\n",
    "\n",
    "        offset = 0\n",
    "        for atk in attack_lens:\n",
    "            exp_pl = np.exp(- preds[offset:offset+atk] * labels[offset:offset+atk])\n",
    "\n",
    "            inv_sum = 1.0 / np.sum(1.0 + exp_pl)\n",
    "\n",
    "            x_grad = inv_sum * exp_pl\n",
    "\n",
    "            grads[offset:offset+atk] = norm * x_grad * (- labels[offset:offset+atk])\n",
    "            hess[offset:offset+atk]  = norm * x_grad * (1.0 - x_grad)\n",
    "\n",
    "            offset += atk    \n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_log_loss_uma_ext(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    weights = train_data.get_weight()\n",
    "    \n",
    "    grads = np.zeros_like(labels, dtype=np.float64)\n",
    "    hess = np.zeros_like(grads)\n",
    "    \n",
    "    norm = 1.0 / float(len(labels))\n",
    "\n",
    "    exp_pl = np.exp(- preds * labels)\n",
    "\n",
    "    x_grad = weights * exp_pl\n",
    "\n",
    "    grads = norm * x_grad * (- labels)\n",
    "    hess  = norm * x_grad * (1.0 - x_grad)\n",
    "\n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>optimize_non_interferent_log_loss</code>\n",
    "\n",
    "$$\n",
    "\\alpha\\cdot L^A + (1-\\alpha)\\cdot L\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha \\cdot \\underbrace{\\Bigg[\\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}} \\log  \\left( \\sum_{\\mathbf{x}' \\in \\mathit{MaxAtk}({\\mathbf{x}},{A})} e^{\\ell(h(\\mathbf{x}'), y)} \\right)\\Bigg]}_{L^A} + (1-\\alpha) \\cdot \\underbrace{\\Bigg[\\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}} \\ell(h(\\mathbf{x}, y))\\Bigg]}_{L}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_non_interferent_log_loss(preds, train_data, alpha=1.0):\n",
    "    # binary logloss under maximal attack\n",
    "    # grads_uma, hess_uma = optimize_log_loss_uma(preds, train_data)\n",
    "    grads_uma, hess_uma = optimize_log_loss_uma_ext(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    grads_plain, hess_plain = optimize_log_loss(preds, train_data)\n",
    "    \n",
    "    #print (\"uma:   \", grads_uma.min(), grads_uma.max(), hess_uma.min(), hess_uma.max())\n",
    "    #print (\"plain: \", grads_plain.min(), grads_plain.max(), hess_plain.min(), hess_plain.max())\n",
    "    #print (\"uma:   \", np.quantile(grads_uma,[.25, .75]), np.quantile( hess_uma, [.25, .75]) )\n",
    "    #print (\"plain: \", np.quantile(grads_plain,[.25, .75]), np.quantile( hess_plain, [.25, .75]) )\n",
    "    \n",
    "    # combine the above two losses together\n",
    "    k=1\n",
    "    grads = alpha*grads_uma + (1.0-alpha)*grads_plain\n",
    "    hess  = alpha*hess_uma  + (1.0-alpha)*hess_plain\n",
    "#     grads *= k\n",
    "#     hess *= k\n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_non_interferent_log_loss_claudio(preds, train_data, alpha=1.0):\n",
    "    # binary logloss under maximal attack\n",
    "    grads_uma, hess_uma = optimize_log_loss_uma(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    grads_plain, hess_plain = optimize_log_loss(preds, train_data)\n",
    "    \n",
    "    # combine the above two losses together\n",
    "    grads = alpha*grads_uma + (1.0-alpha)*grads_plain\n",
    "    hess  = alpha*hess_uma  + (1.0-alpha)*hess_plain\n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using one objective function for both _standard_ and _non-interferent_ learning\n",
    "\n",
    "The advantage of the <code>optimize_non_interferent_log_loss</code> function defined above is that we can wrap it so that we can use it as the only objective function (<code>fobj</code>) passed in to LightGBM. \n",
    "\n",
    "In other words, if we call <code>fobj=optimize_non_interferent_log_loss</code> with <code>alpha=0.0</code>, this will end up optimizing (i.e., minimizing) the \"vanilla\" objective function (i.e., the standard binary log loss, defined by the function <code>optimize_log_loss</code> above).\n",
    "\n",
    "Conversely, calling <code>fobj=optimize_non_interferent_log_loss</code> with <code>alpha=1.0</code> turns into optimizing (i.e., minimizing) the full non-interferent objective function (i.e., the custom binary log loss under max attack, defined by the function <code>optimize_log_loss_uma</code> above).\n",
    "\n",
    "Anything that sits in between (i.e., <code>0 < alpha < 1</code>) optimizes an objective function that trades off between the standard and the full non-interferent term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard\n",
    "\n",
    "The following function is the one used for evaluating the quality of the learned model (either _standard_, _adversarial-boosting_, or _non-interferent_). This is the standard <code>avg_log_loss</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(p):\n",
    "    return np.log(p/(1-p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>avg_log_loss</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss(preds, train_data):\n",
    "    \n",
    "    labels = train_data.get_label()\n",
    "    losses = np.log(1.0 + np.exp(-preds*labels))\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return 'avg_binary_log_loss', avg_loss, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom\n",
    "\n",
    "Similarly to what we have done for <code>fobj</code>, <code>feval</code> can be computed from a weighted combination of two evaluation metrics:\n",
    "\n",
    "-  <code>avg_log_loss</code> (standard, defined above);\n",
    "-  <code>avg_log_loss_uma</code> (custom, defined below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>avg_log_loss_uma</code>\n",
    "\n",
    "This is the binary log loss yet modified to operate on groups of perturbed instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom metrics\n",
    "def binary_log_loss(pred, true_label):\n",
    "\n",
    "    return np.log(1.0 + np.exp(-pred * true_label))\n",
    "\n",
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss_uma(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    offset = 0\n",
    "    max_logloss = []\n",
    "    avg_max_logloss = 0.0\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "    \n",
    "        for atk in attack_lens:\n",
    "            losses = [binary_log_loss(h,t) for h,t in zip(preds[offset:offset+atk], labels[offset:offset+atk])]\n",
    "            max_logloss.append(max(losses))\n",
    "\n",
    "            offset += atk\n",
    "        \n",
    "        avg_max_logloss = np.mean(max_logloss)  \n",
    "\n",
    "    return 'avg_binary_log_loss_under_max_attack', avg_max_logloss, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>feval=avg_non_interferent_log_loss</code>\n",
    "\n",
    "Used for measuring the validity of any model (either _standard_, _baseline_, or _non-interferent_). More precisely, <code>avg_non_interferent_log_loss</code> is the weighted sum of the binary log loss and the binary log loss under maximal attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_non_interferent_log_loss(preds, train_data, alpha=1.0):\n",
    "    \n",
    "    # binary logloss under maximal attack\n",
    "    _, loss_uma, _    = avg_log_loss_uma(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    #_, loss_plain, _  = avg_log_loss(preds, train_data)\n",
    "    ids = []\n",
    "    attack_lens = train_data.get_group()\n",
    "    if attack_lens is not None:\n",
    "        offset=0\n",
    "        for atk in attack_lens:\n",
    "            ids += [offset]\n",
    "            offset += atk      \n",
    "            \n",
    "    ids = np.array(ids)\n",
    "    labels = train_data.get_label()\n",
    "    losses = binary_log_loss(pred=preds[ids], true_label=labels[ids])\n",
    "    loss_plain = np.mean(losses)\n",
    "\n",
    "    # combine the above two losses together\n",
    "    weighted_loss = alpha*loss_uma + (1.0-alpha)*loss_plain\n",
    "\n",
    "    return 'avg_non_interferent_log_loss [alpha={:.2f}]'.format(alpha), weighted_loss, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adv_boosting_data(model, data, groups, num_atks=1):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    groups : grouping of same attacked instance \n",
    "    returns the new data matrix and new groups\n",
    "    \n",
    "    WARNING: currently works only for binary classification\n",
    "    '''\n",
    "    # score the datataset\n",
    "    labels = data[:,-1]\n",
    "    \n",
    "    # check mispredictions\n",
    "    predictions = model.predict(data[:,:-1]) # exclude labels\n",
    "    matchings = labels * predictions\n",
    "    \n",
    "    # select original data + attacked instances\n",
    "    new_selected = [] # id of selected instances\n",
    "    new_groups   = []\n",
    "    \n",
    "    offset = 0\n",
    "    for g in groups:\n",
    "        if g==0:\n",
    "            print (\"Error !!!!\")\n",
    "        elif g==1:\n",
    "            # there are no attacks, just add original\n",
    "            new_selected += [offset]\n",
    "            new_groups   += [1]\n",
    "        else:\n",
    "            # get a slice of the matching scores\n",
    "            g_matchings = matchings[offset:offset+g]\n",
    "\n",
    "            # most misclassified (smallest margin)\n",
    "            # skip original\n",
    "            #adv_instance = np.argmin(g_matchings[1:])+1\n",
    "            adv_instances = np.argsort(g_matchings[1:])\n",
    "            adv_instances = adv_instances[:num_atks]\n",
    "            adv_instances += offset +1\n",
    "\n",
    "            # add original and adversarial\n",
    "            new_selected += [offset] + list(adv_instances)\n",
    "            new_groups   += [1 + len(adv_instances)]\n",
    "        \n",
    "        offset += g\n",
    "    \n",
    "    new_dataset = data[new_selected,:]\n",
    "    \n",
    "    return new_dataset, new_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_adv_boosting_model(train, valid, cat_fx, input_model=None, num_trees=1, params=None):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    returns the new model (is model modified inplace?)\n",
    "    '''\n",
    "    \n",
    "    if cat_fx is None or len(cat_fx)==0:\n",
    "        cat_fx = \"auto\"\n",
    "        \n",
    "    assert train.shape[1]==valid.shape[1], \"Train/Valid Mismatch!\"\n",
    "\n",
    "    lgbm_train = lightgbm.Dataset(data=train[:,:-1], \n",
    "                                  label=train[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_valid = lightgbm.Dataset(data=valid[:,:-1], \n",
    "                                  label=valid[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_info = {}\n",
    "    lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                num_boost_round = num_trees, \n",
    "                                init_model = input_model,\n",
    "                                fobj = optimize_log_loss, \n",
    "                                feval = avg_log_loss,\n",
    "                                evals_result = lgbm_info,\n",
    "                                valid_sets   = [lgbm_train, lgbm_valid], \n",
    "                                valid_names  = ['train', 'valid'],\n",
    "                                verbose_eval=5)\n",
    "\n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdvBoosting(atk_train, atk_valid, trees, \n",
    "                 cat_fx,\n",
    "                 params,\n",
    "                 output_model_file,\n",
    "                 partial_save=1000, \n",
    "                 adv_rounds=1):\n",
    "    ''' \n",
    "    atk_data: full dataset including all valid attacks\n",
    "    atk_groups: lenght of each attack set\n",
    "    trees: total number of trees to be produced\n",
    "    adv_rounds: adversarial instance injecting frequency\n",
    "    '''\n",
    "    # temp lgbm file\n",
    "    temp = output_model_file+\".tmp\"\n",
    "    \n",
    "    # get groups and remove instance ids\n",
    "    atk_groups = atk_train['instance_id'].value_counts().sort_index().values\n",
    "    atk_valid_groups = atk_valid['instance_id'].value_counts().sort_index().values\n",
    "    \n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(atk_train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])  \n",
    "    # print (\"CatFX:\", atk_train.columns.values[cat_fx])\n",
    "\n",
    "    # prepare data (avoiding pandas)\n",
    "    atk_data   = atk_train.iloc[:,1:].values\n",
    "    atk_valid  = atk_valid.iloc[:,1:].values\n",
    "    cat_fx = [x - 1 for x in cat_fx]\n",
    "\n",
    "    # train first trees\n",
    "    original_ids = np.cumsum(atk_groups[:-1])\n",
    "    original_ids = np.insert(original_ids, 0, 0)\n",
    "    \n",
    "    original_valid_ids = np.cumsum(atk_valid_groups[:-1])\n",
    "    original_valid_ids = np.insert(original_valid_ids, 0, 0)\n",
    "    \n",
    "    model, model_info = extend_adv_boosting_model(atk_data[original_ids, :], \n",
    "                                                  atk_valid[original_valid_ids, :],\n",
    "                                                  cat_fx=cat_fx,\n",
    "                                                  input_model=None, \n",
    "                                                  num_trees=adv_rounds, \n",
    "                                                  params=params)\n",
    "    \n",
    "    best_model = model\n",
    "    best_info = model_info\n",
    "    best_loss = np.min(model_info['valid']['avg_binary_log_loss'])\n",
    "    best_round = 1\n",
    "        \n",
    "    # train remaining trees\n",
    "    for t in range(adv_rounds+1, trees+1, adv_rounds):\n",
    "        # attack dataset\n",
    "        adv_data, _       = gen_adv_boosting_data(model, atk_data, atk_groups)\n",
    "        adv_valid_data, _ = gen_adv_boosting_data(model, atk_valid, atk_valid_groups)\n",
    "        \n",
    "        # train additional trees\n",
    "        model.save_model(temp)\n",
    "        model, model_info = extend_adv_boosting_model(adv_data, \n",
    "                                                      adv_valid_data,\n",
    "                                                      cat_fx=cat_fx,\n",
    "                                                      input_model=temp, \n",
    "                                                      num_trees=adv_rounds, \n",
    "                                                      params=params)\n",
    "\n",
    "        if np.min(model_info['valid']['avg_binary_log_loss']) < best_loss:\n",
    "            best_model = model\n",
    "            best_info  = model_info\n",
    "            best_loss  = np.min(model_info['valid']['avg_binary_log_loss'])\n",
    "            best_round = t\n",
    "        \n",
    "        # save partial model\n",
    "        if t % partial_save == 0 and t != trees:\n",
    "            partial_filename = \"{:s}_T{:d}-of-{:d}_S{:04d}_L{:d}.model.tmp\".format(output_model_file, \n",
    "                                                                                   t, \n",
    "                                                                                   trees, \n",
    "                                                                                   int(params['learning_rate'] * 1000),\n",
    "                                                                                   params['num_leaves']\n",
    "                                                                                  )\n",
    "            \n",
    "            print(\"Save partial model to {}\".format(partial_filename))\n",
    "            model.save_model(filename=partial_filename)\n",
    "            \n",
    "    \n",
    "    return model, model_info, best_loss, best_round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Standard GBDT (_baseline 1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gradient_boosting_baseline( train_file, valid_file, test_file,\n",
    "                                output_model_file):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'best_round', 'avg_binary_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    \n",
    "    assert \"instance_id\" not in train.columns.values, \"Wrong training set file for GBDT\"\n",
    "\n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])  \n",
    "    print (\"CatFX:\", train.columns.values[cat_fx])\n",
    "    \n",
    "\n",
    "    for num_trees in [500]:\n",
    "        best_model = None\n",
    "        best_info = None\n",
    "        best_loss = np.inf\n",
    "        for learning_rate in [0.1]: #[0.01, 0.05, 0.1]:\n",
    "            for num_leaves in [16]: #[8, 16, 24]:\n",
    "                # datasets\n",
    "                lgbm_train = lightgbm.Dataset(data=train.iloc[:,:-1].values, \n",
    "                                              label=train.iloc[:,-1].values,\n",
    "                                              categorical_feature = cat_fx)\n",
    "\n",
    "                lgbm_valid = lightgbm.Dataset(data=valid.iloc[:,:-1].values, \n",
    "                                              label=valid.iloc[:,-1].values,\n",
    "                                              categorical_feature = cat_fx)\n",
    "\n",
    "                # run train\n",
    "                lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                'num_leaves': num_leaves} \n",
    "                lgbm_info = {}\n",
    "                lgbm_model = lightgbm.train(lgbm_params, lgbm_train, \n",
    "                                            num_boost_round = num_trees,\n",
    "                                            fobj            = optimize_log_loss, \n",
    "                                            feval           = avg_log_loss,\n",
    "                                            evals_result    = lgbm_info,\n",
    "                                            valid_sets      = [lgbm_train, lgbm_valid], \n",
    "                                            valid_names     = ['train', 'valid'],\n",
    "                                            verbose_eval    = 5)\n",
    "                \n",
    "                if np.min(lgbm_info['valid']['avg_binary_log_loss']) < best_loss:\n",
    "                    best_model = lgbm_model\n",
    "                    best_info = lgbm_info\n",
    "                    best_loss = np.min(lgbm_info['valid']['avg_binary_log_loss'])\n",
    "                    best_info['num_trees'] = num_trees\n",
    "                    best_info['learning_rate'] = learning_rate\n",
    "                    best_info['num_leaves'] = num_leaves\n",
    "                    \n",
    "                    \n",
    "                best_valid_iter = np.argmin(lgbm_info['valid']['avg_binary_log_loss'])\n",
    "                \n",
    "                # update experimental results\n",
    "                exp = exp.append({'num_trees': num_trees, \n",
    "                                  'learning_rate':learning_rate,\n",
    "                                  'num_leaves':num_leaves, \n",
    "                                  'best_round':best_valid_iter+1, \n",
    "                                  'avg_binary_log_loss':lgbm_info['valid']['avg_binary_log_loss'][best_valid_iter]},\n",
    "                                 ignore_index=True)\n",
    "                \n",
    "        \n",
    "        # save file\n",
    "        best_valid_iter = np.argmin(best_info['valid']['avg_binary_log_loss'])\n",
    "\n",
    "        model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                        best_info['num_trees'],\n",
    "                                                                        int(best_info['learning_rate']*1000),\n",
    "                                                                        best_info['num_leaves'],\n",
    "                                                                        best_valid_iter + 1\n",
    "                                                                       )\n",
    "        \n",
    "        best_model.save_model(model_file_name)\n",
    "        print (\"Model saved to\", model_file_name)\n",
    "        \n",
    "        best_model = lightgbm.Booster(model_file=model_file_name)\n",
    "        print (\"Check valid score:\", avg_log_loss(preds=best_model.predict(valid.iloc[:,:-1].values),\n",
    "                                                  train_data=lgbm_valid))\n",
    "\n",
    "    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n",
      "CatFX: ['workclass' 'marital_status' 'occupation' 'relationship' 'race' 'sex'\n",
      " 'native_country']\n",
      "[5]\ttrain's avg_binary_log_loss: 0.5195\tvalid's avg_binary_log_loss: 0.520269\n",
      "[10]\ttrain's avg_binary_log_loss: 0.442694\tvalid's avg_binary_log_loss: 0.443485\n",
      "[15]\ttrain's avg_binary_log_loss: 0.404239\tvalid's avg_binary_log_loss: 0.404565\n",
      "[20]\ttrain's avg_binary_log_loss: 0.383888\tvalid's avg_binary_log_loss: 0.384591\n",
      "[25]\ttrain's avg_binary_log_loss: 0.372059\tvalid's avg_binary_log_loss: 0.372916\n",
      "[30]\ttrain's avg_binary_log_loss: 0.364054\tvalid's avg_binary_log_loss: 0.365226\n",
      "[35]\ttrain's avg_binary_log_loss: 0.359023\tvalid's avg_binary_log_loss: 0.360431\n",
      "[40]\ttrain's avg_binary_log_loss: 0.354723\tvalid's avg_binary_log_loss: 0.35629\n",
      "[45]\ttrain's avg_binary_log_loss: 0.346831\tvalid's avg_binary_log_loss: 0.349523\n",
      "[50]\ttrain's avg_binary_log_loss: 0.332285\tvalid's avg_binary_log_loss: 0.336143\n",
      "[55]\ttrain's avg_binary_log_loss: 0.326506\tvalid's avg_binary_log_loss: 0.331138\n",
      "[60]\ttrain's avg_binary_log_loss: 0.318182\tvalid's avg_binary_log_loss: 0.32389\n",
      "[65]\ttrain's avg_binary_log_loss: 0.312118\tvalid's avg_binary_log_loss: 0.318482\n",
      "[70]\ttrain's avg_binary_log_loss: 0.309225\tvalid's avg_binary_log_loss: 0.316324\n",
      "[75]\ttrain's avg_binary_log_loss: 0.308228\tvalid's avg_binary_log_loss: 0.315905\n",
      "[80]\ttrain's avg_binary_log_loss: 0.304639\tvalid's avg_binary_log_loss: 0.312861\n",
      "[85]\ttrain's avg_binary_log_loss: 0.302875\tvalid's avg_binary_log_loss: 0.311535\n",
      "[90]\ttrain's avg_binary_log_loss: 0.301099\tvalid's avg_binary_log_loss: 0.310236\n",
      "[95]\ttrain's avg_binary_log_loss: 0.298815\tvalid's avg_binary_log_loss: 0.308413\n",
      "[100]\ttrain's avg_binary_log_loss: 0.298178\tvalid's avg_binary_log_loss: 0.308143\n",
      "[105]\ttrain's avg_binary_log_loss: 0.296133\tvalid's avg_binary_log_loss: 0.306573\n",
      "[110]\ttrain's avg_binary_log_loss: 0.294962\tvalid's avg_binary_log_loss: 0.305781\n",
      "[115]\ttrain's avg_binary_log_loss: 0.294326\tvalid's avg_binary_log_loss: 0.305505\n",
      "[120]\ttrain's avg_binary_log_loss: 0.292473\tvalid's avg_binary_log_loss: 0.30415\n",
      "[125]\ttrain's avg_binary_log_loss: 0.291884\tvalid's avg_binary_log_loss: 0.303843\n",
      "[130]\ttrain's avg_binary_log_loss: 0.290919\tvalid's avg_binary_log_loss: 0.303309\n",
      "[135]\ttrain's avg_binary_log_loss: 0.289891\tvalid's avg_binary_log_loss: 0.302703\n",
      "[140]\ttrain's avg_binary_log_loss: 0.288578\tvalid's avg_binary_log_loss: 0.301621\n",
      "[145]\ttrain's avg_binary_log_loss: 0.288096\tvalid's avg_binary_log_loss: 0.301603\n",
      "[150]\ttrain's avg_binary_log_loss: 0.287229\tvalid's avg_binary_log_loss: 0.301281\n",
      "[155]\ttrain's avg_binary_log_loss: 0.286115\tvalid's avg_binary_log_loss: 0.300422\n",
      "[160]\ttrain's avg_binary_log_loss: 0.285523\tvalid's avg_binary_log_loss: 0.300122\n",
      "[165]\ttrain's avg_binary_log_loss: 0.28508\tvalid's avg_binary_log_loss: 0.300049\n",
      "[170]\ttrain's avg_binary_log_loss: 0.284396\tvalid's avg_binary_log_loss: 0.299671\n",
      "[175]\ttrain's avg_binary_log_loss: 0.284013\tvalid's avg_binary_log_loss: 0.299611\n",
      "[180]\ttrain's avg_binary_log_loss: 0.283122\tvalid's avg_binary_log_loss: 0.298957\n",
      "[185]\ttrain's avg_binary_log_loss: 0.282794\tvalid's avg_binary_log_loss: 0.298794\n",
      "[190]\ttrain's avg_binary_log_loss: 0.281846\tvalid's avg_binary_log_loss: 0.298127\n",
      "[195]\ttrain's avg_binary_log_loss: 0.281299\tvalid's avg_binary_log_loss: 0.297973\n",
      "[200]\ttrain's avg_binary_log_loss: 0.280975\tvalid's avg_binary_log_loss: 0.297961\n",
      "[205]\ttrain's avg_binary_log_loss: 0.280439\tvalid's avg_binary_log_loss: 0.297844\n",
      "[210]\ttrain's avg_binary_log_loss: 0.279836\tvalid's avg_binary_log_loss: 0.29764\n",
      "[215]\ttrain's avg_binary_log_loss: 0.279328\tvalid's avg_binary_log_loss: 0.297395\n",
      "[220]\ttrain's avg_binary_log_loss: 0.278995\tvalid's avg_binary_log_loss: 0.297354\n",
      "[225]\ttrain's avg_binary_log_loss: 0.278666\tvalid's avg_binary_log_loss: 0.297261\n",
      "[230]\ttrain's avg_binary_log_loss: 0.278107\tvalid's avg_binary_log_loss: 0.297029\n",
      "[235]\ttrain's avg_binary_log_loss: 0.277556\tvalid's avg_binary_log_loss: 0.296811\n",
      "[240]\ttrain's avg_binary_log_loss: 0.277261\tvalid's avg_binary_log_loss: 0.296731\n",
      "[245]\ttrain's avg_binary_log_loss: 0.277011\tvalid's avg_binary_log_loss: 0.296621\n",
      "[250]\ttrain's avg_binary_log_loss: 0.276761\tvalid's avg_binary_log_loss: 0.296578\n",
      "[255]\ttrain's avg_binary_log_loss: 0.276553\tvalid's avg_binary_log_loss: 0.296544\n",
      "[260]\ttrain's avg_binary_log_loss: 0.276254\tvalid's avg_binary_log_loss: 0.296493\n",
      "[265]\ttrain's avg_binary_log_loss: 0.276039\tvalid's avg_binary_log_loss: 0.296491\n",
      "[270]\ttrain's avg_binary_log_loss: 0.275801\tvalid's avg_binary_log_loss: 0.296568\n",
      "[275]\ttrain's avg_binary_log_loss: 0.275013\tvalid's avg_binary_log_loss: 0.296155\n",
      "[280]\ttrain's avg_binary_log_loss: 0.274656\tvalid's avg_binary_log_loss: 0.296143\n",
      "[285]\ttrain's avg_binary_log_loss: 0.274172\tvalid's avg_binary_log_loss: 0.296098\n",
      "[290]\ttrain's avg_binary_log_loss: 0.273919\tvalid's avg_binary_log_loss: 0.296115\n",
      "[295]\ttrain's avg_binary_log_loss: 0.273588\tvalid's avg_binary_log_loss: 0.296138\n",
      "[300]\ttrain's avg_binary_log_loss: 0.27334\tvalid's avg_binary_log_loss: 0.296146\n",
      "[305]\ttrain's avg_binary_log_loss: 0.273087\tvalid's avg_binary_log_loss: 0.296178\n",
      "[310]\ttrain's avg_binary_log_loss: 0.272691\tvalid's avg_binary_log_loss: 0.296188\n",
      "[315]\ttrain's avg_binary_log_loss: 0.272081\tvalid's avg_binary_log_loss: 0.295973\n",
      "[320]\ttrain's avg_binary_log_loss: 0.271861\tvalid's avg_binary_log_loss: 0.295991\n",
      "[325]\ttrain's avg_binary_log_loss: 0.271586\tvalid's avg_binary_log_loss: 0.296049\n",
      "[330]\ttrain's avg_binary_log_loss: 0.271186\tvalid's avg_binary_log_loss: 0.295909\n",
      "[335]\ttrain's avg_binary_log_loss: 0.270944\tvalid's avg_binary_log_loss: 0.295982\n",
      "[340]\ttrain's avg_binary_log_loss: 0.270492\tvalid's avg_binary_log_loss: 0.295852\n",
      "[345]\ttrain's avg_binary_log_loss: 0.27025\tvalid's avg_binary_log_loss: 0.295909\n",
      "[350]\ttrain's avg_binary_log_loss: 0.269972\tvalid's avg_binary_log_loss: 0.295878\n",
      "[355]\ttrain's avg_binary_log_loss: 0.269555\tvalid's avg_binary_log_loss: 0.295865\n",
      "[360]\ttrain's avg_binary_log_loss: 0.269356\tvalid's avg_binary_log_loss: 0.295909\n",
      "[365]\ttrain's avg_binary_log_loss: 0.269146\tvalid's avg_binary_log_loss: 0.295876\n",
      "[370]\ttrain's avg_binary_log_loss: 0.268957\tvalid's avg_binary_log_loss: 0.295838\n",
      "[375]\ttrain's avg_binary_log_loss: 0.268421\tvalid's avg_binary_log_loss: 0.29575\n",
      "[380]\ttrain's avg_binary_log_loss: 0.268071\tvalid's avg_binary_log_loss: 0.295547\n",
      "[385]\ttrain's avg_binary_log_loss: 0.267841\tvalid's avg_binary_log_loss: 0.295588\n",
      "[390]\ttrain's avg_binary_log_loss: 0.267625\tvalid's avg_binary_log_loss: 0.295608\n",
      "[395]\ttrain's avg_binary_log_loss: 0.267368\tvalid's avg_binary_log_loss: 0.295563\n",
      "[400]\ttrain's avg_binary_log_loss: 0.267179\tvalid's avg_binary_log_loss: 0.295593\n",
      "[405]\ttrain's avg_binary_log_loss: 0.266981\tvalid's avg_binary_log_loss: 0.295585\n",
      "[410]\ttrain's avg_binary_log_loss: 0.266742\tvalid's avg_binary_log_loss: 0.295662\n",
      "[415]\ttrain's avg_binary_log_loss: 0.266543\tvalid's avg_binary_log_loss: 0.295651\n",
      "[420]\ttrain's avg_binary_log_loss: 0.266376\tvalid's avg_binary_log_loss: 0.295654\n",
      "[425]\ttrain's avg_binary_log_loss: 0.266213\tvalid's avg_binary_log_loss: 0.295742\n",
      "[430]\ttrain's avg_binary_log_loss: 0.26605\tvalid's avg_binary_log_loss: 0.295791\n",
      "[435]\ttrain's avg_binary_log_loss: 0.265842\tvalid's avg_binary_log_loss: 0.29586\n",
      "[440]\ttrain's avg_binary_log_loss: 0.265644\tvalid's avg_binary_log_loss: 0.295774\n",
      "[445]\ttrain's avg_binary_log_loss: 0.265467\tvalid's avg_binary_log_loss: 0.295802\n",
      "[450]\ttrain's avg_binary_log_loss: 0.265313\tvalid's avg_binary_log_loss: 0.295829\n",
      "[455]\ttrain's avg_binary_log_loss: 0.265128\tvalid's avg_binary_log_loss: 0.295869\n",
      "[460]\ttrain's avg_binary_log_loss: 0.264844\tvalid's avg_binary_log_loss: 0.295832\n",
      "[465]\ttrain's avg_binary_log_loss: 0.26454\tvalid's avg_binary_log_loss: 0.295706\n",
      "[470]\ttrain's avg_binary_log_loss: 0.264352\tvalid's avg_binary_log_loss: 0.295737\n",
      "[475]\ttrain's avg_binary_log_loss: 0.264176\tvalid's avg_binary_log_loss: 0.295796\n",
      "[480]\ttrain's avg_binary_log_loss: 0.264\tvalid's avg_binary_log_loss: 0.295888\n",
      "[485]\ttrain's avg_binary_log_loss: 0.263842\tvalid's avg_binary_log_loss: 0.295865\n",
      "[490]\ttrain's avg_binary_log_loss: 0.263671\tvalid's avg_binary_log_loss: 0.295858\n",
      "[495]\ttrain's avg_binary_log_loss: 0.263494\tvalid's avg_binary_log_loss: 0.295857\n",
      "[500]\ttrain's avg_binary_log_loss: 0.263347\tvalid's avg_binary_log_loss: 0.295932\n",
      "Model saved to ../out/models/std_gbdt_census_T500_S0100_L16_R394.model\n",
      "Check valid score: ('avg_binary_log_loss', 0.2959318732349072, False)\n",
      "   num_trees  learning_rate  num_leaves  best_round  avg_binary_log_loss\n",
      "0      500.0            0.1        16.0       394.0             0.295525\n"
     ]
    }
   ],
   "source": [
    "# enable/disable LGBM Baseline\n",
    "if True:\n",
    "    experiments = train_gradient_boosting_baseline(\"../data/census/train_ori.csv.bz2\",\n",
    "                                                     \"../data/census/valid_ori.csv.bz2\",\n",
    "                                                     \"../data/census/test_ori.csv.bz2\",\n",
    "                                                     \"../out/models/std_gbdt_census\")  \n",
    "\n",
    "    experiments.to_csv('../out/models/std_gbdt_census.csv', index=False)\n",
    "\n",
    "    print (experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Adversarial Boosting (_baseline 2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial_boosting(train_file, valid_file, test_file, output_model_file):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'best_round', 'avg_binary_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    assert \"instance_id\" in train.columns.values, \"Wrong training set file for GBDT\"\n",
    "\n",
    "    for num_trees in [200]:\n",
    "        for learning_rate in [0.1]: #[0.01, 0.05, 0.1]:\n",
    "            for num_leaves in [16]: #[8, 16, 24]:\n",
    "                      \n",
    "                lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                'num_leaves': num_leaves} \n",
    "                \n",
    "                lgbm_model, lgbm_info, best_loss, best_valid_iter = AdvBoosting(train,\n",
    "                                                    valid,\n",
    "                                                    trees=num_trees, \n",
    "                                                    cat_fx = cat_fx, \n",
    "                                                    output_model_file=output_model_file, \n",
    "                                                    adv_rounds=1,\n",
    "                                                    params=lgbm_params)\n",
    "                \n",
    "\n",
    "                ####\n",
    "                #best_model = lightgbm.Booster(model_file=model_file_name)\n",
    "                atk_valid_groups = valid['instance_id'].value_counts().sort_index().values\n",
    "                cat_fx = np.where(valid.columns.isin(cat_fx))[0]\n",
    "                cat_fx = list([int(x) for x in cat_fx])  \n",
    "\n",
    "                atk_valid  = valid.iloc[:,1:].values\n",
    "                cat_fx = [x - 1 for x in cat_fx]\n",
    "                \n",
    "                original_valid_ids = np.cumsum(atk_valid_groups[:-1])\n",
    "                original_valid_ids = np.insert(original_valid_ids, 0, 0)\n",
    "\n",
    "                lgbm_valid = lightgbm.Dataset(data=atk_valid[original_valid_ids,:-1], \n",
    "                                              label=atk_valid[original_valid_ids,-1],\n",
    "                                              categorical_feature = cat_fx)\n",
    "                \n",
    "                \n",
    "                lgbm_valid_att = lightgbm.Dataset(data=atk_valid[:,:-1], \n",
    "                                              label=atk_valid[:,-1],\n",
    "                                              categorical_feature = cat_fx)\n",
    "                \n",
    "                print (\"Check valid score without attacks:\", avg_log_loss(preds=lgbm_model.predict(atk_valid[original_valid_ids,:-1]),\n",
    "                                                  train_data=lgbm_valid))\n",
    "                \n",
    "                print (\"Check valid score with attacks:\", avg_log_loss(preds=lgbm_model.predict(atk_valid[:,:-1]),\n",
    "                                                                       train_data=lgbm_valid_att))\n",
    "\n",
    "                \n",
    "                ####\n",
    "                # update experimental results\n",
    "                exp = exp.append({'num_trees': num_trees, \n",
    "                                  'learning_rate':learning_rate,\n",
    "                                  'num_leaves':num_leaves, \n",
    "                                  'best_round':best_valid_iter, \n",
    "                                  'avg_binary_log_loss':best_loss},\n",
    "                                 ignore_index=True)\n",
    "        \n",
    "        # save file\n",
    "        model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                                num_trees,\n",
    "                                                                                int(learning_rate*1000),\n",
    "                                                                                num_leaves,\n",
    "                                                                                best_valid_iter\n",
    "                                                                               )\n",
    "        lgbm_model.save_model(model_file_name)\n",
    "        print (\"Model saved to\", model_file_name)\n",
    "                \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# enable/disable\n",
    "if True:\n",
    "    for B in [150]: #[5, 15, 150, 300]:\n",
    "\n",
    "        experiments = train_adversarial_boosting ( \"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/test_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../out/models/adv_boosting_census_B{:d}\".format(B))  \n",
    "\n",
    "        experiments.to_csv('../out/models/adv_boosting_census_B{:d}.csv'.format(B), index=False)\n",
    "\n",
    "        print (experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Check valid score without attacks: ('avg_binary_log_loss', 0.3237495543276321, False)\n",
    "    Check valid score with attacks: ('avg_binary_log_loss', 0.03632538905070899, False)\n",
    "    Model saved to ../out/models/adv_boosting_census_B150_T200_S0100_L16_R200.model\n",
    "       num_trees  learning_rate  num_leaves  best_round  avg_binary_log_loss\n",
    "    0      200.0            0.1        16.0       200.0             0.224304"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Non-Interferent GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_non_interf_model(atk_train, atk_valid, cat_fx, \n",
    "                            alpha=1.0, num_trees=1, params=None):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    returns the new model (is model modified inplace?)\n",
    "    '''\n",
    "    \n",
    "    if cat_fx is None or len(cat_fx)==0:\n",
    "        cat_fx = \"auto\"\n",
    "        \n",
    "    assert atk_train.shape[1]==atk_valid.shape[1], \"Train/Valid Mismatch!\"\n",
    "    \n",
    "    train_groups = atk_train['instance_id'].value_counts().sort_index().values\n",
    "    valid_groups = atk_valid['instance_id'].value_counts().sort_index().values\n",
    "\n",
    "    original_train_ids = np.cumsum(train_groups[:-1])\n",
    "    original_train_ids = np.insert(original_train_ids, 0, 0)\n",
    "    \n",
    "    original_valid_ids = np.cumsum(valid_groups[:-1])\n",
    "    original_valid_ids = np.insert(original_valid_ids, 0, 0)\n",
    "\n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(atk_train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])\n",
    "    print (\"CatFX:\", atk_train.columns.values[cat_fx])\n",
    "\n",
    "    # remove instance id\n",
    "    atk_train = atk_train.iloc[:,1:].values\n",
    "    atk_valid = atk_valid.iloc[:,1:].values\n",
    "    cat_fx = [x - 1 for x in cat_fx]\n",
    "        \n",
    "    unatk_train = atk_train[original_train_ids,:]\n",
    "    unatk_valid = atk_valid[original_valid_ids,:]\n",
    "\n",
    "    \n",
    "    # -------------------------\n",
    "    # train first iteration\n",
    "    lgbm_train = lightgbm.Dataset(data=unatk_train[:,:-1], \n",
    "                                  label=unatk_train[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_valid = lightgbm.Dataset(data=unatk_valid[:,:-1], \n",
    "                                  label=unatk_valid[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "\n",
    "    lgbm_info = {}\n",
    "    lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                num_boost_round = 1,\n",
    "                                fobj  = optimize_log_loss, \n",
    "                                feval = avg_log_loss,\n",
    "                                evals_result = lgbm_info,\n",
    "                                valid_sets   = [lgbm_train, lgbm_valid], \n",
    "                                valid_names  = ['train', 'valid'],\n",
    "                                verbose_eval=5)\n",
    "\n",
    "    # -------------------------\n",
    "    # train other iteration\n",
    "    def get_ni_w_old(preds, labels, groups):\n",
    "        # \"weights\"\n",
    "        w = np.ones(len(groups))\n",
    "\n",
    "        offset = 0\n",
    "        for instance_id, g in enumerate(groups):\n",
    "            exp_pl = np.exp(- preds[offset:offset+g] * labels[offset:offset+g])\n",
    "            w[instance_id] = 1.0 / np.sum(1.0 + exp_pl)\n",
    "            offset += g  \n",
    "\n",
    "        return w\n",
    "    \n",
    "    def get_ni_w(preds, labels, groups):\n",
    "        # \"weights\"\n",
    "        w = np.ones(len(groups))\n",
    "\n",
    "        offset = 0\n",
    "        for instance_id, g in enumerate(groups):\n",
    "            exp_pl = np.exp(- preds[offset:offset+g] * labels[offset:offset+g])\n",
    "            # can we replace with e^max\n",
    "            w[instance_id] = 1.0 / np.max(1.0 + exp_pl)\n",
    "            offset += g  \n",
    "\n",
    "        return w\n",
    "    \n",
    "    def get_ni_w_num(preds, labels, groups):\n",
    "        # \"weights\"\n",
    "        w = np.ones(len(groups))\n",
    "\n",
    "        offset = 0\n",
    "        for instance_id, g in enumerate(groups):\n",
    "            exp_pl = np.exp(- preds[offset:offset+g] * labels[offset:offset+g])\n",
    "            w[instance_id] = 1.0 / np.sum(1.0 + exp_pl)\n",
    "            w[instance_id] *= np.exp(-2.0) \n",
    "            offset += g  \n",
    "\n",
    "        return w\n",
    "\n",
    "    for t in range (1, num_trees):\n",
    "    \n",
    "        # get predictions on atk instances\n",
    "        train_preds  = lgbm_model.predict(atk_train[:,:-1])\n",
    "        train_labels = atk_train[:,-1]\n",
    "        train_weights = get_ni_w(train_preds, train_labels, train_groups)\n",
    "                \n",
    "        # repeat for validation\n",
    "        valid_preds  = lgbm_model.predict(atk_valid[:,:-1])\n",
    "        valid_labels = atk_valid[:,-1]\n",
    "        valid_weights = get_ni_w(valid_preds, valid_labels, valid_groups)\n",
    "        \n",
    "        # prepare data and train\n",
    "        lgbm_train = lightgbm.Dataset(data=unatk_train[:,:-1], \n",
    "                                      label=unatk_train[:,-1],\n",
    "                                      weight=train_weights,\n",
    "                                      categorical_feature = cat_fx)\n",
    "\n",
    "        lgbm_valid = lightgbm.Dataset(data=unatk_valid[:,:-1], \n",
    "                                      label=unatk_valid[:,-1],\n",
    "                                      weight=valid_weights,\n",
    "                                      categorical_feature = cat_fx)\n",
    "\n",
    "        new_lgbm_info = {}\n",
    "        lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                    num_boost_round = 1, \n",
    "                                    init_model = lgbm_model,\n",
    "                                    fobj  = functools.partial(optimize_non_interferent_log_loss, alpha=alpha), \n",
    "                                    feval = avg_log_loss,# functools.partial(avg_non_interferent_log_loss, alpha=alpha),\n",
    "                                    evals_result = new_lgbm_info,\n",
    "                                    valid_sets   = [lgbm_train, lgbm_valid], \n",
    "                                    valid_names  = ['train', 'valid'],\n",
    "                                    verbose_eval=5)\n",
    "        \n",
    "        awesome_hack = \"avg_binary_log_loss\"\n",
    "        lgbm_info['train'][awesome_hack] += new_lgbm_info['train'][awesome_hack]\n",
    "        lgbm_info['valid'][awesome_hack] += new_lgbm_info['valid'][awesome_hack]\n",
    "\n",
    "\n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_non_interferent(train_file, valid_file, test_file, output_model_file):\n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'alpha', 'best_round', 'avg_non_interferent_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    atk_train, atk_valid, atk_test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    \n",
    "    for num_trees in [200]:\n",
    "        for alpha in [0.5]: #[0.25, 0.50, 0.75, 1.00]:\n",
    "            best_model = None\n",
    "            best_info = None\n",
    "            best_loss = np.inf\n",
    "            awesome_hack = \"avg_non_interferent_log_loss\" + \" [alpha={:.2f}]\".format(alpha)\n",
    "            awesome_hack = \"avg_binary_log_loss\"\n",
    "            \n",
    "            for learning_rate in [0.1]: #[0.01, 0.05, 0.1]:\n",
    "                for num_leaves in [24]: #[8, 16, 24, 32]:\n",
    "                    \n",
    "                    \n",
    "                    lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                    'num_leaves': num_leaves} \n",
    "                    lgbm_model, lgbm_info = extend_non_interf_model(atk_train, atk_valid, cat_fx, \n",
    "                                alpha=alpha, num_trees=num_trees, params=lgbm_params)\n",
    "                    \n",
    "                    if np.min(lgbm_info['valid'][awesome_hack]) < best_loss:\n",
    "                        best_model = lgbm_model\n",
    "                        best_info = lgbm_info\n",
    "                        best_loss = np.min(lgbm_info['valid'][awesome_hack])\n",
    "                        best_info['num_trees'] = num_trees\n",
    "                        best_info['learning_rate'] = learning_rate\n",
    "                        best_info['num_leaves'] = num_leaves\n",
    "                \n",
    "\n",
    "                    # save file\n",
    "\n",
    "                    best_valid_iter = np.argmin(lgbm_info['valid'][awesome_hack])\n",
    "\n",
    "                    # update experimental results\n",
    "                    exp = exp.append({'num_trees': num_trees, \n",
    "                                  'learning_rate':learning_rate,\n",
    "                                  'num_leaves':num_leaves, \n",
    "                                  'alpha': alpha,\n",
    "                                  'best_round':best_valid_iter+1, \n",
    "                                  'avg_non_interferent_log_loss':lgbm_info['valid'][awesome_hack][best_valid_iter]},\n",
    "                                 ignore_index=True)\n",
    "            \n",
    "                best_valid_iter = np.argmin(best_info['valid'][awesome_hack])\n",
    "            \n",
    "                model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_A{:03d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                                    best_info['num_trees'],\n",
    "                                                                                    int(best_info['learning_rate']*1000),\n",
    "                                                                                    best_info['num_leaves'],\n",
    "                                                                                    int(alpha * 100),\n",
    "                                                                                    best_valid_iter + 1\n",
    "                                                                                   )\n",
    "            \n",
    "            \n",
    "                best_model.save_model(model_file_name)\n",
    "                print (\"Model saved to\", model_file_name)\n",
    "                    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n",
      "CatFX: ['workclass' 'marital_status' 'occupation' 'relationship' 'race' 'sex'\n",
      " 'native_country']\n",
      "[5]\ttrain's avg_binary_log_loss: 0.515172\tvalid's avg_binary_log_loss: 0.516217\n",
      "[10]\ttrain's avg_binary_log_loss: 0.437\tvalid's avg_binary_log_loss: 0.438303\n",
      "[15]\ttrain's avg_binary_log_loss: 0.383875\tvalid's avg_binary_log_loss: 0.386024\n",
      "[20]\ttrain's avg_binary_log_loss: 0.349702\tvalid's avg_binary_log_loss: 0.353112\n",
      "[25]\ttrain's avg_binary_log_loss: 0.328831\tvalid's avg_binary_log_loss: 0.333189\n",
      "[30]\ttrain's avg_binary_log_loss: 0.314788\tvalid's avg_binary_log_loss: 0.319582\n",
      "[35]\ttrain's avg_binary_log_loss: 0.30553\tvalid's avg_binary_log_loss: 0.311086\n",
      "[40]\ttrain's avg_binary_log_loss: 0.298597\tvalid's avg_binary_log_loss: 0.304944\n",
      "[45]\ttrain's avg_binary_log_loss: 0.293196\tvalid's avg_binary_log_loss: 0.300221\n",
      "[50]\ttrain's avg_binary_log_loss: 0.289252\tvalid's avg_binary_log_loss: 0.297562\n",
      "[55]\ttrain's avg_binary_log_loss: 0.28556\tvalid's avg_binary_log_loss: 0.295135\n",
      "[60]\ttrain's avg_binary_log_loss: 0.282516\tvalid's avg_binary_log_loss: 0.293882\n",
      "[65]\ttrain's avg_binary_log_loss: 0.279834\tvalid's avg_binary_log_loss: 0.292857\n",
      "[70]\ttrain's avg_binary_log_loss: 0.277418\tvalid's avg_binary_log_loss: 0.291941\n",
      "[75]\ttrain's avg_binary_log_loss: 0.275364\tvalid's avg_binary_log_loss: 0.291509\n",
      "[80]\ttrain's avg_binary_log_loss: 0.273135\tvalid's avg_binary_log_loss: 0.290413\n",
      "[85]\ttrain's avg_binary_log_loss: 0.271121\tvalid's avg_binary_log_loss: 0.289602\n",
      "[90]\ttrain's avg_binary_log_loss: 0.269436\tvalid's avg_binary_log_loss: 0.288883\n",
      "[95]\ttrain's avg_binary_log_loss: 0.26778\tvalid's avg_binary_log_loss: 0.288486\n",
      "[100]\ttrain's avg_binary_log_loss: 0.266153\tvalid's avg_binary_log_loss: 0.287819\n",
      "[105]\ttrain's avg_binary_log_loss: 0.264681\tvalid's avg_binary_log_loss: 0.287609\n",
      "[110]\ttrain's avg_binary_log_loss: 0.263282\tvalid's avg_binary_log_loss: 0.287394\n",
      "[115]\ttrain's avg_binary_log_loss: 0.261769\tvalid's avg_binary_log_loss: 0.287019\n",
      "[120]\ttrain's avg_binary_log_loss: 0.260315\tvalid's avg_binary_log_loss: 0.286489\n",
      "[125]\ttrain's avg_binary_log_loss: 0.258829\tvalid's avg_binary_log_loss: 0.286347\n",
      "[130]\ttrain's avg_binary_log_loss: 0.257594\tvalid's avg_binary_log_loss: 0.286298\n",
      "[135]\ttrain's avg_binary_log_loss: 0.256638\tvalid's avg_binary_log_loss: 0.286238\n",
      "[140]\ttrain's avg_binary_log_loss: 0.255321\tvalid's avg_binary_log_loss: 0.286015\n",
      "[145]\ttrain's avg_binary_log_loss: 0.254125\tvalid's avg_binary_log_loss: 0.285644\n",
      "[150]\ttrain's avg_binary_log_loss: 0.252942\tvalid's avg_binary_log_loss: 0.285619\n",
      "[155]\ttrain's avg_binary_log_loss: 0.251668\tvalid's avg_binary_log_loss: 0.285709\n",
      "[160]\ttrain's avg_binary_log_loss: 0.250668\tvalid's avg_binary_log_loss: 0.28596\n",
      "[165]\ttrain's avg_binary_log_loss: 0.249381\tvalid's avg_binary_log_loss: 0.285614\n",
      "[170]\ttrain's avg_binary_log_loss: 0.248353\tvalid's avg_binary_log_loss: 0.285545\n",
      "[175]\ttrain's avg_binary_log_loss: 0.24738\tvalid's avg_binary_log_loss: 0.285711\n",
      "[180]\ttrain's avg_binary_log_loss: 0.246401\tvalid's avg_binary_log_loss: 0.2858\n",
      "[185]\ttrain's avg_binary_log_loss: 0.245381\tvalid's avg_binary_log_loss: 0.285782\n",
      "[190]\ttrain's avg_binary_log_loss: 0.244467\tvalid's avg_binary_log_loss: 0.285651\n",
      "[195]\ttrain's avg_binary_log_loss: 0.243713\tvalid's avg_binary_log_loss: 0.285747\n",
      "[200]\ttrain's avg_binary_log_loss: 0.242944\tvalid's avg_binary_log_loss: 0.285824\n",
      "Model saved to ../out/models/non_interferent_census_B150_T200_S0100_L24_A050_R169.model\n",
      "   num_trees  learning_rate  num_leaves  alpha  best_round  \\\n",
      "0      200.0            0.1        24.0    0.5       169.0   \n",
      "\n",
      "   avg_non_interferent_log_loss  \n",
      "0                      0.285418  \n"
     ]
    }
   ],
   "source": [
    "# enable/disable\n",
    "if True:\n",
    "    for B in [150]: #[5, 15, 150, 300]:\n",
    "\n",
    "        experiments = train_non_interferent(\"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/test_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../out/models/non_interferent_census_B{:d}\".format(B))  \n",
    "\n",
    "        experiments.to_csv('../out/models/non_interferent_census_B{:d}.csv'.format(B), index=False)\n",
    "\n",
    "        print (experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Boosting with Our Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NIBoosting(atk_train, atk_valid, trees, \n",
    "                 cat_fx,\n",
    "                 params,\n",
    "                 output_model_file,\n",
    "                 partial_save=1000, \n",
    "                 adv_rounds=1,\n",
    "                 alpha = 1.0):\n",
    "    ''' \n",
    "    atk_data: full dataset including all valid attacks\n",
    "    atk_groups: lenght of each attack set\n",
    "    trees: total number of trees to be produced\n",
    "    adv_rounds: adversarial instance injecting frequency\n",
    "    '''\n",
    "    # temp lgbm file\n",
    "    temp = output_model_file+\".tmp\"\n",
    "    \n",
    "    # get groups and remove instance ids\n",
    "    atk_groups       = atk_train['instance_id'].value_counts().sort_index().values\n",
    "    atk_valid_groups = atk_valid['instance_id'].value_counts().sort_index().values\n",
    "    \n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(atk_train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])  \n",
    "    # print (\"CatFX:\", atk_train.columns.values[cat_fx])\n",
    "\n",
    "    # prepare data (avoiding pandas)\n",
    "    atk_train  = atk_train.iloc[:,1:].values\n",
    "    atk_valid  = atk_valid.iloc[:,1:].values\n",
    "    cat_fx = [x - 1 for x in cat_fx]\n",
    "\n",
    "    # train first trees\n",
    "    original_ids = np.cumsum(atk_groups[:-1])\n",
    "    original_ids = np.insert(original_ids, 0, 0)\n",
    "    \n",
    "    original_valid_ids = np.cumsum(atk_valid_groups[:-1])\n",
    "    original_valid_ids = np.insert(original_valid_ids, 0, 0)\n",
    "    \n",
    "    # -------------------------\n",
    "    # train first iteration\n",
    "    lgbm_train = lightgbm.Dataset(data=atk_train[original_ids,:-1], \n",
    "                                  label=atk_train[original_ids,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_valid = lightgbm.Dataset(data=atk_valid[original_valid_ids,:-1], \n",
    "                                  label=atk_valid[original_valid_ids,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "\n",
    "    lgbm_info = {}\n",
    "    lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                num_boost_round = adv_rounds,\n",
    "                                fobj  = optimize_log_loss, \n",
    "                                feval = avg_log_loss,\n",
    "                                evals_result = lgbm_info,\n",
    "                                valid_sets   = [lgbm_train, lgbm_valid], \n",
    "                                valid_names  = ['train', 'valid'],\n",
    "                                verbose_eval=5)\n",
    "\n",
    "    awesome_hack = \"avg_non_interferent_log_loss\" + \" [alpha={:.2f}]\".format(alpha)\n",
    "    awesome_hack = \"avg_binary_log_loss\"\n",
    "    lgbm_info['train'][awesome_hack] = lgbm_info['train'][\"avg_binary_log_loss\"]\n",
    "    lgbm_info['valid'][awesome_hack] = lgbm_info['valid'][\"avg_binary_log_loss\"]\n",
    "\n",
    "    # train remaining trees\n",
    "    for t in range(adv_rounds+1, trees+1, adv_rounds):\n",
    "        # attack dataset\n",
    "        adv_train, adv_train_groups = gen_adv_boosting_data(lgbm_model, atk_train, atk_groups, num_atks=5)\n",
    "        adv_valid, adv_valid_groups = gen_adv_boosting_data(lgbm_model, atk_valid, atk_valid_groups, num_atks=5)\n",
    "                \n",
    "        # prepare data and train\n",
    "        lgbm_train = lightgbm.Dataset(data=adv_train[:,:-1], \n",
    "                                      label=adv_train[:,-1],\n",
    "                                      categorical_feature = cat_fx)\n",
    "\n",
    "        lgbm_valid = lightgbm.Dataset(data=adv_valid[:,:-1], \n",
    "                                      label=adv_valid[:,-1],\n",
    "                                      categorical_feature = cat_fx)\n",
    "\n",
    "        new_lgbm_info = {}\n",
    "        lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                    num_boost_round = adv_rounds, \n",
    "                                    init_model = lgbm_model,\n",
    "                                    fobj  = functools.partial(optimize_non_interferent_log_loss_claudio, alpha=alpha), \n",
    "                                    feval = avg_log_loss,# functools.partial(avg_non_interferent_log_loss, alpha=alpha),\n",
    "                                    evals_result = new_lgbm_info,\n",
    "                                    valid_sets   = [lgbm_train, lgbm_valid], \n",
    "                                    valid_names  = ['train', 'valid'],\n",
    "                                    verbose_eval=5)\n",
    "        \n",
    "        lgbm_info['train'][awesome_hack] += new_lgbm_info['train'][awesome_hack]\n",
    "        lgbm_info['valid'][awesome_hack] += new_lgbm_info['valid'][awesome_hack]\n",
    "\n",
    "    \n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_claudio(train_file, valid_file, test_file, output_model_file):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'best_round', 'avg_binary_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    assert \"instance_id\" in train.columns.values, \"Wrong training set file for GBDT\"\n",
    "\n",
    "    for num_trees in [200]:\n",
    "        for alpha in [0.5]: #[0.25, 0.50, 0.75, 1.00]:\n",
    "            best_model = None\n",
    "            best_info = None\n",
    "            best_loss = np.inf\n",
    "            awesome_hack = \"avg_non_interferent_log_loss\" + \" [alpha={:.2f}]\".format(alpha)\n",
    "            awesome_hack = \"avg_binary_log_loss\"\n",
    "\n",
    "            for learning_rate in [1.0]: #[0.01, 0.05, 0.1]:\n",
    "                for num_leaves in [16]: #[8, 16, 24]:\n",
    "\n",
    "                    lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                    'num_leaves': num_leaves} \n",
    "\n",
    "                    lgbm_model, lgbm_info = NIBoosting(train,\n",
    "                                                        valid,\n",
    "                                                        trees=num_trees, \n",
    "                                                        cat_fx = cat_fx, \n",
    "                                                        output_model_file=output_model_file, \n",
    "                                                        adv_rounds=1,\n",
    "                                                        params=lgbm_params,\n",
    "                                                        alpha=alpha)\n",
    "\n",
    "                    if np.min(lgbm_info['valid'][awesome_hack]) < best_loss:\n",
    "                        best_model = lgbm_model\n",
    "                        best_info = lgbm_info\n",
    "                        best_loss = np.min(lgbm_info['valid'][awesome_hack])\n",
    "                        best_info['num_trees'] = num_trees\n",
    "                        best_info['learning_rate'] = learning_rate\n",
    "                        best_info['num_leaves'] = num_leaves\n",
    "\n",
    "                    \n",
    "                    ####\n",
    "                    # update experimental results\n",
    "                    best_valid_iter = np.argmin(lgbm_info['valid'][awesome_hack])\n",
    "                    exp = exp.append({'num_trees': num_trees, \n",
    "                                      'learning_rate':learning_rate,\n",
    "                                      'num_leaves':num_leaves, \n",
    "                                      'best_round':best_valid_iter, \n",
    "                                      awesome_hack:lgbm_info['valid'][awesome_hack][best_valid_iter]},\n",
    "                                      ignore_index=True)\n",
    "\n",
    "            # save file\n",
    "            best_valid_iter = np.argmin(best_info['valid'][awesome_hack])\n",
    "            \n",
    "            model_file_name = \"{:s}_claudio_T{:d}_S{:04d}_L{:d}_A{:03d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                                best_info['num_trees'],\n",
    "                                                                                int(best_info['learning_rate']*1000),\n",
    "                                                                                best_info['num_leaves'],\n",
    "                                                                                int(alpha * 100),\n",
    "                                                                                best_valid_iter + 1\n",
    "                                                                               )\n",
    "            \n",
    "            \n",
    "            best_model.save_model(model_file_name)\n",
    "            print (\"Model saved to\", model_file_name)\n",
    "                \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n",
      "[5]\ttrain's avg_binary_log_loss: 0.13994\tvalid's avg_binary_log_loss: 0.141381\n",
      "[10]\ttrain's avg_binary_log_loss: 0.137907\tvalid's avg_binary_log_loss: 0.140562\n",
      "[15]\ttrain's avg_binary_log_loss: 0.13639\tvalid's avg_binary_log_loss: 0.139487\n",
      "[20]\ttrain's avg_binary_log_loss: 0.134786\tvalid's avg_binary_log_loss: 0.139052\n",
      "[25]\ttrain's avg_binary_log_loss: 0.133823\tvalid's avg_binary_log_loss: 0.138367\n",
      "[30]\ttrain's avg_binary_log_loss: 0.133116\tvalid's avg_binary_log_loss: 0.138354\n",
      "[35]\ttrain's avg_binary_log_loss: 0.132402\tvalid's avg_binary_log_loss: 0.137978\n",
      "[40]\ttrain's avg_binary_log_loss: 0.117862\tvalid's avg_binary_log_loss: 0.124742\n",
      "[45]\ttrain's avg_binary_log_loss: 0.117203\tvalid's avg_binary_log_loss: 0.124434\n",
      "[50]\ttrain's avg_binary_log_loss: 0.116629\tvalid's avg_binary_log_loss: 0.123937\n",
      "[55]\ttrain's avg_binary_log_loss: 0.105531\tvalid's avg_binary_log_loss: 0.113309\n",
      "[60]\ttrain's avg_binary_log_loss: 0.104778\tvalid's avg_binary_log_loss: 0.112722\n",
      "[65]\ttrain's avg_binary_log_loss: 0.104397\tvalid's avg_binary_log_loss: 0.112579\n",
      "[70]\ttrain's avg_binary_log_loss: 0.104033\tvalid's avg_binary_log_loss: 0.112534\n",
      "[75]\ttrain's avg_binary_log_loss: 0.10371\tvalid's avg_binary_log_loss: 0.112226\n",
      "[80]\ttrain's avg_binary_log_loss: 0.103368\tvalid's avg_binary_log_loss: 0.112227\n",
      "[85]\ttrain's avg_binary_log_loss: 0.103002\tvalid's avg_binary_log_loss: 0.112107\n",
      "[90]\ttrain's avg_binary_log_loss: 0.102616\tvalid's avg_binary_log_loss: 0.111856\n",
      "[95]\ttrain's avg_binary_log_loss: 0.102215\tvalid's avg_binary_log_loss: 0.11201\n",
      "[100]\ttrain's avg_binary_log_loss: 0.101874\tvalid's avg_binary_log_loss: 0.111609\n",
      "[105]\ttrain's avg_binary_log_loss: 0.101564\tvalid's avg_binary_log_loss: 0.111507\n",
      "[110]\ttrain's avg_binary_log_loss: 0.101292\tvalid's avg_binary_log_loss: 0.111422\n",
      "[115]\ttrain's avg_binary_log_loss: 0.101046\tvalid's avg_binary_log_loss: 0.111184\n",
      "[120]\ttrain's avg_binary_log_loss: 0.100812\tvalid's avg_binary_log_loss: 0.111118\n",
      "[125]\ttrain's avg_binary_log_loss: 0.100586\tvalid's avg_binary_log_loss: 0.111166\n",
      "[130]\ttrain's avg_binary_log_loss: 0.100377\tvalid's avg_binary_log_loss: 0.111205\n",
      "[135]\ttrain's avg_binary_log_loss: 0.100195\tvalid's avg_binary_log_loss: 0.111077\n",
      "[140]\ttrain's avg_binary_log_loss: 0.100013\tvalid's avg_binary_log_loss: 0.111158\n",
      "[145]\ttrain's avg_binary_log_loss: 0.0997641\tvalid's avg_binary_log_loss: 0.110961\n",
      "[150]\ttrain's avg_binary_log_loss: 0.0995187\tvalid's avg_binary_log_loss: 0.110873\n",
      "[155]\ttrain's avg_binary_log_loss: 0.0993368\tvalid's avg_binary_log_loss: 0.110738\n",
      "[160]\ttrain's avg_binary_log_loss: 0.0990965\tvalid's avg_binary_log_loss: 0.110868\n",
      "[165]\ttrain's avg_binary_log_loss: 0.0989114\tvalid's avg_binary_log_loss: 0.110814\n",
      "[170]\ttrain's avg_binary_log_loss: 0.0987573\tvalid's avg_binary_log_loss: 0.11086\n",
      "[175]\ttrain's avg_binary_log_loss: 0.098572\tvalid's avg_binary_log_loss: 0.110797\n",
      "[180]\ttrain's avg_binary_log_loss: 0.098416\tvalid's avg_binary_log_loss: 0.110624\n",
      "[185]\ttrain's avg_binary_log_loss: 0.0982864\tvalid's avg_binary_log_loss: 0.110658\n",
      "[190]\ttrain's avg_binary_log_loss: 0.0981658\tvalid's avg_binary_log_loss: 0.110539\n",
      "[195]\ttrain's avg_binary_log_loss: 0.0980036\tvalid's avg_binary_log_loss: 0.110725\n",
      "[200]\ttrain's avg_binary_log_loss: 0.097855\tvalid's avg_binary_log_loss: 0.110761\n",
      "Model saved to ../out/models/adv_boosting_census_B150_claudio_T200_S1000_L16_A050_R190.model\n",
      "   num_trees  learning_rate  num_leaves  best_round  avg_binary_log_loss\n",
      "0      200.0            1.0        16.0       189.0             0.110539\n"
     ]
    }
   ],
   "source": [
    "# enable/disable\n",
    "if True:\n",
    "    for B in [150]: #[5, 15, 150, 300]:\n",
    "\n",
    "        experiments = train_claudio ( \"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/test_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../out/models/adv_boosting_census_B{:d}\".format(B))  \n",
    "\n",
    "        experiments.to_csv('../out/models/adv_boosting_census_B{:d}.csv'.format(B), index=False)\n",
    "\n",
    "        print (experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Tests & Old implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_non_interferent(train_file, valid_file, test_file, output_model_file):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'alpha', 'best_round', 'avg_non_interferent_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    train_groups = train['instance_id'].value_counts().sort_index().values\n",
    "    valid_groups = valid['instance_id'].value_counts().sort_index().values\n",
    "    test_groups = test['instance_id'].value_counts().sort_index().values\n",
    "    \n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])\n",
    "    print (\"CatFX:\", train.columns.values[cat_fx])\n",
    "    \n",
    "    # prepare data\n",
    "    train = train.iloc[:,1:]\n",
    "    valid = valid.iloc[:,1:]\n",
    "    test = test.iloc[:,1:]\n",
    "    \n",
    "    cat_fx = [x - 1 for x in cat_fx]\n",
    "\n",
    "    for num_trees in [100]:\n",
    "        for alpha in [1.0]: #[0.25, 0.50, 0.75, 1.00]:\n",
    "            best_model = None\n",
    "            best_info = None\n",
    "            best_loss = np.inf\n",
    "            awesome_hack = \"avg_non_interferent_log_loss\" + \" [alpha={:.2f}]\".format(alpha)\n",
    "            \n",
    "            for learning_rate in [0.1]: #[0.01, 0.05, 0.1]:\n",
    "                for num_leaves in [16]: #[8, 16, 24, 32]:\n",
    "                    # datasets\n",
    "                    lgbm_train = lightgbm.Dataset(data=train.iloc[:,:-1].values,\n",
    "                                                  label=train.iloc[:,-1].values,\n",
    "                                                  group=train_groups,\n",
    "                                                  categorical_feature = cat_fx)\n",
    "\n",
    "                    lgbm_valid = lightgbm.Dataset(data=valid.iloc[:,:-1].values,\n",
    "                                                  label=valid.iloc[:,-1].values,\n",
    "                                                  group=valid_groups,\n",
    "                                                  #reference=lgbm_train, \n",
    "                                                  #free_raw_data=False,\n",
    "                                                  categorical_feature = cat_fx)\n",
    "\n",
    "                    # run train\n",
    "                    lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                    'num_leaves': num_leaves} \n",
    "                    lgbm_info = {}\n",
    "                    lgbm_model = lightgbm.train(lgbm_params, lgbm_train, \n",
    "                                                num_boost_round = num_trees,\n",
    "                                                fobj            = lambda x,y: optimize_non_interferent_log_loss(x,y,alpha=alpha), # functools.partial(optimize_non_interferent_log_loss, alpha=alpha),\n",
    "                                                feval           = lambda x,y: avg_non_interferent_log_loss(x,y,alpha=alpha), #, functools.partial(avg_non_interferent_log_loss, alpha=alpha),\n",
    "                                                evals_result    = lgbm_info,\n",
    "                                                valid_sets      = [lgbm_train, lgbm_valid], \n",
    "                                                valid_names     = ['train', 'valid'],\n",
    "                                                verbose_eval    = 5)\n",
    "                    \n",
    "                    \n",
    "                    if np.min(lgbm_info['valid'][awesome_hack]) < best_loss:\n",
    "                        best_model = lgbm_model\n",
    "                        best_info = lgbm_info\n",
    "                        best_loss = np.min(lgbm_info['valid'][awesome_hack])\n",
    "                        best_info['num_trees'] = num_trees\n",
    "                        best_info['learning_rate'] = learning_rate\n",
    "                        best_info['num_leaves'] = num_leaves\n",
    "                \n",
    "\n",
    "                    # save file\n",
    "                    \n",
    "                    best_valid_iter = np.argmin(lgbm_info['valid'][awesome_hack])\n",
    "\n",
    "                    # update experimental results\n",
    "                    exp = exp.append({'num_trees': num_trees, \n",
    "                                      'learning_rate':learning_rate,\n",
    "                                      'num_leaves':num_leaves, \n",
    "                                      'alpha': alpha,\n",
    "                                      'best_round':best_valid_iter+1, \n",
    "                                      'avg_non_interferent_log_loss':lgbm_info['valid'][awesome_hack][best_valid_iter]},\n",
    "                                     ignore_index=True)\n",
    "            \n",
    "            best_valid_iter = np.argmin(best_info['valid'][awesome_hack])\n",
    "            \n",
    "            model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_A{:03d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                                    best_info['num_trees'],\n",
    "                                                                                    int(best_info['learning_rate']*1000),\n",
    "                                                                                    best_info['num_leaves'],\n",
    "                                                                                    int(alpha * 100),\n",
    "                                                                                    best_valid_iter + 1\n",
    "                                                                                   )\n",
    "            \n",
    "            \n",
    "            best_model.save_model(model_file_name)\n",
    "            print (\"Model saved to\", model_file_name)\n",
    "                    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n",
      "CatFX: ['workclass' 'marital_status' 'occupation' 'relationship' 'race' 'sex'\n",
      " 'native_country']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-8c47ff471954>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                    \u001b[0;34m\"../data/census/valid_B{:d}.csv.bz2\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                    \u001b[0;34m\"../data/census/test_B{:d}.csv.bz2\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                                    \"../out/models/non_interferent_census_B{:d}\".format(B))  \n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mexperiments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../out/models/non_interferent_census_B{:d}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-12870b47cbef>\u001b[0m in \u001b[0;36mtrain_non_interferent\u001b[0;34m(train_file, valid_file, test_file, output_model_file)\u001b[0m\n\u001b[1;32m     54\u001b[0m                                                 \u001b[0mvalid_sets\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlgbm_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlgbm_valid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                                                 \u001b[0mvalid_names\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                                                 verbose_eval    = 5)\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1806\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"none\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1808\u001b[0;31m             \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__inner_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1809\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__boost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-12870b47cbef>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     lgbm_model = lightgbm.train(lgbm_params, lgbm_train, \n\u001b[1;32m     50\u001b[0m                                                 \u001b[0mnum_boost_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_trees\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                                                 \u001b[0mfobj\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimize_non_interferent_log_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# functools.partial(optimize_non_interferent_log_loss, alpha=alpha),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                                                 \u001b[0mfeval\u001b[0m           \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mavg_non_interferent_log_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#, functools.partial(avg_non_interferent_log_loss, alpha=alpha),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                                 \u001b[0mevals_result\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mlgbm_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-a61218b88a8e>\u001b[0m in \u001b[0;36moptimize_non_interferent_log_loss\u001b[0;34m(preds, train_data, alpha)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# binary logloss under maximal attack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# grads_uma, hess_uma = optimize_log_loss_uma(preds, train_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgrads_uma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhess_uma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize_log_loss_uma_ext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# binary logloss (plain)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3d2e4b806fbc>\u001b[0m in \u001b[0;36moptimize_log_loss_uma_ext\u001b[0;34m(preds, train_data)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mexp_pl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mx_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mexp_pl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_grad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "# enable/disable\n",
    "if True:\n",
    "    for B in [150]: #[5, 15, 150, 300]:\n",
    "\n",
    "        experiments = train_non_interferent(\"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/test_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../out/models/non_interferent_census_B{:d}\".format(B))  \n",
    "\n",
    "        experiments.to_csv('../out/models/non_interferent_census_B{:d}.csv'.format(B), index=False)\n",
    "\n",
    "        print (experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check groups are the same after re-splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def check_groups(atk_train_file, atk_valid_file, atk_test_file):\n",
    "    # Load post-resplitting data\n",
    "    post_train, post_valid, post_test, cat_fx = load_atk_train_valid_test(atk_train_file, atk_valid_file, atk_test_file)\n",
    "    post_train_groups = post_train['instance_id'].value_counts().sort_index().values\n",
    "    post_valid_groups = post_valid['instance_id'].value_counts().sort_index().values\n",
    "    post_test_groups  = post_test['instance_id'].value_counts().sort_index().values\n",
    "    \n",
    "    # load pre-re-splitting data\n",
    "    pre_train = pd.read_csv(atk_train_file)\n",
    "    pre_valid = pd.read_csv(atk_valid_file)\n",
    "    pre_test  = pd.read_csv(atk_test_file)\n",
    "    pre_train_groups = pre_train['instance_id'].value_counts().sort_index().values\n",
    "    pre_valid_groups = pre_valid['instance_id'].value_counts().sort_index().values\n",
    "    pre_test_groups  = pre_test['instance_id'].value_counts().sort_index().values\n",
    "\n",
    "    # check global lenght\n",
    "    print (\"PRE  TOTAL instances:\", len(pre_train)+ len(pre_valid)+ len(pre_test))\n",
    "    print (\"POST TOTAL instances:\", len(post_train)+ len(post_valid)+ len(post_test))\n",
    "    \n",
    "    assert len(pre_train)+ len(pre_valid)+ len(pre_test) == \\\n",
    "            len(post_train)+ len(post_valid)+ len(post_test),\\\n",
    "            \"Different number of instances !\"\n",
    "            \n",
    "    # check groups\n",
    "    print (\"PRE  lengths in groups:\", len(pre_train_groups), len(pre_valid_groups), len(pre_test_groups))\n",
    "    print (\"POST lengths in groups:\", len(post_train_groups), len(post_valid_groups), len(post_test_groups))\n",
    "    pre_all_groups  = np.concatenate([pre_train_groups, pre_valid_groups, pre_test_groups])\n",
    "    post_all_groups = np.concatenate([post_train_groups, post_valid_groups, post_test_groups])\n",
    "    print (\"PRE  TOTAL groups:\", len(pre_all_groups))\n",
    "    print (\"POST TOTAL groups:\", len(post_all_groups))\n",
    "    \n",
    "    assert len(pre_all_groups)==len(post_all_groups),\\\n",
    "        \"Different number of groups!\"\n",
    "    \n",
    "    # compare values\n",
    "    assert (pre_all_groups==post_all_groups).all(),\\\n",
    "        \"Groups have different sizes!\"\n",
    "\n",
    "B=5\n",
    "check_groups(\"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                   \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                   \"../data/census/test_B{:d}.csv.bz2\".format(B)) \n",
    "B=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check extraction of non-attacked instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nonatk_ids(atk_data):\n",
    "    # get groups and remove instance ids\n",
    "    atk_groups = atk_data['instance_id'].value_counts().sort_index().values\n",
    "\n",
    "    original_ids = np.cumsum(atk_groups[:-1])\n",
    "    original_ids = np.insert(original_ids, 0, 0)\n",
    "\n",
    "    return original_ids\n",
    "    \n",
    "def check_nonatk_filter(atk_train_file, atk_valid_file, atk_test_file,\n",
    "                        train_file, valid_file, test_file):\n",
    "    # Load post-resplitting data\n",
    "    atk_train, atk_valid, atk_test, cat_fx = load_atk_train_valid_test(atk_train_file, atk_valid_file, atk_test_file)\n",
    "    \n",
    "    # Load non attacked instances\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "\n",
    "    # Filter atks and skip instance id\n",
    "    filtered_train = atk_train.iloc[get_nonatk_ids(atk_train),1:]\n",
    "    filtered_valid = atk_valid.iloc[get_nonatk_ids(atk_valid),1:]\n",
    "    filtered_test  = atk_test.iloc[get_nonatk_ids(atk_test),1:]\n",
    "\n",
    "    print (\"Attacked shapes\", atk_train.shape, atk_valid.shape, atk_test.shape)\n",
    "    print (\"Filteres shapes\", filtered_train.shape, filtered_valid.shape, filtered_test.shape)\n",
    "    print (\"Original shapes\", train.shape, valid.shape, test.shape)\n",
    "    \n",
    "    assert np.array_equal(train.values,filtered_train.values), \"Different Data !\"\n",
    "    assert np.array_equal(valid.values,filtered_valid.values), \"Different Data !\"\n",
    "    assert np.array_equal(test.values,filtered_test.values), \"Different Data !\"\n",
    "\n",
    "\n",
    "\n",
    "B=5\n",
    "check_nonatk_filter(\"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                       \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                       \"../data/census/test_B{:d}.csv.bz2\".format(B),\n",
    "                   \"../data/census/train_ori.csv.bz2\".format(B),\n",
    "                       \"../data/census/valid_ori.csv.bz2\".format(B),\n",
    "                       \"../data/census/test_ori.csv.bz2\".format(B) ) \n",
    "B=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(train_file, valid_file, test_file, output_model_file):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['C', 'avg_binary_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    X_train = train.iloc[:,:-1].values\n",
    "    y_train = train.iloc[:,-1].values\n",
    "    y_train[y_train == -1] = 0\n",
    "    \n",
    "    X_valid = valid.iloc[:,:-1].values\n",
    "    y_valid = valid.iloc[:,-1].values\n",
    "    \n",
    "    for c in [0.001, 0.01, 0.1, 1.0]:\n",
    "        \n",
    "        model = SVC(kernel='rbf', probability=True)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_preds = model.predict_proba(X_valid)[:,0]\n",
    "        cur_avg_binary_log_loss = np.mean(binary_log_loss(y_preds, y_valid))\n",
    "        \n",
    "        model_file_name = \"{:s}_C{:04d}.model\".format(output_model_file, int(c * 1000))\n",
    "        \n",
    "        with open(model_file_name, 'wb') as fout:\n",
    "            pickle.dump(model, fout)\n",
    "        \n",
    "        print (\"Model saved to\", model_file_name)\n",
    "        \n",
    "        # update experimental results\n",
    "        exp = exp.append({'C': c, \n",
    "                          'avg_binary_log_loss':cur_avg_binary_log_loss},\n",
    "                         ignore_index=True)\n",
    "    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable/disable LGBM Baseline\n",
    "if False:\n",
    "    experiments = train_svm ( \"../data/census/train_ori.csv.bz2\",\n",
    "                                                     \"../data/census/valid_ori.csv.bz2\",\n",
    "                                                     \"../data/census/test_ori.csv.bz2\",\n",
    "                                                     \"../out/models/svm_census\")  \n",
    "\n",
    "    experiments.to_csv('../out/models/svm_census.csv', index=False)\n",
    "\n",
    "    print (experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
