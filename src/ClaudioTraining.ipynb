{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models\n",
    "\n",
    "This notebook contains the code used for training the following learning models:\n",
    "\n",
    "-  **Standard GBDT** (_baseline 1_)\n",
    "-  **Adversarial Boosting** (_baseline 2_)\n",
    "-  **Non-Interferent GBDT** (our proposal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    " - http://lightgbm.readthedocs.io/en/latest/\n",
    " - http://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
    " - https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "import pickle\n",
    "import json\n",
    "import functools\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(dataset, categorical_features):\n",
    "    dataset_le = dataset.copy()\n",
    "    for column in dataset_le.columns:\n",
    "        if column in categorical_features:\n",
    "            dataset_le[column] = dataset_le[column].astype('category')\n",
    "            dataset_le[column] = dataset_le[column].cat.codes.astype(np.int32)\n",
    "    return dataset_le\n",
    "\n",
    "def load_atk_train_valid_test(atk_train_file, atk_valid_file, atk_test_file, \n",
    "                              train_split=0.6, valid_split=0.2, force=False):\n",
    "    \n",
    "    \n",
    "    if  (force or \n",
    "          not os.path.exists(atk_train_file+\".cat.bz2\") or\n",
    "          not os.path.exists(atk_valid_file+\".cat.bz2\") or\n",
    "          not os.path.exists(atk_test_file+\".cat.bz2\") or \n",
    "          not os.path.exists(atk_train_file+\".cat.json\") ):\n",
    "    \n",
    "        print (\"Pre-processing original files...\")\n",
    "\n",
    "        print (\"Loading:\", atk_train_file)\n",
    "        print (\"Loading:\", atk_valid_file)\n",
    "        print (\"Loading:\", atk_test_file)\n",
    "\n",
    "        train = pd.read_csv(atk_train_file)\n",
    "        valid = pd.read_csv(atk_valid_file)\n",
    "        test  = pd.read_csv(atk_test_file)\n",
    "        \n",
    "        print (\"Train/Valid/Test sizes:\", train.shape, valid.shape, test.shape)\n",
    "        print (\"Train/Valid/Test split: {:.2f} {:.2f} {:.2f}\"\n",
    "                   .format( train.shape[0]/(train.shape[0]+valid.shape[0]+test.shape[0]),\n",
    "                            valid.shape[0]/(train.shape[0]+valid.shape[0]+test.shape[0]),\n",
    "                            test.shape[0] /(train.shape[0]+valid.shape[0]+test.shape[0]) ) )\n",
    "\n",
    "\n",
    "        # split-back into train valid test\n",
    "        if 'instance_id' in train.columns.values:\n",
    "            print ('   ... with instance ids')\n",
    "            valid['instance_id'] += train.iloc[-1,0]\n",
    "            test['instance_id']  += valid.iloc[-1,0]\n",
    "            assert max(train['instance_id'])<min(valid['instance_id']), \"Instance ID mismatch\"\n",
    "            assert max(valid['instance_id'])<min(test['instance_id']), \"Instance ID mismatch\"\n",
    "            \n",
    "            groups = np.concatenate( [ train['instance_id'].value_counts().sort_index().values,\n",
    "                                       valid['instance_id'].value_counts().sort_index().values,\n",
    "                                       test['instance_id'].value_counts().sort_index().values ] )\n",
    "            \n",
    "            num_train_groups = int( len(groups)*train_split )\n",
    "            train_size = sum(groups[:num_train_groups])\n",
    "            num_valid_groups = int( len(groups)*valid_split )\n",
    "            valid_size = sum(groups[num_train_groups:num_train_groups+num_valid_groups])\n",
    "        else:\n",
    "            full_size = len(train) + len(valid) + len(test)\n",
    "            train_size = int( full_size*train_split )\n",
    "            valid_size = int( full_size*valid_split )\n",
    "        \n",
    "        # concat to process correctly label encoding\n",
    "        full = pd.concat( [train, valid, test] )\n",
    "\n",
    "        # get index of categorical features (-1 because of instance_id)\n",
    "        cat_fx = full.columns.values[np.where(full.dtypes=='object')[0]]\n",
    "        cat_fx = list(cat_fx)    \n",
    "        full = label_encode(full, cat_fx)\n",
    "        with open(atk_train_file+\".cat.json\", 'w') as fp:\n",
    "            json.dump(cat_fx, fp)\n",
    "        print (\"CatFX:\", cat_fx)\n",
    "\n",
    "        train_cat = full.iloc[0:train_size,:]\n",
    "        valid_cat = full.iloc[train_size:train_size+valid_size,:]\n",
    "        test_cat  = full.iloc[train_size+valid_size:,:]\n",
    "        \n",
    "        assert len(train_cat)+len(valid_cat)+len(test_cat)==len(full), \"Split sizes mismatch\"\n",
    "        \n",
    "\n",
    "        print (\"Train/Valid/Test sizes:\", train_cat.shape, valid_cat.shape, test_cat.shape)\n",
    "        print (\"Train/Valid/Test split: {:.2f} {:.2f} {:.2f}\"\n",
    "                   .format( train_cat.shape[0]/(train_cat.shape[0]+valid_cat.shape[0]+test_cat.shape[0]),\n",
    "                            valid_cat.shape[0]/(train_cat.shape[0]+valid_cat.shape[0]+test_cat.shape[0]),\n",
    "                            test_cat.shape[0] /(train_cat.shape[0]+valid_cat.shape[0]+test_cat.shape[0]) ) )\n",
    "\n",
    "        # save to file\n",
    "        print (\"Saving processed files *.cat.bz2\")\n",
    "        train_cat.to_csv(atk_train_file+\".cat.bz2\", compression=\"bz2\", index=False)\n",
    "        valid_cat.to_csv(atk_valid_file+\".cat.bz2\", compression=\"bz2\", index=False)\n",
    "        test_cat.to_csv (atk_test_file+\".cat.bz2\",  compression=\"bz2\", index=False)\n",
    "        \n",
    "    else:\n",
    "        print (\"Loading pre-processed files...\")\n",
    "\n",
    "        train_cat = pd.read_csv(atk_train_file+\".cat.bz2\")\n",
    "        valid_cat = pd.read_csv(atk_valid_file+\".cat.bz2\")\n",
    "        test_cat  = pd.read_csv(atk_test_file+\".cat.bz2\")\n",
    "        \n",
    "        with open(atk_train_file+\".cat.json\", 'r') as fp:\n",
    "            cat_fx = json.load(fp)\n",
    "    \n",
    "    # return data\n",
    "    return train_cat, valid_cat, test_cat, cat_fx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard\n",
    "\n",
    "The following function, called <code>optimize_log_loss</code>, is the one that should be optimized (i.e., minimized) for learning _standard_ and _baseline_ approaches. More specifically, this is the standard binary log loss which is used to train any _standard_ or _baseline_ model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $L$ = <code>optimize_log_loss</code>\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}}\\ell(h(\\mathbf{x}), y)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\ell(h(\\mathbf{x}), y) = log(1+e^{(-yh(\\mathbf{x}))})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_log_loss(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    exp_pl = np.exp(preds * labels)\n",
    "    # http://www.wolframalpha.com/input/?i=differentiate+log(1+%2B+exp(-kx)+)\n",
    "    grads = -labels / (1.0 +  exp_pl)  \n",
    "    # http://www.wolframalpha.com/input/?i=d%5E2%2Fdx%5E2+log(1+%2B+exp(-kx)+)\n",
    "    hess = labels**2 * exp_pl / (1.0 + exp_pl)**2 \n",
    "\n",
    "    # this is to optimize average logloss\n",
    "    norm = 1.0/len(preds)\n",
    "    grads *= norm\n",
    "    hess *= norm\n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom\n",
    "\n",
    "In addition to the standard binary log loss used to train a model, we introduce our custom <code>optimize_non_interferent_log_loss</code>, which is computed as the weighted combination of two objective functions, as follows:\n",
    "\n",
    "-  $L$ = <code>optimize_log_loss</code> (standard, already seen above);\n",
    "-  $L^A$ = <code>optimize_log_loss_uma</code> (custom, defined below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $L^A$ = <code>optimize_log_loss_uma</code>\n",
    "\n",
    "This function is used to train a **full** _non-interferent_ model; in other words, full non-interferent models are learned by optimizing (i.e., minimizing) the function which measures the binary log loss **under the maximal attack** possible.\n",
    "\n",
    "$$\n",
    "L^A = \\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}} \\log  \\left( \\sum_{\\mathbf{x}' \\in \\mathit{MaxAtk}({\\mathbf{x}},{A})} e^{\\ell(h(\\mathbf{x}'), y)} \\right).\n",
    "$$\n",
    "\n",
    "where still:\n",
    "\n",
    "$$\n",
    "\\ell(h(\\mathbf{x}), y) = log(1+e^{(-yh(\\mathbf{x}))})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_log_loss_uma(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    grads = np.zeros_like(labels, dtype=np.float64)\n",
    "    hess = np.zeros_like(grads)\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "\n",
    "        norm = 1.0 / float(len(attack_lens))\n",
    "\n",
    "        offset = 0\n",
    "        for atk in attack_lens:\n",
    "            exp_pl = np.exp(- preds[offset:offset+atk] * labels[offset:offset+atk])\n",
    "\n",
    "            inv_sum = 1.0 / np.sum(1.0 + exp_pl)\n",
    "\n",
    "            x_grad = inv_sum * exp_pl\n",
    "\n",
    "            grads[offset:offset+atk] = norm * x_grad * (- labels[offset:offset+atk])\n",
    "            hess[offset:offset+atk]  = norm * x_grad * (1.0 - x_grad)\n",
    "\n",
    "            offset += atk    \n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_log_loss_uma_ext(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    weights = train_data.get_weight()\n",
    "    \n",
    "    grads = np.zeros_like(labels, dtype=np.float64)\n",
    "    hess = np.zeros_like(grads)\n",
    "    \n",
    "    norm = 1.0 / float(len(labels))\n",
    "\n",
    "    exp_pl = np.exp(- preds * labels)\n",
    "\n",
    "    x_grad = weights * exp_pl\n",
    "\n",
    "    grads = norm * x_grad * (- labels)\n",
    "    hess  = norm * x_grad * (1.0 - x_grad)\n",
    "\n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>optimize_non_interferent_log_loss</code>\n",
    "\n",
    "$$\n",
    "\\alpha\\cdot L^A + (1-\\alpha)\\cdot L\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha \\cdot \\underbrace{\\Bigg[\\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}} \\log  \\left( \\sum_{\\mathbf{x}' \\in \\mathit{MaxAtk}({\\mathbf{x}},{A})} e^{\\ell(h(\\mathbf{x}'), y)} \\right)\\Bigg]}_{L^A} + (1-\\alpha) \\cdot \\underbrace{\\Bigg[\\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}} \\ell(h(\\mathbf{x}, y))\\Bigg]}_{L}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_non_interferent_log_loss(preds, train_data, alpha=1.0):\n",
    "    # binary logloss under maximal attack\n",
    "    # grads_uma, hess_uma = optimize_log_loss_uma(preds, train_data)\n",
    "    grads_uma, hess_uma = optimize_log_loss_uma_ext(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    grads_plain, hess_plain = optimize_log_loss(preds, train_data)\n",
    "    \n",
    "    #print (grads_uma.min(), grads_uma.max(), hess_uma.min(), hess_uma.max())\n",
    "    #print (grads_plain.min(), grads_plain.max(), hess_plain.min(), hess_plain.max())\n",
    "    \n",
    "    # combine the above two losses together\n",
    "    K = 100.0\n",
    "    grads = K * (alpha*grads_uma + (1.0-alpha)*grads_plain)\n",
    "    hess  = K * (alpha*hess_uma  + (1.0-alpha)*hess_plain)\n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using one objective function for both _standard_ and _non-interferent_ learning\n",
    "\n",
    "The advantage of the <code>optimize_non_interferent_log_loss</code> function defined above is that we can wrap it so that we can use it as the only objective function (<code>fobj</code>) passed in to LightGBM. \n",
    "\n",
    "In other words, if we call <code>fobj=optimize_non_interferent_log_loss</code> with <code>alpha=0.0</code>, this will end up optimizing (i.e., minimizing) the \"vanilla\" objective function (i.e., the standard binary log loss, defined by the function <code>optimize_log_loss</code> above).\n",
    "\n",
    "Conversely, calling <code>fobj=optimize_non_interferent_log_loss</code> with <code>alpha=1.0</code> turns into optimizing (i.e., minimizing) the full non-interferent objective function (i.e., the custom binary log loss under max attack, defined by the function <code>optimize_log_loss_uma</code> above).\n",
    "\n",
    "Anything that sits in between (i.e., <code>0 < alpha < 1</code>) optimizes an objective function that trades off between the standard and the full non-interferent term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard\n",
    "\n",
    "The following function is the one used for evaluating the quality of the learned model (either _standard_, _adversarial-boosting_, or _non-interferent_). This is the standard <code>avg_log_loss</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(p):\n",
    "    return np.log(p/(1-p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>avg_log_loss</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss(preds, train_data):\n",
    "    \n",
    "    labels = train_data.get_label()\n",
    "    losses = np.log(1.0 + np.exp(-preds*labels))\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return 'avg_binary_log_loss', avg_loss, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom\n",
    "\n",
    "Similarly to what we have done for <code>fobj</code>, <code>feval</code> can be computed from a weighted combination of two evaluation metrics:\n",
    "\n",
    "-  <code>avg_log_loss</code> (standard, defined above);\n",
    "-  <code>avg_log_loss_uma</code> (custom, defined below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>avg_log_loss_uma</code>\n",
    "\n",
    "This is the binary log loss yet modified to operate on groups of perturbed instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom metrics\n",
    "def binary_log_loss(pred, true_label):\n",
    "\n",
    "    return np.log(1.0 + np.exp(-pred * true_label))\n",
    "\n",
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss_uma(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    offset = 0\n",
    "    max_logloss = []\n",
    "    avg_max_logloss = 0.0\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "    \n",
    "        for atk in attack_lens:\n",
    "            losses = [binary_log_loss(h,t) for h,t in zip(preds[offset:offset+atk], labels[offset:offset+atk])]\n",
    "            max_logloss.append(max(losses))\n",
    "\n",
    "            offset += atk\n",
    "        \n",
    "        avg_max_logloss = np.mean(max_logloss)  \n",
    "\n",
    "    return 'avg_binary_log_loss_under_max_attack', avg_max_logloss, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>feval=avg_non_interferent_log_loss</code>\n",
    "\n",
    "Used for measuring the validity of any model (either _standard_, _baseline_, or _non-interferent_). More precisely, <code>avg_non_interferent_log_loss</code> is the weighted sum of the binary log loss and the binary log loss under maximal attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_non_interferent_log_loss(preds, train_data, alpha=1.0):\n",
    "    \n",
    "    # binary logloss under maximal attack\n",
    "    _, loss_uma, _    = avg_log_loss_uma(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    #_, loss_plain, _  = avg_log_loss(preds, train_data)\n",
    "    ids = []\n",
    "    attack_lens = train_data.get_group()\n",
    "    if attack_lens is not None:\n",
    "        offset=0\n",
    "        for atk in attack_lens:\n",
    "            ids += [offset]\n",
    "            offset += atk      \n",
    "            \n",
    "    ids = np.array(ids)\n",
    "    labels = train_data.get_label()\n",
    "    losses = binary_log_loss(pred=preds[ids], true_label=labels[ids])\n",
    "    loss_plain = np.mean(losses)\n",
    "\n",
    "    # combine the above two losses together\n",
    "    weighted_loss = alpha*loss_uma + (1.0-alpha)*loss_plain\n",
    "\n",
    "    return 'avg_non_interferent_log_loss [alpha={:.2f}]'.format(alpha), weighted_loss, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adv_boosting_data(model, data, groups):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    groups : grouping of same attacked instance \n",
    "    returns the new data matrix and new groups\n",
    "    \n",
    "    WARNING: currently works only for binary classification\n",
    "    '''\n",
    "    # score the datataset\n",
    "    labels = data[:,-1]\n",
    "    \n",
    "    # check mispredictions\n",
    "    predictions = model.predict(data[:,:-1]) # exclude labels\n",
    "    matchings = labels * predictions\n",
    "    \n",
    "    # select original data + attacked instances\n",
    "    new_selected = [] # id of selected instances\n",
    "    new_groups   = []\n",
    "    \n",
    "    offset = 0\n",
    "    for g in groups:\n",
    "        if g==0:\n",
    "            print (\"Error !!!!\")\n",
    "        elif g==1:\n",
    "            # there are no attacks, just add original\n",
    "            new_selected += [offset]\n",
    "            new_groups   += [1]\n",
    "        else:\n",
    "            # get a slice of the matching scores\n",
    "            g_matchings = matchings[offset:offset+g]\n",
    "\n",
    "            # most misclassified (smallest margin)\n",
    "            # skip original\n",
    "            adv_instance = np.argmin(g_matchings[1:])+1\n",
    "\n",
    "            # add original and adversarial\n",
    "            new_selected += [offset, offset+adv_instance]\n",
    "            new_groups   += [2]\n",
    "        \n",
    "        offset += g\n",
    "    \n",
    "    new_dataset = data[new_selected,:]\n",
    "    \n",
    "    return new_dataset, new_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_adv_boosting_model(train, valid, cat_fx, input_model=None, num_trees=1, params=None):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    returns the new model (is model modified inplace?)\n",
    "    '''\n",
    "    \n",
    "    if cat_fx is None or len(cat_fx)==0:\n",
    "        cat_fx = \"auto\"\n",
    "        \n",
    "    assert train.shape[1]==valid.shape[1], \"Train/Valid Mismatch!\"\n",
    "\n",
    "    lgbm_train = lightgbm.Dataset(data=train[:,:-1], \n",
    "                                  label=train[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_valid = lightgbm.Dataset(data=valid[:,:-1], \n",
    "                                  label=valid[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_info = {}\n",
    "    lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                num_boost_round = num_trees, \n",
    "                                init_model = input_model,\n",
    "                                fobj = optimize_log_loss, \n",
    "                                feval = avg_log_loss,\n",
    "                                evals_result = lgbm_info,\n",
    "                                valid_sets   = [lgbm_train, lgbm_valid], \n",
    "                                valid_names  = ['train', 'valid'],\n",
    "                                verbose_eval=5)\n",
    "\n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdvBoosting(atk_train, atk_valid, trees, \n",
    "                 cat_fx,\n",
    "                 params,\n",
    "                 output_model_file,\n",
    "                 partial_save=1000, \n",
    "                 adv_rounds=1):\n",
    "    ''' \n",
    "    atk_data: full dataset including all valid attacks\n",
    "    atk_groups: lenght of each attack set\n",
    "    trees: total number of trees to be produced\n",
    "    adv_rounds: adversarial instance injecting frequency\n",
    "    '''\n",
    "    # temp lgbm file\n",
    "    temp = output_model_file+\".tmp\"\n",
    "    \n",
    "    # get groups and remove instance ids\n",
    "    atk_groups = atk_train['instance_id'].value_counts().sort_index().values\n",
    "    atk_valid_groups = atk_valid['instance_id'].value_counts().sort_index().values\n",
    "    \n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(atk_train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])  \n",
    "    # print (\"CatFX:\", atk_train.columns.values[cat_fx])\n",
    "\n",
    "    # prepare data (avoiding pandas)\n",
    "    atk_data   = atk_train.iloc[:,1:].values\n",
    "    atk_valid  = atk_valid.iloc[:,1:].values\n",
    "    cat_fx = [x - 1 for x in cat_fx]\n",
    "\n",
    "    # train first trees\n",
    "    original_ids = np.cumsum(atk_groups[:-1])\n",
    "    original_ids = np.insert(original_ids, 0, 0)\n",
    "    \n",
    "    original_valid_ids = np.cumsum(atk_valid_groups[:-1])\n",
    "    original_valid_ids = np.insert(original_valid_ids, 0, 0)\n",
    "    \n",
    "    model, model_info = extend_adv_boosting_model(atk_data[original_ids, :], \n",
    "                                                  atk_valid[original_valid_ids, :],\n",
    "                                                  cat_fx=cat_fx,\n",
    "                                                  input_model=None, \n",
    "                                                  num_trees=adv_rounds, \n",
    "                                                  params=params)\n",
    "    \n",
    "    best_model = model\n",
    "    best_info = model_info\n",
    "    best_loss = np.min(model_info['valid']['avg_binary_log_loss'])\n",
    "    best_round = 1\n",
    "        \n",
    "    # train remaining trees\n",
    "    for t in range(adv_rounds+1, trees+1, adv_rounds):\n",
    "        # attack dataset\n",
    "        adv_data, _       = gen_adv_boosting_data(model, atk_data, atk_groups)\n",
    "        adv_valid_data, _ = gen_adv_boosting_data(model, atk_valid, atk_valid_groups)\n",
    "        \n",
    "        # train additional trees\n",
    "        model.save_model(temp)\n",
    "        model, model_info = extend_adv_boosting_model(adv_data, \n",
    "                                                      adv_valid_data,\n",
    "                                                      cat_fx=cat_fx,\n",
    "                                                      input_model=temp, \n",
    "                                                      num_trees=adv_rounds, \n",
    "                                                      params=params)\n",
    "\n",
    "        if np.min(model_info['valid']['avg_binary_log_loss']) < best_loss:\n",
    "            best_model = model\n",
    "            best_info  = model_info\n",
    "            best_loss  = np.min(model_info['valid']['avg_binary_log_loss'])\n",
    "            best_round = t\n",
    "        \n",
    "        # save partial model\n",
    "        if t % partial_save == 0 and t != trees:\n",
    "            partial_filename = \"{:s}_T{:d}-of-{:d}_S{:04d}_L{:d}.model.tmp\".format(output_model_file, \n",
    "                                                                                   t, \n",
    "                                                                                   trees, \n",
    "                                                                                   int(params['learning_rate'] * 1000),\n",
    "                                                                                   params['num_leaves']\n",
    "                                                                                  )\n",
    "            \n",
    "            print(\"Save partial model to {}\".format(partial_filename))\n",
    "            model.save_model(filename=partial_filename)\n",
    "            \n",
    "    \n",
    "    return model, model_info, best_loss, best_round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Standard GBDT (_baseline 1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gradient_boosting_baseline( train_file, valid_file, test_file,\n",
    "                                output_model_file):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'best_round', 'avg_binary_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    \n",
    "    assert \"instance_id\" not in train.columns.values, \"Wrong training set file for GBDT\"\n",
    "\n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])  \n",
    "    print (\"CatFX:\", train.columns.values[cat_fx])\n",
    "    \n",
    "\n",
    "    for num_trees in [500]:\n",
    "        best_model = None\n",
    "        best_info = None\n",
    "        best_loss = np.inf\n",
    "        for learning_rate in [0.1]: #[0.01, 0.05, 0.1]:\n",
    "            for num_leaves in [16]: #[8, 16, 24]:\n",
    "                # datasets\n",
    "                lgbm_train = lightgbm.Dataset(data=train.iloc[:,:-1].values, \n",
    "                                              label=train.iloc[:,-1].values,\n",
    "                                              categorical_feature = cat_fx)\n",
    "\n",
    "                lgbm_valid = lightgbm.Dataset(data=valid.iloc[:,:-1].values, \n",
    "                                              label=valid.iloc[:,-1].values,\n",
    "                                              categorical_feature = cat_fx)\n",
    "\n",
    "                # run train\n",
    "                lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                'num_leaves': num_leaves} \n",
    "                lgbm_info = {}\n",
    "                lgbm_model = lightgbm.train(lgbm_params, lgbm_train, \n",
    "                                            num_boost_round = num_trees,\n",
    "                                            fobj            = optimize_log_loss, \n",
    "                                            feval           = avg_log_loss,\n",
    "                                            evals_result    = lgbm_info,\n",
    "                                            valid_sets      = [lgbm_train, lgbm_valid], \n",
    "                                            valid_names     = ['train', 'valid'],\n",
    "                                            verbose_eval    = 5)\n",
    "                \n",
    "                if np.min(lgbm_info['valid']['avg_binary_log_loss']) < best_loss:\n",
    "                    best_model = lgbm_model\n",
    "                    best_info = lgbm_info\n",
    "                    best_loss = np.min(lgbm_info['valid']['avg_binary_log_loss'])\n",
    "                    best_info['num_trees'] = num_trees\n",
    "                    best_info['learning_rate'] = learning_rate\n",
    "                    best_info['num_leaves'] = num_leaves\n",
    "                    \n",
    "                    \n",
    "                best_valid_iter = np.argmin(lgbm_info['valid']['avg_binary_log_loss'])\n",
    "                \n",
    "                # update experimental results\n",
    "                exp = exp.append({'num_trees': num_trees, \n",
    "                                  'learning_rate':learning_rate,\n",
    "                                  'num_leaves':num_leaves, \n",
    "                                  'best_round':best_valid_iter+1, \n",
    "                                  'avg_binary_log_loss':lgbm_info['valid']['avg_binary_log_loss'][best_valid_iter]},\n",
    "                                 ignore_index=True)\n",
    "                \n",
    "        \n",
    "        # save file\n",
    "        best_valid_iter = np.argmin(best_info['valid']['avg_binary_log_loss'])\n",
    "\n",
    "        model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                        best_info['num_trees'],\n",
    "                                                                        int(best_info['learning_rate']*1000),\n",
    "                                                                        best_info['num_leaves'],\n",
    "                                                                        best_valid_iter + 1\n",
    "                                                                       )\n",
    "        \n",
    "        best_model.save_model(model_file_name)\n",
    "        print (\"Model saved to\", model_file_name)\n",
    "        \n",
    "        best_model = lightgbm.Booster(model_file=model_file_name)\n",
    "        print (\"Check valid score:\", avg_log_loss(preds=best_model.predict(valid.iloc[:,:-1].values),\n",
    "                                                  train_data=lgbm_valid))\n",
    "\n",
    "    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# enable/disable LGBM Baseline\n",
    "if True:\n",
    "    experiments = train_gradient_boosting_baseline(\"../data/census/train_ori.csv.bz2\",\n",
    "                                                     \"../data/census/valid_ori.csv.bz2\",\n",
    "                                                     \"../data/census/test_ori.csv.bz2\",\n",
    "                                                     \"../out/models/std_gbdt_census\")  \n",
    "\n",
    "    experiments.to_csv('../out/models/std_gbdt_census.csv', index=False)\n",
    "\n",
    "    print (experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Model saved to ../out/models/std_gbdt_census_T500_S0100_L8_R493.model\n",
    "       num_trees  learning_rate  num_leaves  best_round  avg_binary_log_loss\n",
    "    0      500.0           0.01         8.0       500.0             0.358958\n",
    "    1      500.0           0.01        16.0       500.0             0.336156\n",
    "    2      500.0           0.01        24.0       500.0             0.329954\n",
    "    3      500.0           0.05         8.0       500.0             0.298972\n",
    "    4      500.0           0.05        16.0       498.0             0.296245\n",
    "    5      500.0           0.05        24.0       493.0             0.298961\n",
    "    6      500.0           0.10         8.0       493.0             0.292258\n",
    "    7      500.0           0.10        16.0       394.0             0.295525\n",
    "    8      500.0           0.10        24.0       234.0             0.298616\n",
    "    \n",
    "Best model\n",
    "\n",
    "    6      500.0           0.10         8.0       493.0             0.292258\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Adversarial Boosting (_baseline 2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial_boosting(train_file, valid_file, test_file, output_model_file):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'best_round', 'avg_binary_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    assert \"instance_id\" in train.columns.values, \"Wrong training set file for GBDT\"\n",
    "\n",
    "    for num_trees in [200]:\n",
    "        for learning_rate in [0.1]: #[0.01, 0.05, 0.1]:\n",
    "            for num_leaves in [16]: #[8, 16, 24]:\n",
    "                      \n",
    "                lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                'num_leaves': num_leaves} \n",
    "                \n",
    "                lgbm_model, lgbm_info, best_loss, best_valid_iter = AdvBoosting(train,\n",
    "                                                    valid,\n",
    "                                                    trees=num_trees, \n",
    "                                                    cat_fx = cat_fx, \n",
    "                                                    output_model_file=output_model_file, \n",
    "                                                    adv_rounds=1,\n",
    "                                                    params=lgbm_params)\n",
    "                \n",
    "\n",
    "                ####\n",
    "                #best_model = lightgbm.Booster(model_file=model_file_name)\n",
    "                atk_valid_groups = valid['instance_id'].value_counts().sort_index().values\n",
    "                cat_fx = np.where(valid.columns.isin(cat_fx))[0]\n",
    "                cat_fx = list([int(x) for x in cat_fx])  \n",
    "\n",
    "                atk_valid  = valid.iloc[:,1:].values\n",
    "                cat_fx = [x - 1 for x in cat_fx]\n",
    "                \n",
    "                original_valid_ids = np.cumsum(atk_valid_groups[:-1])\n",
    "                original_valid_ids = np.insert(original_valid_ids, 0, 0)\n",
    "\n",
    "                lgbm_valid = lightgbm.Dataset(data=atk_valid[original_valid_ids,:-1], \n",
    "                                              label=atk_valid[original_valid_ids,-1],\n",
    "                                              categorical_feature = cat_fx)\n",
    "                \n",
    "                \n",
    "                lgbm_valid_att = lightgbm.Dataset(data=atk_valid[:,:-1], \n",
    "                                              label=atk_valid[:,-1],\n",
    "                                              categorical_feature = cat_fx)\n",
    "                \n",
    "                print (\"Check valid score without attacks:\", avg_log_loss(preds=lgbm_model.predict(atk_valid[original_valid_ids,:-1]),\n",
    "                                                  train_data=lgbm_valid))\n",
    "                \n",
    "                print (\"Check valid score with attacks:\", avg_log_loss(preds=lgbm_model.predict(atk_valid[:,:-1]),\n",
    "                                                                       train_data=lgbm_valid_att))\n",
    "\n",
    "                \n",
    "                ####\n",
    "                # update experimental results\n",
    "                exp = exp.append({'num_trees': num_trees, \n",
    "                                  'learning_rate':learning_rate,\n",
    "                                  'num_leaves':num_leaves, \n",
    "                                  'best_round':best_valid_iter, \n",
    "                                  'avg_binary_log_loss':best_loss},\n",
    "                                 ignore_index=True)\n",
    "        \n",
    "        # save file\n",
    "        model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                                num_trees,\n",
    "                                                                                int(learning_rate*1000),\n",
    "                                                                                num_leaves,\n",
    "                                                                                best_valid_iter\n",
    "                                                                               )\n",
    "        lgbm_model.save_model(model_file_name)\n",
    "        print (\"Model saved to\", model_file_name)\n",
    "                \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# enable/disable\n",
    "if True:\n",
    "    for B in [150]: #[5, 15, 150, 300]:\n",
    "\n",
    "        experiments = train_adversarial_boosting ( \"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/test_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../out/models/adv_boosting_census_B{:d}\".format(B))  \n",
    "\n",
    "        experiments.to_csv('../out/models/adv_boosting_census_B{:d}.csv'.format(B), index=False)\n",
    "\n",
    "        print (experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Non-Interferent GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_non_interf_model(atk_train, atk_valid, cat_fx, \n",
    "                            alpha=1.0, num_trees=1, params=None):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    returns the new model (is model modified inplace?)\n",
    "    '''\n",
    "    \n",
    "    if cat_fx is None or len(cat_fx)==0:\n",
    "        cat_fx = \"auto\"\n",
    "        \n",
    "    assert atk_train.shape[1]==atk_valid.shape[1], \"Train/Valid Mismatch!\"\n",
    "    \n",
    "    train_groups = atk_train['instance_id'].value_counts().sort_index().values\n",
    "    valid_groups = atk_valid['instance_id'].value_counts().sort_index().values\n",
    "\n",
    "    original_train_ids = np.cumsum(train_groups[:-1])\n",
    "    original_train_ids = np.insert(original_train_ids, 0, 0)\n",
    "    \n",
    "    original_valid_ids = np.cumsum(valid_groups[:-1])\n",
    "    original_valid_ids = np.insert(original_valid_ids, 0, 0)\n",
    "\n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(atk_train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])\n",
    "    print (\"CatFX:\", atk_train.columns.values[cat_fx])\n",
    "\n",
    "    # remove instance id\n",
    "    atk_train = atk_train.iloc[:,1:].values\n",
    "    atk_valid = atk_valid.iloc[:,1:].values\n",
    "    cat_fx = [x - 1 for x in cat_fx]\n",
    "        \n",
    "    unatk_train = atk_train[original_train_ids,:]\n",
    "    unatk_valid = atk_valid[original_valid_ids,:]\n",
    "\n",
    "    \n",
    "    # -------------------------\n",
    "    # train first iteration\n",
    "    lgbm_train = lightgbm.Dataset(data=unatk_train[:,:-1], \n",
    "                                  label=unatk_train[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_valid = lightgbm.Dataset(data=unatk_valid[:,:-1], \n",
    "                                  label=unatk_valid[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "\n",
    "    lgbm_info = {}\n",
    "    lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                num_boost_round = 1,\n",
    "                                fobj  = optimize_log_loss, \n",
    "                                feval = avg_log_loss,\n",
    "                                evals_result = lgbm_info,\n",
    "                                valid_sets   = [lgbm_train, lgbm_valid], \n",
    "                                valid_names  = ['train', 'valid'],\n",
    "                                verbose_eval=5)\n",
    "\n",
    "    # -------------------------\n",
    "    # train other iteration\n",
    "    def get_ni_w(preds, labels, groups):\n",
    "        # \"weights\"\n",
    "        w = np.ones(len(groups))\n",
    "\n",
    "        offset = 0\n",
    "        for instance_id, g in enumerate(groups):\n",
    "            exp_pl = np.exp(- preds[offset:offset+g] * labels[offset:offset+g])\n",
    "            w[instance_id] = 1.0 / np.max(1.0 + exp_pl)\n",
    "            offset += g  \n",
    "\n",
    "        return w\n",
    "    \n",
    "    for t in range (1, num_trees):\n",
    "    \n",
    "        # get predictions on atk instances\n",
    "        train_preds  = lgbm_model.predict(atk_train[:,:-1])\n",
    "        train_labels = atk_train[:,-1]\n",
    "        train_weights = get_ni_w(train_preds, train_labels, train_groups)\n",
    "                \n",
    "        # repeat for validation\n",
    "        valid_preds  = lgbm_model.predict(atk_valid[:,:-1])\n",
    "        valid_labels = atk_valid[:,-1]\n",
    "        valid_weights = get_ni_w(valid_preds, valid_labels, valid_groups)\n",
    "        \n",
    "        # prepare data and train\n",
    "        lgbm_train = lightgbm.Dataset(data=unatk_train[:,:-1], \n",
    "                                      label=unatk_train[:,-1],\n",
    "                                      weight=train_weights,\n",
    "                                      categorical_feature = cat_fx)\n",
    "\n",
    "        lgbm_valid = lightgbm.Dataset(data=unatk_valid[:,:-1], \n",
    "                                      label=unatk_valid[:,-1],\n",
    "                                      weight=valid_weights,\n",
    "                                      categorical_feature = cat_fx)\n",
    "\n",
    "        new_lgbm_info = {}\n",
    "        lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                    num_boost_round = 1, \n",
    "                                    init_model = lgbm_model,\n",
    "                                    fobj  = functools.partial(optimize_non_interferent_log_loss, alpha=alpha), \n",
    "                                    feval = avg_log_loss,# functools.partial(avg_non_interferent_log_loss, alpha=alpha),\n",
    "                                    evals_result = new_lgbm_info,\n",
    "                                    valid_sets   = [lgbm_train, lgbm_valid], \n",
    "                                    valid_names  = ['train', 'valid'],\n",
    "                                    verbose_eval=5)\n",
    "        \n",
    "        awesome_hack = \"avg_binary_log_loss\"\n",
    "        lgbm_info['train'][awesome_hack] += new_lgbm_info['train'][awesome_hack]\n",
    "        lgbm_info['valid'][awesome_hack] += new_lgbm_info['valid'][awesome_hack]\n",
    "\n",
    "\n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_non_interferent(train_file, valid_file, test_file, output_model_file):\n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'alpha', 'best_round', 'avg_non_interferent_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    atk_train, atk_valid, atk_test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    \n",
    "    for num_trees in [300]:\n",
    "        for alpha in [0.01, 0.05, 1.0]: #[0.25, 0.50, 0.75, 1.00]:\n",
    "            best_model = None\n",
    "            best_info = None\n",
    "            best_loss = np.inf\n",
    "            awesome_hack = \"avg_non_interferent_log_loss\" + \" [alpha={:.2f}]\".format(alpha)\n",
    "            awesome_hack = \"avg_binary_log_loss\"\n",
    "            \n",
    "            for learning_rate in [0.1]: #[0.01, 0.05, 0.1]:\n",
    "                for num_leaves in [16]: #[8, 16, 24, 32]:\n",
    "                    \n",
    "                    \n",
    "                    lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                    'num_leaves': num_leaves} \n",
    "                    lgbm_model, lgbm_info = extend_non_interf_model(atk_train, atk_valid, cat_fx, \n",
    "                                alpha=alpha, num_trees=num_trees, params=lgbm_params)\n",
    "                    \n",
    "                    if np.min(lgbm_info['valid'][awesome_hack]) < best_loss:\n",
    "                        best_model = lgbm_model\n",
    "                        best_info = lgbm_info\n",
    "                        best_loss = np.min(lgbm_info['valid'][awesome_hack])\n",
    "                        best_info['num_trees'] = num_trees\n",
    "                        best_info['learning_rate'] = learning_rate\n",
    "                        best_info['num_leaves'] = num_leaves\n",
    "                \n",
    "\n",
    "                    # save file\n",
    "\n",
    "                    best_valid_iter = np.argmin(lgbm_info['valid'][awesome_hack])\n",
    "\n",
    "                    # update experimental results\n",
    "                    exp = exp.append({'num_trees': num_trees, \n",
    "                                  'learning_rate':learning_rate,\n",
    "                                  'num_leaves':num_leaves, \n",
    "                                  'alpha': alpha,\n",
    "                                  'best_round':best_valid_iter+1, \n",
    "                                  'avg_non_interferent_log_loss':lgbm_info['valid'][awesome_hack][best_valid_iter]},\n",
    "                                 ignore_index=True)\n",
    "            \n",
    "                best_valid_iter = np.argmin(best_info['valid'][awesome_hack])\n",
    "            \n",
    "                model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_A{:03d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                                    best_info['num_trees'],\n",
    "                                                                                    int(best_info['learning_rate']*1000),\n",
    "                                                                                    best_info['num_leaves'],\n",
    "                                                                                    int(alpha * 100),\n",
    "                                                                                    best_valid_iter + 1\n",
    "                                                                                   )\n",
    "            \n",
    "            \n",
    "                best_model.save_model(model_file_name)\n",
    "                print (\"Model saved to\", model_file_name)\n",
    "                    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n",
      "CatFX: ['workclass' 'marital_status' 'occupation' 'relationship' 'race' 'sex'\n",
      " 'native_country']\n",
      "[5]\ttrain's avg_binary_log_loss: 0.518744\tvalid's avg_binary_log_loss: 0.519532\n",
      "[10]\ttrain's avg_binary_log_loss: 0.435351\tvalid's avg_binary_log_loss: 0.437038\n",
      "[15]\ttrain's avg_binary_log_loss: 0.378209\tvalid's avg_binary_log_loss: 0.3797\n",
      "[20]\ttrain's avg_binary_log_loss: 0.346649\tvalid's avg_binary_log_loss: 0.348714\n",
      "[25]\ttrain's avg_binary_log_loss: 0.329043\tvalid's avg_binary_log_loss: 0.331692\n",
      "[30]\ttrain's avg_binary_log_loss: 0.315199\tvalid's avg_binary_log_loss: 0.317957\n",
      "[35]\ttrain's avg_binary_log_loss: 0.306508\tvalid's avg_binary_log_loss: 0.309562\n",
      "[40]\ttrain's avg_binary_log_loss: 0.300496\tvalid's avg_binary_log_loss: 0.304136\n",
      "[45]\ttrain's avg_binary_log_loss: 0.295937\tvalid's avg_binary_log_loss: 0.300053\n",
      "[50]\ttrain's avg_binary_log_loss: 0.291981\tvalid's avg_binary_log_loss: 0.297064\n",
      "[55]\ttrain's avg_binary_log_loss: 0.288955\tvalid's avg_binary_log_loss: 0.294856\n",
      "[60]\ttrain's avg_binary_log_loss: 0.286114\tvalid's avg_binary_log_loss: 0.292793\n",
      "[65]\ttrain's avg_binary_log_loss: 0.283703\tvalid's avg_binary_log_loss: 0.29114\n",
      "[70]\ttrain's avg_binary_log_loss: 0.2814\tvalid's avg_binary_log_loss: 0.289788\n",
      "[75]\ttrain's avg_binary_log_loss: 0.279413\tvalid's avg_binary_log_loss: 0.288734\n",
      "[80]\ttrain's avg_binary_log_loss: 0.277817\tvalid's avg_binary_log_loss: 0.288317\n",
      "[85]\ttrain's avg_binary_log_loss: 0.276413\tvalid's avg_binary_log_loss: 0.287654\n",
      "[90]\ttrain's avg_binary_log_loss: 0.275004\tvalid's avg_binary_log_loss: 0.287206\n",
      "[95]\ttrain's avg_binary_log_loss: 0.273531\tvalid's avg_binary_log_loss: 0.286619\n",
      "[100]\ttrain's avg_binary_log_loss: 0.272251\tvalid's avg_binary_log_loss: 0.286058\n",
      "[105]\ttrain's avg_binary_log_loss: 0.270977\tvalid's avg_binary_log_loss: 0.285519\n",
      "[110]\ttrain's avg_binary_log_loss: 0.269763\tvalid's avg_binary_log_loss: 0.285416\n",
      "[115]\ttrain's avg_binary_log_loss: 0.268565\tvalid's avg_binary_log_loss: 0.28532\n",
      "[120]\ttrain's avg_binary_log_loss: 0.267573\tvalid's avg_binary_log_loss: 0.28501\n",
      "[125]\ttrain's avg_binary_log_loss: 0.266416\tvalid's avg_binary_log_loss: 0.284997\n",
      "[130]\ttrain's avg_binary_log_loss: 0.265155\tvalid's avg_binary_log_loss: 0.284535\n",
      "[135]\ttrain's avg_binary_log_loss: 0.264365\tvalid's avg_binary_log_loss: 0.284043\n",
      "[140]\ttrain's avg_binary_log_loss: 0.263422\tvalid's avg_binary_log_loss: 0.283891\n",
      "[145]\ttrain's avg_binary_log_loss: 0.262506\tvalid's avg_binary_log_loss: 0.283668\n",
      "[150]\ttrain's avg_binary_log_loss: 0.261735\tvalid's avg_binary_log_loss: 0.283578\n",
      "[155]\ttrain's avg_binary_log_loss: 0.260798\tvalid's avg_binary_log_loss: 0.283202\n",
      "[160]\ttrain's avg_binary_log_loss: 0.260065\tvalid's avg_binary_log_loss: 0.283136\n",
      "[165]\ttrain's avg_binary_log_loss: 0.259194\tvalid's avg_binary_log_loss: 0.283042\n",
      "[170]\ttrain's avg_binary_log_loss: 0.258345\tvalid's avg_binary_log_loss: 0.282783\n",
      "[175]\ttrain's avg_binary_log_loss: 0.257662\tvalid's avg_binary_log_loss: 0.282848\n",
      "[180]\ttrain's avg_binary_log_loss: 0.256839\tvalid's avg_binary_log_loss: 0.282878\n",
      "[185]\ttrain's avg_binary_log_loss: 0.256236\tvalid's avg_binary_log_loss: 0.282981\n",
      "[190]\ttrain's avg_binary_log_loss: 0.25558\tvalid's avg_binary_log_loss: 0.283015\n",
      "[195]\ttrain's avg_binary_log_loss: 0.254834\tvalid's avg_binary_log_loss: 0.283133\n",
      "[200]\ttrain's avg_binary_log_loss: 0.254112\tvalid's avg_binary_log_loss: 0.283177\n",
      "[205]\ttrain's avg_binary_log_loss: 0.253462\tvalid's avg_binary_log_loss: 0.28302\n",
      "[210]\ttrain's avg_binary_log_loss: 0.252726\tvalid's avg_binary_log_loss: 0.28299\n",
      "[215]\ttrain's avg_binary_log_loss: 0.252094\tvalid's avg_binary_log_loss: 0.282977\n",
      "[220]\ttrain's avg_binary_log_loss: 0.251427\tvalid's avg_binary_log_loss: 0.2831\n",
      "[225]\ttrain's avg_binary_log_loss: 0.250777\tvalid's avg_binary_log_loss: 0.283163\n",
      "[230]\ttrain's avg_binary_log_loss: 0.250065\tvalid's avg_binary_log_loss: 0.283291\n",
      "[235]\ttrain's avg_binary_log_loss: 0.249505\tvalid's avg_binary_log_loss: 0.283236\n",
      "[240]\ttrain's avg_binary_log_loss: 0.24884\tvalid's avg_binary_log_loss: 0.283205\n",
      "[245]\ttrain's avg_binary_log_loss: 0.248192\tvalid's avg_binary_log_loss: 0.282972\n",
      "[250]\ttrain's avg_binary_log_loss: 0.247375\tvalid's avg_binary_log_loss: 0.282933\n",
      "[255]\ttrain's avg_binary_log_loss: 0.246727\tvalid's avg_binary_log_loss: 0.28292\n",
      "[260]\ttrain's avg_binary_log_loss: 0.245972\tvalid's avg_binary_log_loss: 0.282953\n",
      "[265]\ttrain's avg_binary_log_loss: 0.245448\tvalid's avg_binary_log_loss: 0.282931\n",
      "[270]\ttrain's avg_binary_log_loss: 0.244868\tvalid's avg_binary_log_loss: 0.283028\n",
      "[275]\ttrain's avg_binary_log_loss: 0.24429\tvalid's avg_binary_log_loss: 0.282964\n",
      "[280]\ttrain's avg_binary_log_loss: 0.243532\tvalid's avg_binary_log_loss: 0.282871\n",
      "[285]\ttrain's avg_binary_log_loss: 0.242958\tvalid's avg_binary_log_loss: 0.282935\n",
      "[290]\ttrain's avg_binary_log_loss: 0.242258\tvalid's avg_binary_log_loss: 0.282945\n",
      "[295]\ttrain's avg_binary_log_loss: 0.241767\tvalid's avg_binary_log_loss: 0.282994\n",
      "[300]\ttrain's avg_binary_log_loss: 0.241127\tvalid's avg_binary_log_loss: 0.282984\n",
      "Model saved to ../out/models/non_interferent_census_B150_T300_S0100_L16_A001_R167.model\n",
      "CatFX: ['workclass' 'marital_status' 'occupation' 'relationship' 'race' 'sex'\n",
      " 'native_country']\n",
      "[5]\ttrain's avg_binary_log_loss: 0.518755\tvalid's avg_binary_log_loss: 0.519543\n",
      "[10]\ttrain's avg_binary_log_loss: 0.435307\tvalid's avg_binary_log_loss: 0.436772\n",
      "[15]\ttrain's avg_binary_log_loss: 0.379306\tvalid's avg_binary_log_loss: 0.380562\n",
      "[20]\ttrain's avg_binary_log_loss: 0.346041\tvalid's avg_binary_log_loss: 0.347624\n",
      "[25]\ttrain's avg_binary_log_loss: 0.328663\tvalid's avg_binary_log_loss: 0.33082\n",
      "[30]\ttrain's avg_binary_log_loss: 0.315032\tvalid's avg_binary_log_loss: 0.317333\n",
      "[35]\ttrain's avg_binary_log_loss: 0.306319\tvalid's avg_binary_log_loss: 0.308921\n",
      "[40]\ttrain's avg_binary_log_loss: 0.300273\tvalid's avg_binary_log_loss: 0.303354\n",
      "[45]\ttrain's avg_binary_log_loss: 0.295742\tvalid's avg_binary_log_loss: 0.299514\n",
      "[50]\ttrain's avg_binary_log_loss: 0.291939\tvalid's avg_binary_log_loss: 0.296709\n",
      "[55]\ttrain's avg_binary_log_loss: 0.288874\tvalid's avg_binary_log_loss: 0.294237\n",
      "[60]\ttrain's avg_binary_log_loss: 0.285849\tvalid's avg_binary_log_loss: 0.2919\n",
      "[65]\ttrain's avg_binary_log_loss: 0.28337\tvalid's avg_binary_log_loss: 0.290484\n",
      "[70]\ttrain's avg_binary_log_loss: 0.281313\tvalid's avg_binary_log_loss: 0.289553\n",
      "[75]\ttrain's avg_binary_log_loss: 0.279302\tvalid's avg_binary_log_loss: 0.288407\n",
      "[80]\ttrain's avg_binary_log_loss: 0.277576\tvalid's avg_binary_log_loss: 0.286982\n",
      "[85]\ttrain's avg_binary_log_loss: 0.276201\tvalid's avg_binary_log_loss: 0.286522\n",
      "[90]\ttrain's avg_binary_log_loss: 0.274681\tvalid's avg_binary_log_loss: 0.286013\n",
      "[95]\ttrain's avg_binary_log_loss: 0.273365\tvalid's avg_binary_log_loss: 0.285857\n",
      "[100]\ttrain's avg_binary_log_loss: 0.272168\tvalid's avg_binary_log_loss: 0.285293\n",
      "[105]\ttrain's avg_binary_log_loss: 0.270955\tvalid's avg_binary_log_loss: 0.28491\n",
      "[110]\ttrain's avg_binary_log_loss: 0.269689\tvalid's avg_binary_log_loss: 0.284873\n",
      "[115]\ttrain's avg_binary_log_loss: 0.268542\tvalid's avg_binary_log_loss: 0.28476\n",
      "[120]\ttrain's avg_binary_log_loss: 0.267574\tvalid's avg_binary_log_loss: 0.284568\n",
      "[125]\ttrain's avg_binary_log_loss: 0.266592\tvalid's avg_binary_log_loss: 0.284366\n",
      "[130]\ttrain's avg_binary_log_loss: 0.265627\tvalid's avg_binary_log_loss: 0.284035\n",
      "[135]\ttrain's avg_binary_log_loss: 0.26453\tvalid's avg_binary_log_loss: 0.283665\n",
      "[140]\ttrain's avg_binary_log_loss: 0.263536\tvalid's avg_binary_log_loss: 0.283408\n",
      "[145]\ttrain's avg_binary_log_loss: 0.262749\tvalid's avg_binary_log_loss: 0.283304\n",
      "[150]\ttrain's avg_binary_log_loss: 0.261788\tvalid's avg_binary_log_loss: 0.282922\n",
      "[155]\ttrain's avg_binary_log_loss: 0.260945\tvalid's avg_binary_log_loss: 0.282776\n",
      "[160]\ttrain's avg_binary_log_loss: 0.260215\tvalid's avg_binary_log_loss: 0.282685\n",
      "[165]\ttrain's avg_binary_log_loss: 0.259464\tvalid's avg_binary_log_loss: 0.28245\n",
      "[170]\ttrain's avg_binary_log_loss: 0.258698\tvalid's avg_binary_log_loss: 0.282514\n",
      "[175]\ttrain's avg_binary_log_loss: 0.257776\tvalid's avg_binary_log_loss: 0.282466\n",
      "[180]\ttrain's avg_binary_log_loss: 0.257032\tvalid's avg_binary_log_loss: 0.282111\n",
      "[185]\ttrain's avg_binary_log_loss: 0.256207\tvalid's avg_binary_log_loss: 0.282304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[190]\ttrain's avg_binary_log_loss: 0.25552\tvalid's avg_binary_log_loss: 0.282076\n",
      "[195]\ttrain's avg_binary_log_loss: 0.254745\tvalid's avg_binary_log_loss: 0.282002\n",
      "[200]\ttrain's avg_binary_log_loss: 0.254098\tvalid's avg_binary_log_loss: 0.281902\n",
      "[205]\ttrain's avg_binary_log_loss: 0.25338\tvalid's avg_binary_log_loss: 0.281983\n",
      "[210]\ttrain's avg_binary_log_loss: 0.252605\tvalid's avg_binary_log_loss: 0.282105\n",
      "[215]\ttrain's avg_binary_log_loss: 0.251868\tvalid's avg_binary_log_loss: 0.2822\n",
      "[220]\ttrain's avg_binary_log_loss: 0.251259\tvalid's avg_binary_log_loss: 0.282225\n",
      "[225]\ttrain's avg_binary_log_loss: 0.250414\tvalid's avg_binary_log_loss: 0.282396\n",
      "[230]\ttrain's avg_binary_log_loss: 0.249794\tvalid's avg_binary_log_loss: 0.282404\n",
      "[235]\ttrain's avg_binary_log_loss: 0.249121\tvalid's avg_binary_log_loss: 0.282305\n",
      "[240]\ttrain's avg_binary_log_loss: 0.248281\tvalid's avg_binary_log_loss: 0.28246\n",
      "[245]\ttrain's avg_binary_log_loss: 0.247686\tvalid's avg_binary_log_loss: 0.282454\n",
      "[250]\ttrain's avg_binary_log_loss: 0.246826\tvalid's avg_binary_log_loss: 0.282491\n",
      "[255]\ttrain's avg_binary_log_loss: 0.246181\tvalid's avg_binary_log_loss: 0.282385\n",
      "[260]\ttrain's avg_binary_log_loss: 0.245299\tvalid's avg_binary_log_loss: 0.282345\n",
      "[265]\ttrain's avg_binary_log_loss: 0.24473\tvalid's avg_binary_log_loss: 0.282412\n",
      "[270]\ttrain's avg_binary_log_loss: 0.244041\tvalid's avg_binary_log_loss: 0.282363\n",
      "[275]\ttrain's avg_binary_log_loss: 0.243284\tvalid's avg_binary_log_loss: 0.282346\n",
      "[280]\ttrain's avg_binary_log_loss: 0.242669\tvalid's avg_binary_log_loss: 0.282402\n",
      "[285]\ttrain's avg_binary_log_loss: 0.242221\tvalid's avg_binary_log_loss: 0.282464\n",
      "[290]\ttrain's avg_binary_log_loss: 0.241556\tvalid's avg_binary_log_loss: 0.282744\n",
      "[295]\ttrain's avg_binary_log_loss: 0.241082\tvalid's avg_binary_log_loss: 0.282932\n",
      "[300]\ttrain's avg_binary_log_loss: 0.240361\tvalid's avg_binary_log_loss: 0.283015\n",
      "Model saved to ../out/models/non_interferent_census_B150_T300_S0100_L16_A005_R197.model\n",
      "CatFX: ['workclass' 'marital_status' 'occupation' 'relationship' 'race' 'sex'\n",
      " 'native_country']\n",
      "[5]\ttrain's avg_binary_log_loss: 0.518955\tvalid's avg_binary_log_loss: 0.519967\n",
      "[10]\ttrain's avg_binary_log_loss: 0.435474\tvalid's avg_binary_log_loss: 0.436906\n",
      "[15]\ttrain's avg_binary_log_loss: 0.380186\tvalid's avg_binary_log_loss: 0.381655\n",
      "[20]\ttrain's avg_binary_log_loss: 0.347954\tvalid's avg_binary_log_loss: 0.349692\n",
      "[25]\ttrain's avg_binary_log_loss: 0.329268\tvalid's avg_binary_log_loss: 0.332153\n",
      "[30]\ttrain's avg_binary_log_loss: 0.316535\tvalid's avg_binary_log_loss: 0.319881\n",
      "[35]\ttrain's avg_binary_log_loss: 0.308288\tvalid's avg_binary_log_loss: 0.311829\n",
      "[40]\ttrain's avg_binary_log_loss: 0.302732\tvalid's avg_binary_log_loss: 0.307041\n",
      "[45]\ttrain's avg_binary_log_loss: 0.298457\tvalid's avg_binary_log_loss: 0.303627\n",
      "[50]\ttrain's avg_binary_log_loss: 0.294611\tvalid's avg_binary_log_loss: 0.300561\n",
      "[55]\ttrain's avg_binary_log_loss: 0.291149\tvalid's avg_binary_log_loss: 0.297583\n",
      "[60]\ttrain's avg_binary_log_loss: 0.288184\tvalid's avg_binary_log_loss: 0.295744\n",
      "[65]\ttrain's avg_binary_log_loss: 0.285685\tvalid's avg_binary_log_loss: 0.294294\n",
      "[70]\ttrain's avg_binary_log_loss: 0.283738\tvalid's avg_binary_log_loss: 0.292957\n",
      "[75]\ttrain's avg_binary_log_loss: 0.282038\tvalid's avg_binary_log_loss: 0.292132\n",
      "[80]\ttrain's avg_binary_log_loss: 0.280296\tvalid's avg_binary_log_loss: 0.290978\n",
      "[85]\ttrain's avg_binary_log_loss: 0.278851\tvalid's avg_binary_log_loss: 0.290595\n",
      "[90]\ttrain's avg_binary_log_loss: 0.27729\tvalid's avg_binary_log_loss: 0.289488\n",
      "[95]\ttrain's avg_binary_log_loss: 0.275814\tvalid's avg_binary_log_loss: 0.289323\n",
      "[100]\ttrain's avg_binary_log_loss: 0.274703\tvalid's avg_binary_log_loss: 0.289132\n",
      "[105]\ttrain's avg_binary_log_loss: 0.273319\tvalid's avg_binary_log_loss: 0.288763\n",
      "[110]\ttrain's avg_binary_log_loss: 0.27221\tvalid's avg_binary_log_loss: 0.288345\n",
      "[115]\ttrain's avg_binary_log_loss: 0.270995\tvalid's avg_binary_log_loss: 0.288039\n",
      "[120]\ttrain's avg_binary_log_loss: 0.270128\tvalid's avg_binary_log_loss: 0.287796\n",
      "[125]\ttrain's avg_binary_log_loss: 0.269162\tvalid's avg_binary_log_loss: 0.287799\n",
      "[130]\ttrain's avg_binary_log_loss: 0.268269\tvalid's avg_binary_log_loss: 0.287541\n",
      "[135]\ttrain's avg_binary_log_loss: 0.26727\tvalid's avg_binary_log_loss: 0.287212\n",
      "[140]\ttrain's avg_binary_log_loss: 0.266417\tvalid's avg_binary_log_loss: 0.287201\n",
      "[145]\ttrain's avg_binary_log_loss: 0.265349\tvalid's avg_binary_log_loss: 0.287006\n",
      "[150]\ttrain's avg_binary_log_loss: 0.264534\tvalid's avg_binary_log_loss: 0.286699\n",
      "[155]\ttrain's avg_binary_log_loss: 0.263616\tvalid's avg_binary_log_loss: 0.286529\n",
      "[160]\ttrain's avg_binary_log_loss: 0.262842\tvalid's avg_binary_log_loss: 0.286655\n",
      "[165]\ttrain's avg_binary_log_loss: 0.262036\tvalid's avg_binary_log_loss: 0.286864\n",
      "[170]\ttrain's avg_binary_log_loss: 0.26122\tvalid's avg_binary_log_loss: 0.286763\n",
      "[175]\ttrain's avg_binary_log_loss: 0.260472\tvalid's avg_binary_log_loss: 0.286854\n",
      "[180]\ttrain's avg_binary_log_loss: 0.25983\tvalid's avg_binary_log_loss: 0.286976\n",
      "[185]\ttrain's avg_binary_log_loss: 0.25891\tvalid's avg_binary_log_loss: 0.286835\n",
      "[190]\ttrain's avg_binary_log_loss: 0.258204\tvalid's avg_binary_log_loss: 0.287068\n",
      "[195]\ttrain's avg_binary_log_loss: 0.257439\tvalid's avg_binary_log_loss: 0.286962\n",
      "[200]\ttrain's avg_binary_log_loss: 0.256558\tvalid's avg_binary_log_loss: 0.286547\n",
      "[205]\ttrain's avg_binary_log_loss: 0.255829\tvalid's avg_binary_log_loss: 0.286505\n",
      "[210]\ttrain's avg_binary_log_loss: 0.255102\tvalid's avg_binary_log_loss: 0.286251\n",
      "[215]\ttrain's avg_binary_log_loss: 0.254335\tvalid's avg_binary_log_loss: 0.286406\n",
      "[220]\ttrain's avg_binary_log_loss: 0.253543\tvalid's avg_binary_log_loss: 0.286252\n",
      "[225]\ttrain's avg_binary_log_loss: 0.252888\tvalid's avg_binary_log_loss: 0.286443\n",
      "[230]\ttrain's avg_binary_log_loss: 0.2524\tvalid's avg_binary_log_loss: 0.286375\n",
      "[235]\ttrain's avg_binary_log_loss: 0.251944\tvalid's avg_binary_log_loss: 0.286536\n",
      "[240]\ttrain's avg_binary_log_loss: 0.251187\tvalid's avg_binary_log_loss: 0.285999\n",
      "[245]\ttrain's avg_binary_log_loss: 0.250803\tvalid's avg_binary_log_loss: 0.28611\n",
      "[250]\ttrain's avg_binary_log_loss: 0.250188\tvalid's avg_binary_log_loss: 0.286239\n",
      "[255]\ttrain's avg_binary_log_loss: 0.249521\tvalid's avg_binary_log_loss: 0.286298\n",
      "[260]\ttrain's avg_binary_log_loss: 0.248932\tvalid's avg_binary_log_loss: 0.286492\n",
      "[265]\ttrain's avg_binary_log_loss: 0.248408\tvalid's avg_binary_log_loss: 0.286429\n",
      "[270]\ttrain's avg_binary_log_loss: 0.24766\tvalid's avg_binary_log_loss: 0.286534\n",
      "[275]\ttrain's avg_binary_log_loss: 0.246964\tvalid's avg_binary_log_loss: 0.28641\n",
      "[280]\ttrain's avg_binary_log_loss: 0.246175\tvalid's avg_binary_log_loss: 0.286255\n",
      "[285]\ttrain's avg_binary_log_loss: 0.245739\tvalid's avg_binary_log_loss: 0.28645\n",
      "[290]\ttrain's avg_binary_log_loss: 0.245147\tvalid's avg_binary_log_loss: 0.286309\n",
      "[295]\ttrain's avg_binary_log_loss: 0.244748\tvalid's avg_binary_log_loss: 0.286403\n",
      "[300]\ttrain's avg_binary_log_loss: 0.244187\tvalid's avg_binary_log_loss: 0.286465\n",
      "Model saved to ../out/models/non_interferent_census_B150_T300_S0100_L16_A100_R240.model\n",
      "   num_trees  learning_rate  num_leaves  alpha  best_round  \\\n",
      "0      300.0            0.1        16.0   0.01       167.0   \n",
      "1      300.0            0.1        16.0   0.05       197.0   \n",
      "2      300.0            0.1        16.0   1.00       240.0   \n",
      "\n",
      "   avg_non_interferent_log_loss  \n",
      "0                      0.282719  \n",
      "1                      0.281880  \n",
      "2                      0.285999  \n"
     ]
    }
   ],
   "source": [
    "# enable/disable\n",
    "if True:\n",
    "    for B in [150]: #[5, 15, 150, 300]:\n",
    "\n",
    "        experiments = train_non_interferent(\"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/test_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../out/models/non_interferent_census_B{:d}\".format(B))  \n",
    "\n",
    "        experiments.to_csv('../out/models/non_interferent_census_B{:d}.csv'.format(B), index=False)\n",
    "\n",
    "        print (experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Boosting with Our Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Tests & Old implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_non_interferent(train_file, valid_file, test_file, output_model_file):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'alpha', 'best_round', 'avg_non_interferent_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    train_groups = train['instance_id'].value_counts().sort_index().values\n",
    "    valid_groups = valid['instance_id'].value_counts().sort_index().values\n",
    "    test_groups = test['instance_id'].value_counts().sort_index().values\n",
    "    \n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])\n",
    "    print (\"CatFX:\", train.columns.values[cat_fx])\n",
    "    \n",
    "    # prepare data\n",
    "    train = train.iloc[:,1:]\n",
    "    valid = valid.iloc[:,1:]\n",
    "    test = test.iloc[:,1:]\n",
    "    \n",
    "    cat_fx = [x - 1 for x in cat_fx]\n",
    "\n",
    "    for num_trees in [100]:\n",
    "        for alpha in [0.5]: #[0.25, 0.50, 0.75, 1.00]:\n",
    "            best_model = None\n",
    "            best_info = None\n",
    "            best_loss = np.inf\n",
    "            awesome_hack = \"avg_non_interferent_log_loss\" + \" [alpha={:.2f}]\".format(alpha)\n",
    "            \n",
    "            for learning_rate in [0.1]: #[0.01, 0.05, 0.1]:\n",
    "                for num_leaves in [16]: #[8, 16, 24, 32]:\n",
    "                    # datasets\n",
    "                    lgbm_train = lightgbm.Dataset(data=train.iloc[:,:-1].values,\n",
    "                                                  label=train.iloc[:,-1].values,\n",
    "                                                  group=train_groups,\n",
    "                                                  categorical_feature = cat_fx)\n",
    "\n",
    "                    lgbm_valid = lightgbm.Dataset(data=valid.iloc[:,:-1].values,\n",
    "                                                  label=valid.iloc[:,-1].values,\n",
    "                                                  group=valid_groups,\n",
    "                                                  #reference=lgbm_train, \n",
    "                                                  #free_raw_data=False,\n",
    "                                                  categorical_feature = cat_fx)\n",
    "\n",
    "                    # run train\n",
    "                    lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                    'num_leaves': num_leaves} \n",
    "                    lgbm_info = {}\n",
    "                    lgbm_model = lightgbm.train(lgbm_params, lgbm_train, \n",
    "                                                num_boost_round = num_trees,\n",
    "                                                fobj            = lambda x,y: optimize_non_interferent_log_loss(x,y,alpha=alpha), # functools.partial(optimize_non_interferent_log_loss, alpha=alpha),\n",
    "                                                feval           = lambda x,y: avg_non_interferent_log_loss(x,y,alpha=alpha), #, functools.partial(avg_non_interferent_log_loss, alpha=alpha),\n",
    "                                                evals_result    = lgbm_info,\n",
    "                                                valid_sets      = [lgbm_train, lgbm_valid], \n",
    "                                                valid_names     = ['train', 'valid'],\n",
    "                                                verbose_eval    = 5)\n",
    "                    \n",
    "                    \n",
    "                    if np.min(lgbm_info['valid'][awesome_hack]) < best_loss:\n",
    "                        best_model = lgbm_model\n",
    "                        best_info = lgbm_info\n",
    "                        best_loss = np.min(lgbm_info['valid'][awesome_hack])\n",
    "                        best_info['num_trees'] = num_trees\n",
    "                        best_info['learning_rate'] = learning_rate\n",
    "                        best_info['num_leaves'] = num_leaves\n",
    "                \n",
    "\n",
    "                    # save file\n",
    "                    \n",
    "                    best_valid_iter = np.argmin(lgbm_info['valid'][awesome_hack])\n",
    "\n",
    "                    # update experimental results\n",
    "                    exp = exp.append({'num_trees': num_trees, \n",
    "                                      'learning_rate':learning_rate,\n",
    "                                      'num_leaves':num_leaves, \n",
    "                                      'alpha': alpha,\n",
    "                                      'best_round':best_valid_iter+1, \n",
    "                                      'avg_non_interferent_log_loss':lgbm_info['valid'][awesome_hack][best_valid_iter]},\n",
    "                                     ignore_index=True)\n",
    "            \n",
    "            best_valid_iter = np.argmin(best_info['valid'][awesome_hack])\n",
    "            \n",
    "            model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_A{:03d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                                    best_info['num_trees'],\n",
    "                                                                                    int(best_info['learning_rate']*1000),\n",
    "                                                                                    best_info['num_leaves'],\n",
    "                                                                                    int(alpha * 100),\n",
    "                                                                                    best_valid_iter + 1\n",
    "                                                                                   )\n",
    "            \n",
    "            \n",
    "            best_model.save_model(model_file_name)\n",
    "            print (\"Model saved to\", model_file_name)\n",
    "                    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# enable/disable\n",
    "if True:\n",
    "    for B in [150]: #[5, 15, 150, 300]:\n",
    "\n",
    "        experiments = train_non_interferent(\"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/test_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../out/models/non_interferent_census_B{:d}\".format(B))  \n",
    "\n",
    "        experiments.to_csv('../out/models/non_interferent_census_B{:d}.csv'.format(B), index=False)\n",
    "\n",
    "        print (experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check groups are the same after re-splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def check_groups(atk_train_file, atk_valid_file, atk_test_file):\n",
    "    # Load post-resplitting data\n",
    "    post_train, post_valid, post_test, cat_fx = load_atk_train_valid_test(atk_train_file, atk_valid_file, atk_test_file)\n",
    "    post_train_groups = post_train['instance_id'].value_counts().sort_index().values\n",
    "    post_valid_groups = post_valid['instance_id'].value_counts().sort_index().values\n",
    "    post_test_groups  = post_test['instance_id'].value_counts().sort_index().values\n",
    "    \n",
    "    # load pre-re-splitting data\n",
    "    pre_train = pd.read_csv(atk_train_file)\n",
    "    pre_valid = pd.read_csv(atk_valid_file)\n",
    "    pre_test  = pd.read_csv(atk_test_file)\n",
    "    pre_train_groups = pre_train['instance_id'].value_counts().sort_index().values\n",
    "    pre_valid_groups = pre_valid['instance_id'].value_counts().sort_index().values\n",
    "    pre_test_groups  = pre_test['instance_id'].value_counts().sort_index().values\n",
    "\n",
    "    # check global lenght\n",
    "    print (\"PRE  TOTAL instances:\", len(pre_train)+ len(pre_valid)+ len(pre_test))\n",
    "    print (\"POST TOTAL instances:\", len(post_train)+ len(post_valid)+ len(post_test))\n",
    "    \n",
    "    assert len(pre_train)+ len(pre_valid)+ len(pre_test) == \\\n",
    "            len(post_train)+ len(post_valid)+ len(post_test),\\\n",
    "            \"Different number of instances !\"\n",
    "            \n",
    "    # check groups\n",
    "    print (\"PRE  lengths in groups:\", len(pre_train_groups), len(pre_valid_groups), len(pre_test_groups))\n",
    "    print (\"POST lengths in groups:\", len(post_train_groups), len(post_valid_groups), len(post_test_groups))\n",
    "    pre_all_groups  = np.concatenate([pre_train_groups, pre_valid_groups, pre_test_groups])\n",
    "    post_all_groups = np.concatenate([post_train_groups, post_valid_groups, post_test_groups])\n",
    "    print (\"PRE  TOTAL groups:\", len(pre_all_groups))\n",
    "    print (\"POST TOTAL groups:\", len(post_all_groups))\n",
    "    \n",
    "    assert len(pre_all_groups)==len(post_all_groups),\\\n",
    "        \"Different number of groups!\"\n",
    "    \n",
    "    # compare values\n",
    "    assert (pre_all_groups==post_all_groups).all(),\\\n",
    "        \"Groups have different sizes!\"\n",
    "\n",
    "B=5\n",
    "check_groups(\"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                   \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                   \"../data/census/test_B{:d}.csv.bz2\".format(B)) \n",
    "B=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check extraction of non-attacked instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nonatk_ids(atk_data):\n",
    "    # get groups and remove instance ids\n",
    "    atk_groups = atk_data['instance_id'].value_counts().sort_index().values\n",
    "\n",
    "    original_ids = np.cumsum(atk_groups[:-1])\n",
    "    original_ids = np.insert(original_ids, 0, 0)\n",
    "\n",
    "    return original_ids\n",
    "    \n",
    "def check_nonatk_filter(atk_train_file, atk_valid_file, atk_test_file,\n",
    "                        train_file, valid_file, test_file):\n",
    "    # Load post-resplitting data\n",
    "    atk_train, atk_valid, atk_test, cat_fx = load_atk_train_valid_test(atk_train_file, atk_valid_file, atk_test_file)\n",
    "    \n",
    "    # Load non attacked instances\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "\n",
    "    # Filter atks and skip instance id\n",
    "    filtered_train = atk_train.iloc[get_nonatk_ids(atk_train),1:]\n",
    "    filtered_valid = atk_valid.iloc[get_nonatk_ids(atk_valid),1:]\n",
    "    filtered_test  = atk_test.iloc[get_nonatk_ids(atk_test),1:]\n",
    "\n",
    "    print (\"Attacked shapes\", atk_train.shape, atk_valid.shape, atk_test.shape)\n",
    "    print (\"Filteres shapes\", filtered_train.shape, filtered_valid.shape, filtered_test.shape)\n",
    "    print (\"Original shapes\", train.shape, valid.shape, test.shape)\n",
    "    \n",
    "    assert np.array_equal(train.values,filtered_train.values), \"Different Data !\"\n",
    "    assert np.array_equal(valid.values,filtered_valid.values), \"Different Data !\"\n",
    "    assert np.array_equal(test.values,filtered_test.values), \"Different Data !\"\n",
    "\n",
    "\n",
    "\n",
    "B=5\n",
    "check_nonatk_filter(\"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                       \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                       \"../data/census/test_B{:d}.csv.bz2\".format(B),\n",
    "                   \"../data/census/train_ori.csv.bz2\".format(B),\n",
    "                       \"../data/census/valid_ori.csv.bz2\".format(B),\n",
    "                       \"../data/census/test_ori.csv.bz2\".format(B) ) \n",
    "B=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(train_file, valid_file, test_file, output_model_file):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['C', 'avg_binary_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    X_train = train.iloc[:,:-1].values\n",
    "    y_train = train.iloc[:,-1].values\n",
    "    y_train[y_train == -1] = 0\n",
    "    \n",
    "    X_valid = valid.iloc[:,:-1].values\n",
    "    y_valid = valid.iloc[:,-1].values\n",
    "    \n",
    "    for c in [0.001, 0.01, 0.1, 1.0]:\n",
    "        \n",
    "        model = SVC(kernel='rbf', probability=True)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_preds = model.predict_proba(X_valid)[:,0]\n",
    "        cur_avg_binary_log_loss = np.mean(binary_log_loss(y_preds, y_valid))\n",
    "        \n",
    "        model_file_name = \"{:s}_C{:04d}.model\".format(output_model_file, int(c * 1000))\n",
    "        \n",
    "        with open(model_file_name, 'wb') as fout:\n",
    "            pickle.dump(model, fout)\n",
    "        \n",
    "        print (\"Model saved to\", model_file_name)\n",
    "        \n",
    "        # update experimental results\n",
    "        exp = exp.append({'C': c, \n",
    "                          'avg_binary_log_loss':cur_avg_binary_log_loss},\n",
    "                         ignore_index=True)\n",
    "    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable/disable LGBM Baseline\n",
    "if False:\n",
    "    experiments = train_svm ( \"../data/census/train_ori.csv.bz2\",\n",
    "                                                     \"../data/census/valid_ori.csv.bz2\",\n",
    "                                                     \"../data/census/test_ori.csv.bz2\",\n",
    "                                                     \"../out/models/svm_census\")  \n",
    "\n",
    "    experiments.to_csv('../out/models/svm_census.csv', index=False)\n",
    "\n",
    "    print (experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
