{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def load_dataset(path, \n",
    "                 dataset_filename, \n",
    "                 sep=\",\", \n",
    "                 header=None,\n",
    "                 nrows=None,\n",
    "                 names=None, \n",
    "                 index_col=False, \n",
    "                 na_values='?'):\n",
    "    \n",
    "    return pd.read_csv(path+\"/\"+dataset_filename, \n",
    "                       sep=sep, \n",
    "                       header=header,\n",
    "                       nrows=nrows,\n",
    "                       names=colnames, \n",
    "                       index_col=index_col, \n",
    "                       na_values='?')\n",
    "\n",
    "def binarize_data(data, label, threshold):\n",
    "    data[label] = np.where(data[label] >= threshold, 1, -1)\n",
    "\n",
    "def serialize_dataset(p_data, \n",
    "                      path, \n",
    "                      dataset_filename, \n",
    "                      suffix, \n",
    "                      sep=\",\", \n",
    "                      compression=\"bz2\", \n",
    "                      index=False):\n",
    "    \n",
    "    p_data.to_csv(path+\"/\"+dataset_filename.split(\".\")[0]+suffix, \n",
    "                  sep=sep, \n",
    "                  compression=compression, \n",
    "                  index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_instance(x, rules, budget, max_budget_per_feature, thresholds):\n",
    "    \"\"\"\n",
    "    Returns the set of possible perturbations of a given instance.\n",
    "\n",
    "    This function takes as input an instance and returns a set of perturbations of that instance, \n",
    "    using the specified amount of budget and considering the cost of perturbing each individual feature.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : pandas.Series\n",
    "        The original instance\n",
    "    rules : list\n",
    "        The list of modification rules\n",
    "    budget : float\n",
    "        The attacker's budget\n",
    "    max_budget_per_feature : dict\n",
    "        The maximum allowed amount of budget units that can be spend on each feature\n",
    "    thresholds : dict\n",
    "        feature -> list of relevant thresholds\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The set of perturbations (including the original instance, placed at the very beginning)\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the queue (FIFO) with both the original instance, \n",
    "    # the initial budget, and an empty dictionary of budget units spent so far\n",
    "    queue = [(x, budget, {})]\n",
    "    # visited perturbations\n",
    "    seen = { tuple(x): [budget, {}] }\n",
    "    # initialize the set of perturbations of this instance with the empty list\n",
    "    # perturbations = []\n",
    "    \n",
    "    # loop until the queue is not empty\n",
    "    while len(queue)>0:\n",
    "        item = queue.pop() # dequeue the first inserted element\n",
    "        x = item[0] # get the instance\n",
    "        b = item[1] # get the residual budget\n",
    "        budget_units_spent = item[2] # get the dictionary containing the amount of budget spent on each feature, so far\n",
    "        \n",
    "        # loop through all the features subject to the set of attack rules\n",
    "        for r in rules:\n",
    "            f = r['f']  # feature the rule applies to\n",
    "            # check budget\n",
    "            if not( r['cost'] <= b and budget_units_spent.get(f, 0) + r['cost'] <= max_budget_per_feature[f] ):\n",
    "                continue\n",
    "            # check validity\n",
    "            if not r['valid'](x):\n",
    "                continue\n",
    "            \n",
    "            # apply rule to a copy\n",
    "            x_atks = []\n",
    "            if r['is_cat']:\n",
    "                xx = x.copy()\n",
    "                xx[f] = r['value']\n",
    "                x_atks += [xx]\n",
    "            else:\n",
    "#                 xx = x.copy()\n",
    "#                 xx[f] += r['value']\n",
    "#                 x_atks += [xx]\n",
    "                \n",
    "                # Evaluate crossing of multiple thresholds\n",
    "                low,high=sorted([x[f], x[f]+r['value']])\n",
    "                z = set(thresholds[f][np.logical_and(thresholds[f]>=low, thresholds[f]<=high)])\n",
    "                z |= set([low,high])\n",
    "                for zi in z:\n",
    "                    xx = x.copy()\n",
    "                    xx[f] = zi\n",
    "                    x_atks += [xx]\n",
    "\n",
    "                # we are adding all of this to both seen and queue\n",
    "                # the smallest element might not be included in seen\n",
    "                #     as it is not an interesting attack\n",
    "                # still we are missing managing of rule validity thresholds\n",
    "#                 print (x)\n",
    "#                 print (x_prime)\n",
    "#                 print (thresholds[f])\n",
    "#                 print (thresholds[f][crossings[0]:crossings[1]])\n",
    "\n",
    "            # process all atks\n",
    "            for xx in x_atks:\n",
    "                # skip if already seen and with a larger residual budget\n",
    "                xx_t = tuple(xx)\n",
    "                res_b = b - r['cost']\n",
    "                seen_budgets = seen.get(xx_t)\n",
    "                if seen_budgets is not None and seen_budgets[0] >= res_b:\n",
    "                    continue\n",
    "\n",
    "                # update budgets spent\n",
    "                updated_budget_units_spent = budget_units_spent.copy()\n",
    "                updated_budget_units_spent[f] = updated_budget_units_spent.get(f,0) + r['cost']\n",
    "                # add to frontier and to past seen elements\n",
    "                seen[xx_t] = [res_b, updated_budget_units_spent]\n",
    "                queue.append([xx, res_b, updated_budget_units_spent])\n",
    "    \n",
    "    perturbations_df = pd.DataFrame.from_records(list(seen.keys()), columns=x.index.values)\n",
    "    perturbations_df = perturbations_df.drop_duplicates()\n",
    "    return perturbations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_dataset(data, budget, max_budget_per_feature, rules, skip_class=None):\n",
    "    \"\"\"\n",
    "    Returns the dataset extended with all instance perturbations.\n",
    "\n",
    "    This function takes as input a dataset and returns another dataset which is obtained from the original\n",
    "    by adding all the possible perturbations an attacker with budget B can apply to every instance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        The original dataset\n",
    "    rules : list\n",
    "        The list of modification rules\n",
    "    budget : float\n",
    "        The attacker's budget\n",
    "    max_budget_per_feature : dict\n",
    "        The maximum allowed amount of budget units that can be spend on each feature\n",
    "    costs : dict\n",
    "        A mapping between each feature and its cost of perturbation\n",
    "    skip_class : int\n",
    "        if class (i.e. last columns) equals skip_class, then instance is skipped\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The perturbed dataset\n",
    "\n",
    "    \"\"\"\n",
    "    if data is None or data.empty:\n",
    "        return # if not, just return None\n",
    "    \n",
    "    # compute valid thresholds\n",
    "    thresholds = {c:np.unique(data[c]) for c in data.columns}\n",
    "\n",
    "    # prepare the perturbed dataset to be returned, initially empty with an extra \"instance_id\" column\n",
    "    cols = [\"instance_id\"] + data.columns.tolist()\n",
    "    perturbed_data = pd.DataFrame(columns=cols)\n",
    "    \n",
    "    # start with instance_id = 1\n",
    "    instance_id = 1\n",
    "    perturbations = None\n",
    "    \n",
    "    # loop through every single instance in the original dataset\n",
    "    print(\"***** Loop through all the original instances... *****\")\n",
    "    for index, instance in data.iterrows():\n",
    "        if instance_id%500==0:\n",
    "            print(\"***** Perturbing instance [ID = #{}]... *****\".format(instance_id))  \n",
    "        \n",
    "        if skip_class is not None and instance[-1]==skip_class:\n",
    "            # keep the original instance only\n",
    "            perturbations = pd.DataFrame([instance])\n",
    "        else:\n",
    "            # apply perturbations\n",
    "            perturbations = perturb_instance(x=instance, rules=rules, budget=budget, \n",
    "                                             max_budget_per_feature=max_budget_per_feature,\n",
    "                                             thresholds=thresholds)\n",
    "            \n",
    "        perturbations.insert(loc=0, \n",
    "                             column=\"instance_id\", \n",
    "                             value=[instance_id for i in range(perturbations.shape[0])], \n",
    "                             allow_duplicates=True)\n",
    "\n",
    "        perturbed_data = perturbed_data.append(perturbations)\n",
    "        instance_id += 1\n",
    "        \n",
    "    # eventually, return the perturbed dataset\n",
    "    print(\"***** Return the final perturbed dataset *****\")\n",
    "    return perturbed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WINE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"wine2\"\n",
    "\n",
    "colnames = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide',\n",
    "            'total_sulfur_dioxide', 'density', 'pH', 'sulphites', 'alcohol', 'is_white', 'quality']\n",
    "\n",
    "PATH=\"../data/{}\".format(DATASET_NAME)\n",
    "TRAINING_SET=\"train.csv\"\n",
    "VALIDATION_SET=\"valid.csv\"\n",
    "TEST_SET=\"test.csv\"\n",
    "N_TRAIN_INSTANCES=None # replace this with None to load the whole training set\n",
    "N_TEST_INSTANCES=None # replace this with None to load the whole test set\n",
    "\n",
    "######################\n",
    "# Attacker Definition\n",
    "######################\n",
    "attacker_rules = [\n",
    "    # alcohol increment rule\n",
    "    {'f'    :'alcohol',\n",
    "     'valid': lambda x: x['alcohol'] <= 10.0,\n",
    "     'value': .75,\n",
    "     'cost' : 10,\n",
    "     'is_cat': False },\n",
    "    {'f'    : 'residual_sugar',\n",
    "     'valid': lambda x: x['residual_sugar'] >= 8.0,\n",
    "     'value': -1.2,\n",
    "     'cost' : 10,\n",
    "     'is_cat': False },\n",
    "    {'f'    : 'volatile_acidity',\n",
    "     'valid': lambda x: x['volatile_acidity'] >= 0.6,\n",
    "     'value': -0.3,\n",
    "     'cost' : 10,\n",
    "     'is_cat': False }    \n",
    "]\n",
    "\n",
    "max_budget_per_feature = {\n",
    "    'alcohol'     : 100,\n",
    "    'residual_sugar': 100,\n",
    "    'volatile_acidity'    : 100\n",
    "}\n",
    "\n",
    "B = [20, 30, 40]\n",
    "B = [40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "train = load_dataset(PATH, TRAINING_SET, names=colnames, header=0, nrows=N_TRAIN_INSTANCES)\n",
    "valid = load_dataset(PATH, VALIDATION_SET, names=colnames, header=0)\n",
    "test = load_dataset(PATH, TEST_SET, names=colnames, header=0, nrows=N_TEST_INSTANCES)\n",
    "\n",
    "# binarize\n",
    "binarize_data(train, \"quality\", 6)\n",
    "binarize_data(valid, \"quality\", 6)\n",
    "binarize_data(test, \"quality\", 6)\n",
    "\n",
    "print(\"Shape of training set: {}\".format(train.shape))\n",
    "print(\"Shape of validation set: {}\".format(valid.shape))\n",
    "print(\"Shape of test set: {}\".format(test.shape))\n",
    "\n",
    "# train.head()\n",
    "# valid.head()\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize_dataset(train, PATH, TRAINING_SET, \"_ori.csv.bz2\")\n",
    "serialize_dataset(valid, PATH, VALIDATION_SET, \"_ori.csv.bz2\")\n",
    "serialize_dataset(test, PATH, TEST_SET, \"_ori.csv.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for budget in B:\n",
    "    print (\"Processing Budged: \", budget)\n",
    "    train_att = perturb_dataset(train, budget, max_budget_per_feature, attacker_rules, skip_class=1)\n",
    "    valid_att = perturb_dataset(valid, budget, max_budget_per_feature, attacker_rules, skip_class=1)\n",
    "    test_att  = perturb_dataset(test, budget, max_budget_per_feature, attacker_rules, skip_class=1)\n",
    "    \n",
    "    serialize_dataset(train_att, PATH, TRAINING_SET, \"_B{}\".format(budget)+\".csv.bz2\")\n",
    "    serialize_dataset(valid_att, PATH, VALIDATION_SET, \"_B{}\".format(budget)+\".csv.bz2\")\n",
    "    serialize_dataset(test_att, PATH, TEST_SET, \"_B{}\".format(budget)+\".csv.bz2\")\n",
    "    \n",
    "    print(\"Shape of attacked training set: {}\".format(train_att.shape))\n",
    "    print(\"Shape of attacked validation set: {}\".format(valid_att.shape))\n",
    "    print(\"Shape of attacked test set: {}\".format(test_att.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CENSUS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"census2\"\n",
    "\n",
    "colnames = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n",
    "           'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', \n",
    "            'hours_per_week', 'native_country', 'income_greater_than_50k']\n",
    "\n",
    "PATH=\"../data/{}\".format(DATASET_NAME)\n",
    "TRAINING_SET=\"train.csv.bz2\"\n",
    "VALIDATION_SET=\"valid.csv.bz2\"\n",
    "TEST_SET=\"test.csv.bz2\"\n",
    "N_TRAIN_INSTANCES=None # replace this with None to load the whole training set\n",
    "N_TEST_INSTANCES=None # replace this with None to load the whole test set\n",
    "\n",
    "######################\n",
    "# Attacker Definition\n",
    "######################\n",
    "attacker_rules = [\n",
    "    {'f'    :'workclass',\n",
    "     'valid': lambda x: x['workclass'] == 'Never-worked',\n",
    "     'value': 'Without-pay',\n",
    "     'cost' : 1,\n",
    "     'is_cat': True },\n",
    "    {'f'    : 'marital_status',\n",
    "     'valid': lambda x:x['marital_status']=='Divorced' or x['marital_status']=='Separated',\n",
    "     'value': 'Never-married',\n",
    "     'cost' : 1,\n",
    "     'is_cat': True },\n",
    "    {'f'    : 'occupation',\n",
    "     'valid': lambda x: x['occupation'] != 'Other-service',\n",
    "     'value': 'Other-service',\n",
    "     'cost' : 1,\n",
    "     'is_cat': True },\n",
    "    {'f'    : 'education_num',\n",
    "     'valid': lambda x: x['education_num']>1,\n",
    "     'value': -1,\n",
    "     'cost' : 1,\n",
    "     'is_cat': False },\n",
    "    {'f'    : 'capital_gain',\n",
    "     'valid': lambda x: True,\n",
    "     'value': 500,\n",
    "     'cost' : 50,\n",
    "     'is_cat': False },\n",
    "    {'f'    : 'hours_per_week',\n",
    "     'valid': lambda x: True,\n",
    "     'value': 4,\n",
    "     'cost' : 4,\n",
    "     'is_cat': False }\n",
    "]\n",
    "\n",
    "max_budget_per_feature = {\n",
    "    'workclass'     : 1,\n",
    "    'marital_status': 1,\n",
    "    'occupation'    : 1,\n",
    "    'education_num' : 2,\n",
    "    'hours_per_week': 8,\n",
    "    'capital_gain'  : 250\n",
    "}\n",
    "\n",
    "B = [5, 15, 150, 300]\n",
    "B = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (27144, 14)\n",
      "Shape of validation set: (3017, 14)\n",
      "Shape of test set: (15059, 14)\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "train = load_dataset(PATH, TRAINING_SET, names=colnames, header=0, nrows=N_TRAIN_INSTANCES)\n",
    "test = load_dataset(PATH, TEST_SET, names=colnames, header=0, nrows=N_TEST_INSTANCES)\n",
    "\n",
    "# remove education string\n",
    "train = train.drop(['education'], axis=1)\n",
    "test  = test.drop(['education'], axis=1)\n",
    "# drop NA\n",
    "train = train[~train.isnull().any(axis=1)]\n",
    "test  = test[~test.isnull().any(axis=1)]\n",
    "\n",
    "# create the missing validation set\n",
    "train, valid = train_test_split( train, \n",
    "                                 test_size=0.1, \n",
    "                                 random_state=42, \n",
    "                                 stratify=train[\"income_greater_than_50k\"])\n",
    "\n",
    "print(\"Shape of training set: {}\".format(train.shape))\n",
    "print(\"Shape of validation set: {}\".format(valid.shape))\n",
    "print(\"Shape of test set: {}\".format(test.shape))\n",
    "\n",
    "# train.head()\n",
    "# valid.head()\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize_dataset(train, PATH, TRAINING_SET, \"_ori.csv.bz2\")\n",
    "serialize_dataset(valid, PATH, VALIDATION_SET, \"_ori.csv.bz2\")\n",
    "serialize_dataset(test, PATH, TEST_SET, \"_ori.csv.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Budged:  5\n",
      "***** Loop through all the original instances... *****\n",
      "***** Perturbing instance [ID = #500]... *****\n",
      "***** Perturbing instance [ID = #1000]... *****\n",
      "***** Perturbing instance [ID = #1500]... *****\n",
      "***** Perturbing instance [ID = #2000]... *****\n",
      "***** Perturbing instance [ID = #2500]... *****\n",
      "***** Perturbing instance [ID = #3000]... *****\n",
      "***** Perturbing instance [ID = #3500]... *****\n",
      "***** Perturbing instance [ID = #4000]... *****\n",
      "***** Perturbing instance [ID = #4500]... *****\n",
      "***** Perturbing instance [ID = #5000]... *****\n",
      "***** Perturbing instance [ID = #5500]... *****\n",
      "***** Perturbing instance [ID = #6000]... *****\n",
      "***** Perturbing instance [ID = #6500]... *****\n",
      "***** Perturbing instance [ID = #7000]... *****\n",
      "***** Perturbing instance [ID = #7500]... *****\n",
      "***** Perturbing instance [ID = #8000]... *****\n",
      "***** Perturbing instance [ID = #8500]... *****\n",
      "***** Perturbing instance [ID = #9000]... *****\n",
      "***** Perturbing instance [ID = #9500]... *****\n",
      "***** Perturbing instance [ID = #10000]... *****\n",
      "***** Perturbing instance [ID = #10500]... *****\n",
      "***** Perturbing instance [ID = #11000]... *****\n",
      "***** Perturbing instance [ID = #11500]... *****\n",
      "***** Perturbing instance [ID = #12000]... *****\n",
      "***** Perturbing instance [ID = #12500]... *****\n",
      "***** Perturbing instance [ID = #13000]... *****\n",
      "***** Perturbing instance [ID = #13500]... *****\n",
      "***** Perturbing instance [ID = #14000]... *****\n",
      "***** Perturbing instance [ID = #14500]... *****\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-c68c2e5235f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbudget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing Budged: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperturb_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_budget_per_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattacker_rules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvalid_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperturb_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_budget_per_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattacker_rules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_att\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mperturb_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_budget_per_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattacker_rules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-23a8d4671b41>\u001b[0m in \u001b[0;36mperturb_dataset\u001b[0;34m(data, budget, max_budget_per_feature, rules, skip_class)\u001b[0m\n\u001b[1;32m     61\u001b[0m                              allow_duplicates=True)\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mperturbed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperturbed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperturbations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0minstance_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas-0.23.0rc2-py3.6-linux-x86_64.egg/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   6208\u001b[0m         return concat(to_concat, ignore_index=ignore_index,\n\u001b[1;32m   6209\u001b[0m                       \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6210\u001b[0;31m                       sort=sort)\n\u001b[0m\u001b[1;32m   6211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6212\u001b[0m     def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas-0.23.0rc2-py3.6-linux-x86_64.egg/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    223\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                        copy=copy, sort=sort)\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas-0.23.0rc2-py3.6-linux-x86_64.egg/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0mndims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas-0.23.0rc2-py3.6-linux-x86_64.egg/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(self, inplace)\u001b[0m\n\u001b[1;32m   4463\u001b[0m         \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inplace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4465\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4466\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4467\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas-0.23.0rc2-py3.6-linux-x86_64.egg/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4445\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4447\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4449\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas-0.23.0rc2-py3.6-linux-x86_64.egg/pandas/core/generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   4434\u001b[0m         \"\"\"\n\u001b[1;32m   4435\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4436\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4438\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas-0.23.0rc2-py3.6-linux-x86_64.egg/pandas/core/generic.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m   4443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4444\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4445\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas-0.23.0rc2-py3.6-linux-x86_64.egg/pandas/core/internals.py\u001b[0m in \u001b[0;36mconsolidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4096\u001b[0m         \u001b[0mbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4097\u001b[0m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4098\u001b[0;31m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4099\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas-0.23.0rc2-py3.6-linux-x86_64.egg/pandas/core/internals.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4104\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas-0.23.0rc2-py3.6-linux-x86_64.egg/pandas/core/internals.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   5067\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5068\u001b[0m         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n\u001b[0;32m-> 5069\u001b[0;31m                                       _can_consolidate=_can_consolidate)\n\u001b[0m\u001b[1;32m   5070\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5071\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas-0.23.0rc2-py3.6-linux-x86_64.egg/pandas/core/internals.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[1;32m   5087\u001b[0m         \u001b[0;31m# combination of those slices is a slice, too.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5088\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_array\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5089\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_vstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5091\u001b[0m         \u001b[0margsort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas-0.23.0rc2-py3.6-linux-x86_64.egg/pandas/core/internals.py\u001b[0m in \u001b[0;36m_vstack\u001b[0;34m(to_stack, dtype)\u001b[0m\n\u001b[1;32m   5133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5134\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \"\"\"\n\u001b[1;32m    282\u001b[0m     \u001b[0m_warn_for_nonsequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for budget in B:\n",
    "    print (\"Processing Budged: \", budget)\n",
    "    train_att = perturb_dataset(train, budget, max_budget_per_feature, attacker_rules, skip_class=1)\n",
    "    valid_att = perturb_dataset(valid, budget, max_budget_per_feature, attacker_rules, skip_class=1)\n",
    "    test_att  = perturb_dataset(test, budget, max_budget_per_feature, attacker_rules, skip_class=1)\n",
    "    \n",
    "    serialize_dataset(train_att, PATH, TRAINING_SET, \"_B{}\".format(budget)+\".csv.bz2\")\n",
    "    serialize_dataset(valid_att, PATH, VALIDATION_SET, \"_B{}\".format(budget)+\".csv.bz2\")\n",
    "    serialize_dataset(test_att, PATH, TEST_SET, \"_B{}\".format(budget)+\".csv.bz2\")\n",
    "    \n",
    "    print(\"Shape of attacked training set: {}\".format(train_att.shape))\n",
    "    print(\"Shape of attacked validation set: {}\".format(valid_att.shape))\n",
    "    print(\"Shape of attacked test set: {}\".format(test_att.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO WE WANT TO SKIP THE CLASS IN CENSUS?\n",
    "# Robust Forest Ha il budget per feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
