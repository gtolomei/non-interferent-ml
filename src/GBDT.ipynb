{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBDT - LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "import pickle\n",
    "import json\n",
    "import functools\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nilib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gradient_boosting_baseline(train_file, valid_file, test_file, output_model_file, \n",
    "                                     drop_cols=None, random_forest = False):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'best_round', 'metric', 'filename'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    \n",
    "    assert \"instance_id\" not in train.columns.values, \"Wrong training set file for GBDT\"\n",
    "\n",
    "    # dropping some of the columns\n",
    "    if drop_cols is not None:\n",
    "        print (\"Dropping columns:\", drop_cols)\n",
    "        train.drop(columns=drop_cols, inplace=True)\n",
    "        valid.drop(columns=drop_cols, inplace=True)\n",
    "        test.drop(columns=drop_cols, inplace=True)\n",
    "    \n",
    "\n",
    "    for num_trees in [100]:\n",
    "        for learning_rate in [0.05]: #[0.01, 0.05]:\n",
    "            for num_leaves in [2**8]: #[16, 24]:\n",
    "                # datasets\n",
    "                lgbm_train = lightgbm.Dataset(data=train.iloc[:,:-1], \n",
    "                                              label=train.iloc[:,-1])\n",
    "\n",
    "                lgbm_valid = lightgbm.Dataset(data=valid.iloc[:,:-1], \n",
    "                                              label=valid.iloc[:,-1])\n",
    "\n",
    "                # run train\n",
    "                lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                'num_leaves': num_leaves,\n",
    "                                'max_depth': 8,\n",
    "                                'objective': 'regression'\n",
    "                              } \n",
    "                if random_forest:\n",
    "                    lgbm_params['boosting'] = 'rf'\n",
    "                    lgbm_params['bagging_fraction'] = 0.8\n",
    "                    lgbm_params['feature_fraction' ] = 0.8\n",
    "                    lgbm_params['bagging_freq'] = 1\n",
    "\n",
    "                lgbm_info = {}\n",
    "                lgbm_model = lightgbm.train(lgbm_params, lgbm_train, \n",
    "                                            num_boost_round = num_trees,\n",
    "#                                             fobj            = optimize_log_loss, \n",
    "#                                             feval           = avg_log_loss,\n",
    "                                            evals_result    = lgbm_info,\n",
    "                                            valid_sets      = [lgbm_train, lgbm_valid], \n",
    "                                            valid_names     = ['train', 'valid'],\n",
    "                                            verbose_eval    = 1)\n",
    "                \n",
    "                best_valid_iter = np.argmin(lgbm_info['valid']['l2'])\n",
    "                \n",
    "                model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                            num_trees,\n",
    "                                                                            int(learning_rate*1000),\n",
    "                                                                            num_leaves,\n",
    "                                                                            best_valid_iter + 1\n",
    "                                                                           )\n",
    "                \n",
    "                # update experimental results\n",
    "                exp = exp.append({'num_trees': num_trees, \n",
    "                                  'learning_rate':learning_rate,\n",
    "                                  'num_leaves':num_leaves, \n",
    "                                  'best_round':best_valid_iter+1, \n",
    "                                  'metric':lgbm_info['valid']['l2'][best_valid_iter],\n",
    "                                  'filename':model_file_name},\n",
    "                                 ignore_index=True)\n",
    "                \n",
    "        \n",
    "                lgbm_model.save_model(model_file_name)\n",
    "                print(\"Model saved to\", model_file_name)\n",
    "    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WINE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"wine\"\n",
    "DATASET_DIR=\"../data/{}\".format(DATASET_NAME)\n",
    "ATK_DIR=DATASET_DIR + \"/attacks\"\n",
    "MODELS_DIR=\"../out/models/{}\".format(DATASET_NAME)\n",
    "TRAINING_FILENAME=DATASET_DIR + \"/\" + \"train.csv.bz2\"\n",
    "VALIDATION_FILENAME=DATASET_DIR + \"/\" + \"valid.csv.bz2\"\n",
    "TEST_FILENAME=DATASET_DIR + \"/\" + \"test.csv.bz2\"\n",
    "MODEL_FILENAME=MODELS_DIR + \"/std-gbdt_{}\".format(DATASET_NAME)\n",
    "RED_MODEL_FILENAME=MODELS_DIR + \"/red-gbdt_{}\".format(DATASET_NAME)\n",
    "RF_MODEL_FILENAME=MODELS_DIR + \"/rf-gbdt_{}\".format(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# FULL GDBT\n",
    "\n",
    "experiments = train_gradient_boosting_baseline(TRAINING_FILENAME, \n",
    "                                               VALIDATION_FILENAME,\n",
    "                                               TEST_FILENAME,\n",
    "                                               MODEL_FILENAME\n",
    "                                              )  \n",
    "\n",
    "experiments.to_csv(MODEL_FILENAME + \".csv\", index=False)\n",
    "\n",
    "print(experiments)\n",
    "print ('best model is:', experiments.sort_values('metric').iloc[0]['filename'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDUCED GDBT\n",
    "\n",
    "experiments = train_gradient_boosting_baseline(TRAINING_FILENAME, \n",
    "                                               VALIDATION_FILENAME,\n",
    "                                               TEST_FILENAME,\n",
    "                                               RED_MODEL_FILENAME,\n",
    "                                               drop_cols=[\"alcohol\", \"residual_sugar\", \"volatile_acidity\"]\n",
    "                                              )  \n",
    "\n",
    "experiments.to_csv(RED_MODEL_FILENAME + \".csv\", index=False)\n",
    "\n",
    "print(experiments)\n",
    "print ('best model is:', experiments.sort_values('metric').iloc[0]['filename'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST\n",
    "\n",
    "experiments = train_gradient_boosting_baseline(TRAINING_FILENAME, \n",
    "                                               VALIDATION_FILENAME,\n",
    "                                               TEST_FILENAME,\n",
    "                                               RF_MODEL_FILENAME,\n",
    "                                               random_forest=True\n",
    "                                              )  \n",
    "\n",
    "experiments.to_csv(RF_MODEL_FILENAME + \".csv\", index=False)\n",
    "\n",
    "print(experiments)\n",
    "print ('best model is:', experiments.sort_values('metric').iloc[0]['filename'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CENSUS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"census\"\n",
    "DATASET_DIR=\"../data/{}\".format(DATASET_NAME)\n",
    "ATK_DIR=DATASET_DIR + \"/attacks\"\n",
    "MODELS_DIR=\"../out/models/{}\".format(DATASET_NAME)\n",
    "TRAINING_FILENAME=DATASET_DIR + \"/\" + \"train.csv.bz2\"\n",
    "VALIDATION_FILENAME=DATASET_DIR + \"/\" + \"valid.csv.bz2\"\n",
    "TEST_FILENAME=DATASET_DIR + \"/\" + \"test.csv.bz2\"\n",
    "MODEL_FILENAME=MODELS_DIR + \"/std-gbdt_{}\".format(DATASET_NAME)\n",
    "RED_MODEL_FILENAME=MODELS_DIR + \"/red-gbdt_{}\".format(DATASET_NAME)\n",
    "RF_MODEL_FILENAME=MODELS_DIR + \"/rf-gbdt_{}\".format(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# FULL GDBT\n",
    "\n",
    "experiments = train_gradient_boosting_baseline(TRAINING_FILENAME, \n",
    "                                               VALIDATION_FILENAME,\n",
    "                                               TEST_FILENAME,\n",
    "                                               MODEL_FILENAME\n",
    "                                              )  \n",
    "\n",
    "experiments.to_csv(MODEL_FILENAME + \".csv\", index=False)\n",
    "\n",
    "print(experiments)\n",
    "print ('best model is:', experiments.sort_values('metric').iloc[0]['filename'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDUCED GDBT\n",
    "\n",
    "experiments = train_gradient_boosting_baseline(TRAINING_FILENAME, \n",
    "                                               VALIDATION_FILENAME,\n",
    "                                               TEST_FILENAME,\n",
    "                                               RED_MODEL_FILENAME,\n",
    "                                               drop_cols=['workclass','marital_status',\n",
    "                                                          'occupation', 'education_num',\n",
    "                                                          'hours_per_week','capital_gain' ]\n",
    "                                              )\n",
    "\n",
    "experiments.to_csv(RED_MODEL_FILENAME + \".csv\", index=False)\n",
    "\n",
    "print(experiments)\n",
    "print ('best model is:', experiments.sort_values('metric').iloc[0]['filename'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST\n",
    "\n",
    "experiments = train_gradient_boosting_baseline(TRAINING_FILENAME, \n",
    "                                               VALIDATION_FILENAME,\n",
    "                                               TEST_FILENAME,\n",
    "                                               RF_MODEL_FILENAME,\n",
    "                                               random_forest=True\n",
    "                                              )  \n",
    "\n",
    "experiments.to_csv(RF_MODEL_FILENAME + \".csv\", index=False)\n",
    "\n",
    "print(experiments)\n",
    "print ('best model is:', experiments.sort_values('metric').iloc[0]['filename'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPAM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"spam\"\n",
    "DATASET_DIR=\"../data/{}\".format(DATASET_NAME)\n",
    "ATK_DIR=DATASET_DIR + \"/attacks\"\n",
    "MODELS_DIR=\"../out/models/{}\".format(DATASET_NAME)\n",
    "TRAINING_FILENAME=DATASET_DIR + \"/\" + \"train.csv.bz2\"\n",
    "VALIDATION_FILENAME=DATASET_DIR + \"/\" + \"valid.csv.bz2\"\n",
    "TEST_FILENAME=DATASET_DIR + \"/\" + \"test.csv.bz2\"\n",
    "MODEL_FILENAME=MODELS_DIR + \"/std-gbdt_{}\".format(DATASET_NAME)\n",
    "RED_MODEL_FILENAME=MODELS_DIR + \"/red-gbdt_{}\".format(DATASET_NAME)\n",
    "RF_MODEL_FILENAME=MODELS_DIR + \"/rf-gbdt_{}\".format(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# FULL GDBT\n",
    "\n",
    "experiments = train_gradient_boosting_baseline(TRAINING_FILENAME, \n",
    "                                               VALIDATION_FILENAME,\n",
    "                                               TEST_FILENAME,\n",
    "                                               MODEL_FILENAME\n",
    "                                              )  \n",
    "\n",
    "experiments.to_csv(MODEL_FILENAME + \".csv\", index=False)\n",
    "\n",
    "print(experiments)\n",
    "print ('best model is:', experiments.sort_values('metric').iloc[0]['filename'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDUCED GDBT\n",
    "\n",
    "experiments = train_gradient_boosting_baseline(TRAINING_FILENAME, \n",
    "                                               VALIDATION_FILENAME,\n",
    "                                               TEST_FILENAME,\n",
    "                                               RED_MODEL_FILENAME,\n",
    "                                               drop_cols=['char_freq_!', 'word_freq_remove',\n",
    "                                                          'char_freq_$', 'capital_run_length_average',\n",
    "                                                          'capital_run_length_total' ]\n",
    "                                              )\n",
    "\n",
    "experiments.to_csv(RED_MODEL_FILENAME + \".csv\", index=False)\n",
    "\n",
    "print(experiments)\n",
    "print ('best model is:', experiments.sort_values('metric').iloc[0]['filename'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST\n",
    "\n",
    "experiments = train_gradient_boosting_baseline(TRAINING_FILENAME, \n",
    "                                               VALIDATION_FILENAME,\n",
    "                                               TEST_FILENAME,\n",
    "                                               RF_MODEL_FILENAME,\n",
    "                                               random_forest=True\n",
    "                                              )  \n",
    "\n",
    "experiments.to_csv(RF_MODEL_FILENAME + \".csv\", index=False)\n",
    "\n",
    "print(experiments)\n",
    "print ('best model is:', experiments.sort_values('metric').iloc[0]['filename'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###------------_####\n",
    "def print_fx_imp(model, colnames):\n",
    "    fx_uses = model.feature_importance(importance_type='split')\n",
    "    fx_gain = model.feature_importance(importance_type='gain')\n",
    "\n",
    "    for i,f in enumerate(np.argsort(fx_gain)[::-1]):\n",
    "        print (\"{:2d} {:20s} {:.3f} {:4d}\".format(i, colnames[f], fx_gain[f], fx_uses[f]))\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/wine/raw/train.csv.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" -- GDBT --\")    \n",
    "gbdt = lightgbm.Booster(model_file=\"../out/models/wine/std-gbdt_wine_T100_S0050_L256_R97.model\")\n",
    "print(gbdt.num_trees())\n",
    "print_fx_imp(gbdt, df.columns)\n",
    "\n",
    "print(\" -- Reduced GDBT --\")    \n",
    "redf = lightgbm.Booster(model_file=\"../out/models/wine/red-gbdt_wine_T100_S0050_L256_R100.model\")\n",
    "print(redf.num_trees())\n",
    "#print_fx_imp(redf,train.drop(columns=['char_freq_!', 'word_freq_remove',\n",
    "#                                                          'char_freq_$', 'capital_run_length_average',\n",
    "#                                                         'capital_run_length_total', 'word_freq_hp' ]).columns\n",
    "#            )\n",
    "\n",
    "\n",
    "# print(\" -- Adv. Boosting --\")    \n",
    "# advb = lightgbm.Booster(model_file=\"../out/models/census/adv-boosting_census_B30_T100_S0050_L24_R100.model\")\n",
    "# print(advb.num_trees())\n",
    "# print_fx_imp(advb, TRAIN.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREDIT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"credit\"\n",
    "DATASET_DIR=\"../data/{}\".format(DATASET_NAME)\n",
    "ATK_DIR=DATASET_DIR + \"/attacks\"\n",
    "MODELS_DIR=\"../out/models/{}\".format(DATASET_NAME)\n",
    "TRAINING_FILENAME=DATASET_DIR + \"/\" + \"train.csv.bz2\"\n",
    "VALIDATION_FILENAME=DATASET_DIR + \"/\" + \"valid.csv.bz2\"\n",
    "TEST_FILENAME=DATASET_DIR + \"/\" + \"test.csv.bz2\"\n",
    "MODEL_FILENAME=MODELS_DIR + \"/std-gbdt_{}\".format(DATASET_NAME)\n",
    "RED_MODEL_FILENAME=MODELS_DIR + \"/red-gbdt_{}\".format(DATASET_NAME)\n",
    "RF_MODEL_FILENAME=MODELS_DIR + \"/rf-gbdt_{}\".format(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL GDBT\n",
    "\n",
    "experiments = train_gradient_boosting_baseline(TRAINING_FILENAME, \n",
    "                                               VALIDATION_FILENAME,\n",
    "                                               TEST_FILENAME,\n",
    "                                               MODEL_FILENAME\n",
    "                                              )  \n",
    "\n",
    "experiments.to_csv(MODEL_FILENAME + \".csv\", index=False)\n",
    "\n",
    "print(experiments)\n",
    "print ('best model is:', experiments.sort_values('metric').iloc[0]['filename'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDUCED GDBT\n",
    "\n",
    "experiments = train_gradient_boosting_baseline(TRAINING_FILENAME, \n",
    "                                               VALIDATION_FILENAME,\n",
    "                                               TEST_FILENAME,\n",
    "                                               RED_MODEL_FILENAME,\n",
    "                                               drop_cols=['PAY_0', 'BILL_AMT1', 'PAY_2', 'LIMIT_BAL'] # SET TO THE LIST OF ATTACKED FEATURES\n",
    "                                              )\n",
    "\n",
    "experiments.to_csv(RED_MODEL_FILENAME + \".csv\", index=False)\n",
    "\n",
    "print(experiments)\n",
    "print ('best model is:', experiments.sort_values('metric').iloc[0]['filename'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST\n",
    "\n",
    "experiments = train_gradient_boosting_baseline(TRAINING_FILENAME, \n",
    "                                               VALIDATION_FILENAME,\n",
    "                                               TEST_FILENAME,\n",
    "                                               RF_MODEL_FILENAME,\n",
    "                                               random_forest=True\n",
    "                                              )  \n",
    "\n",
    "experiments.to_csv(RF_MODEL_FILENAME + \".csv\", index=False)\n",
    "\n",
    "print(experiments)\n",
    "print ('best model is:', experiments.sort_values('metric').iloc[0]['filename'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEBSITES Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"websites\"\n",
    "DATASET_DIR=\"../data/{}\".format(DATASET_NAME)\n",
    "ATK_DIR=DATASET_DIR + \"/attacks\"\n",
    "MODELS_DIR=\"../out/models/{}\".format(DATASET_NAME)\n",
    "TRAINING_FILENAME=DATASET_DIR + \"/\" + \"train.csv.bz2\"\n",
    "VALIDATION_FILENAME=DATASET_DIR + \"/\" + \"valid.csv.bz2\"\n",
    "TEST_FILENAME=DATASET_DIR + \"/\" + \"test.csv.bz2\"\n",
    "MODEL_FILENAME=MODELS_DIR + \"/std-gbdt_{}\".format(DATASET_NAME)\n",
    "RED_MODEL_FILENAME=MODELS_DIR + \"/red-gbdt_{}\".format(DATASET_NAME)\n",
    "RF_MODEL_FILENAME=MODELS_DIR + \"/rf-gbdt_{}\".format(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing original files...\n",
      "Loading: ../data/websites/train.csv.bz2\n",
      "Loading: ../data/websites/valid.csv.bz2\n",
      "Loading: ../data/websites/test.csv.bz2\n",
      "Train/Valid/Test sizes: (1068, 14) (356, 14) (356, 14)\n",
      "Train/Valid/Test split: 0.60 0.20 0.20\n",
      "CatFX: []\n",
      "Train/Valid/Test sizes: (1068, 14) (356, 14) (356, 14)\n",
      "Train/Valid/Test split: 0.60 0.20 0.20\n",
      "Saving processed files *.atks.bz2\n",
      "[1]\ttrain's l2: 0.390269\tvalid's l2: 0.111974\n",
      "[2]\ttrain's l2: 0.36682\tvalid's l2: 0.109043\n",
      "[3]\ttrain's l2: 0.345656\tvalid's l2: 0.106276\n",
      "[4]\ttrain's l2: 0.326522\tvalid's l2: 0.104704\n",
      "[5]\ttrain's l2: 0.308749\tvalid's l2: 0.103229\n",
      "[6]\ttrain's l2: 0.292668\tvalid's l2: 0.10423\n",
      "[7]\ttrain's l2: 0.278103\tvalid's l2: 0.103541\n",
      "[8]\ttrain's l2: 0.264891\tvalid's l2: 0.103354\n",
      "[9]\ttrain's l2: 0.25334\tvalid's l2: 0.104415\n",
      "[10]\ttrain's l2: 0.242588\tvalid's l2: 0.104841\n",
      "[11]\ttrain's l2: 0.232001\tvalid's l2: 0.106556\n",
      "[12]\ttrain's l2: 0.223196\tvalid's l2: 0.108627\n",
      "[13]\ttrain's l2: 0.214977\tvalid's l2: 0.10957\n",
      "[14]\ttrain's l2: 0.207619\tvalid's l2: 0.110591\n",
      "[15]\ttrain's l2: 0.200347\tvalid's l2: 0.112699\n",
      "[16]\ttrain's l2: 0.193839\tvalid's l2: 0.114096\n",
      "[17]\ttrain's l2: 0.18761\tvalid's l2: 0.115924\n",
      "[18]\ttrain's l2: 0.181666\tvalid's l2: 0.117927\n",
      "[19]\ttrain's l2: 0.175751\tvalid's l2: 0.119979\n",
      "[20]\ttrain's l2: 0.169659\tvalid's l2: 0.119277\n",
      "[21]\ttrain's l2: 0.164159\tvalid's l2: 0.11867\n",
      "[22]\ttrain's l2: 0.159072\tvalid's l2: 0.117389\n",
      "[23]\ttrain's l2: 0.154462\tvalid's l2: 0.116787\n",
      "[24]\ttrain's l2: 0.150315\tvalid's l2: 0.118376\n",
      "[25]\ttrain's l2: 0.146371\tvalid's l2: 0.117124\n",
      "[26]\ttrain's l2: 0.14277\tvalid's l2: 0.118995\n",
      "[27]\ttrain's l2: 0.138884\tvalid's l2: 0.118107\n",
      "[28]\ttrain's l2: 0.135899\tvalid's l2: 0.119481\n",
      "[29]\ttrain's l2: 0.132317\tvalid's l2: 0.120062\n",
      "[30]\ttrain's l2: 0.128631\tvalid's l2: 0.120082\n",
      "[31]\ttrain's l2: 0.125413\tvalid's l2: 0.121005\n",
      "[32]\ttrain's l2: 0.123177\tvalid's l2: 0.11981\n",
      "[33]\ttrain's l2: 0.119791\tvalid's l2: 0.119292\n",
      "[34]\ttrain's l2: 0.117443\tvalid's l2: 0.120175\n",
      "[35]\ttrain's l2: 0.114711\tvalid's l2: 0.119579\n",
      "[36]\ttrain's l2: 0.112534\tvalid's l2: 0.120128\n",
      "[37]\ttrain's l2: 0.110113\tvalid's l2: 0.119678\n",
      "[38]\ttrain's l2: 0.107872\tvalid's l2: 0.119668\n",
      "[39]\ttrain's l2: 0.10581\tvalid's l2: 0.120203\n",
      "[40]\ttrain's l2: 0.103857\tvalid's l2: 0.120464\n",
      "[41]\ttrain's l2: 0.101788\tvalid's l2: 0.120569\n",
      "[42]\ttrain's l2: 0.0999189\tvalid's l2: 0.122334\n",
      "[43]\ttrain's l2: 0.0982382\tvalid's l2: 0.124109\n",
      "[44]\ttrain's l2: 0.0963172\tvalid's l2: 0.124412\n",
      "[45]\ttrain's l2: 0.09433\tvalid's l2: 0.124807\n",
      "[46]\ttrain's l2: 0.0929067\tvalid's l2: 0.125702\n",
      "[47]\ttrain's l2: 0.0912357\tvalid's l2: 0.126149\n",
      "[48]\ttrain's l2: 0.0899478\tvalid's l2: 0.12704\n",
      "[49]\ttrain's l2: 0.0881531\tvalid's l2: 0.126522\n",
      "[50]\ttrain's l2: 0.0867307\tvalid's l2: 0.127315\n",
      "[51]\ttrain's l2: 0.0853993\tvalid's l2: 0.126704\n",
      "[52]\ttrain's l2: 0.0842167\tvalid's l2: 0.127257\n",
      "[53]\ttrain's l2: 0.0829657\tvalid's l2: 0.127139\n",
      "[54]\ttrain's l2: 0.0818679\tvalid's l2: 0.127552\n",
      "[55]\ttrain's l2: 0.080807\tvalid's l2: 0.127388\n",
      "[56]\ttrain's l2: 0.0797094\tvalid's l2: 0.127913\n",
      "[57]\ttrain's l2: 0.0786565\tvalid's l2: 0.127613\n",
      "[58]\ttrain's l2: 0.0773067\tvalid's l2: 0.126983\n",
      "[59]\ttrain's l2: 0.076203\tvalid's l2: 0.126445\n",
      "[60]\ttrain's l2: 0.0753108\tvalid's l2: 0.126461\n",
      "[61]\ttrain's l2: 0.0743479\tvalid's l2: 0.126413\n",
      "[62]\ttrain's l2: 0.0733693\tvalid's l2: 0.126025\n",
      "[63]\ttrain's l2: 0.0724682\tvalid's l2: 0.126452\n",
      "[64]\ttrain's l2: 0.0715133\tvalid's l2: 0.126261\n",
      "[65]\ttrain's l2: 0.0708382\tvalid's l2: 0.126864\n",
      "[66]\ttrain's l2: 0.0701638\tvalid's l2: 0.12769\n",
      "[67]\ttrain's l2: 0.069238\tvalid's l2: 0.12772\n",
      "[68]\ttrain's l2: 0.0685645\tvalid's l2: 0.127543\n",
      "[69]\ttrain's l2: 0.068108\tvalid's l2: 0.128119\n",
      "[70]\ttrain's l2: 0.0674525\tvalid's l2: 0.127927\n",
      "[71]\ttrain's l2: 0.0666675\tvalid's l2: 0.127993\n",
      "[72]\ttrain's l2: 0.0658958\tvalid's l2: 0.127618\n",
      "[73]\ttrain's l2: 0.0652891\tvalid's l2: 0.128461\n",
      "[74]\ttrain's l2: 0.0647416\tvalid's l2: 0.128282\n",
      "[75]\ttrain's l2: 0.0642336\tvalid's l2: 0.128434\n",
      "[76]\ttrain's l2: 0.0637804\tvalid's l2: 0.128252\n",
      "[77]\ttrain's l2: 0.0634038\tvalid's l2: 0.127907\n",
      "[78]\ttrain's l2: 0.0628469\tvalid's l2: 0.128255\n",
      "[79]\ttrain's l2: 0.0623992\tvalid's l2: 0.127854\n",
      "[80]\ttrain's l2: 0.0619007\tvalid's l2: 0.12823\n",
      "[81]\ttrain's l2: 0.0613371\tvalid's l2: 0.128409\n",
      "[82]\ttrain's l2: 0.0608491\tvalid's l2: 0.128319\n",
      "[83]\ttrain's l2: 0.0604241\tvalid's l2: 0.12861\n",
      "[84]\ttrain's l2: 0.0599862\tvalid's l2: 0.128528\n",
      "[85]\ttrain's l2: 0.0595468\tvalid's l2: 0.128973\n",
      "[86]\ttrain's l2: 0.0593047\tvalid's l2: 0.129016\n",
      "[87]\ttrain's l2: 0.0587889\tvalid's l2: 0.128982\n",
      "[88]\ttrain's l2: 0.0581469\tvalid's l2: 0.128431\n",
      "[89]\ttrain's l2: 0.0576867\tvalid's l2: 0.128304\n",
      "[90]\ttrain's l2: 0.0572583\tvalid's l2: 0.128215\n",
      "[91]\ttrain's l2: 0.0569341\tvalid's l2: 0.127974\n",
      "[92]\ttrain's l2: 0.0567229\tvalid's l2: 0.128006\n",
      "[93]\ttrain's l2: 0.0563107\tvalid's l2: 0.128423\n",
      "[94]\ttrain's l2: 0.0560136\tvalid's l2: 0.128316\n",
      "[95]\ttrain's l2: 0.0556322\tvalid's l2: 0.128658\n",
      "[96]\ttrain's l2: 0.0553703\tvalid's l2: 0.128453\n",
      "[97]\ttrain's l2: 0.0549577\tvalid's l2: 0.128594\n",
      "[98]\ttrain's l2: 0.0545927\tvalid's l2: 0.12836\n",
      "[99]\ttrain's l2: 0.054231\tvalid's l2: 0.128761\n",
      "[100]\ttrain's l2: 0.0537401\tvalid's l2: 0.129009\n",
      "Model saved to ../out/models/websites/std-gbdt_websites_T100_S0050_L256_R5.model\n",
      "  num_trees  learning_rate num_leaves best_round    metric  \\\n",
      "0       100           0.05        256          5  0.103229   \n",
      "\n",
      "                                            filename  \n",
      "0  ../out/models/websites/std-gbdt_websites_T100_...  \n",
      "best model is: ../out/models/websites/std-gbdt_websites_T100_S0050_L256_R5.model\n"
     ]
    }
   ],
   "source": [
    "# FULL GDBT\n",
    "\n",
    "experiments = train_gradient_boosting_baseline(TRAINING_FILENAME, \n",
    "                                               VALIDATION_FILENAME,\n",
    "                                               TEST_FILENAME,\n",
    "                                               MODEL_FILENAME\n",
    "                                              )  \n",
    "\n",
    "experiments.to_csv(MODEL_FILENAME + \".csv\", index=False)\n",
    "\n",
    "print(experiments)\n",
    "print ('best model is:', experiments.sort_values('metric').iloc[0]['filename'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDUCED GDBT\n",
    "\n",
    "experiments = train_gradient_boosting_baseline(TRAINING_FILENAME, \n",
    "                                               VALIDATION_FILENAME,\n",
    "                                               TEST_FILENAME,\n",
    "                                               RED_MODEL_FILENAME,\n",
    "                                               drop_cols=[] # SET TO THE LIST OF ATTACKED FEATURES\n",
    "                                              )\n",
    "\n",
    "experiments.to_csv(RED_MODEL_FILENAME + \".csv\", index=False)\n",
    "\n",
    "print(experiments)\n",
    "print ('best model is:', experiments.sort_values('metric').iloc[0]['filename'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing original files...\n",
      "Loading: ../data/websites/train.csv.bz2\n",
      "Loading: ../data/websites/valid.csv.bz2\n",
      "Loading: ../data/websites/test.csv.bz2\n",
      "Train/Valid/Test sizes: (1067, 18) (355, 18) (357, 18)\n",
      "Train/Valid/Test split: 0.60 0.20 0.20\n",
      "CatFX: ['CHARSET', 'SERVER', 'WHOIS_COUNTRY', 'WHOIS_STATEPRO']\n",
      "Train/Valid/Test sizes: (1067, 18) (355, 18) (357, 18)\n",
      "Train/Valid/Test split: 0.60 0.20 0.20\n",
      "Saving processed files *.atks.bz2\n",
      "[50]\ttrain's l2: 0.141662\tvalid's l2: 0.157912\n",
      "[100]\ttrain's l2: 0.14055\tvalid's l2: 0.147378\n",
      "Model saved to ../out/models/websites/rf-gbdt_websites_T100_S0050_L256_R1.model\n",
      "  num_trees  learning_rate num_leaves best_round    metric  \\\n",
      "0       100           0.05        256          1  0.090847   \n",
      "\n",
      "                                            filename  \n",
      "0  ../out/models/websites/rf-gbdt_websites_T100_S...  \n",
      "best model is: ../out/models/websites/rf-gbdt_websites_T100_S0050_L256_R1.model\n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST\n",
    "\n",
    "experiments = train_gradient_boosting_baseline(TRAINING_FILENAME, \n",
    "                                               VALIDATION_FILENAME,\n",
    "                                               TEST_FILENAME,\n",
    "                                               RF_MODEL_FILENAME,\n",
    "                                               random_forest=True\n",
    "                                              )  \n",
    "\n",
    "experiments.to_csv(RF_MODEL_FILENAME + \".csv\", index=False)\n",
    "\n",
    "print(experiments)\n",
    "print ('best model is:', experiments.sort_values('metric').iloc[0]['filename'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
