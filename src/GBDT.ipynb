{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBDT - LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "import pickle\n",
    "import json\n",
    "import functools\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from NILib import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gradient_boosting_baseline( train_file, valid_file, test_file,\n",
    "                                output_model_file):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'best_round', 'avg_binary_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    \n",
    "    assert \"instance_id\" not in train.columns.values, \"Wrong training set file for GBDT\"\n",
    "\n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])  \n",
    "    print (\"CatFX:\", train.columns.values[cat_fx])\n",
    "    \n",
    "\n",
    "    for num_trees in [500]:\n",
    "        best_model = None\n",
    "        best_info = None\n",
    "        best_loss = np.inf\n",
    "        for learning_rate in [0.1]: #[0.01, 0.05, 0.1]:\n",
    "            for num_leaves in [16]: #[8, 16, 24]:\n",
    "                # datasets\n",
    "                lgbm_train = lightgbm.Dataset(data=train.iloc[:,:-1].values, \n",
    "                                              label=train.iloc[:,-1].values,\n",
    "                                              categorical_feature = cat_fx)\n",
    "\n",
    "                lgbm_valid = lightgbm.Dataset(data=valid.iloc[:,:-1].values, \n",
    "                                              label=valid.iloc[:,-1].values,\n",
    "                                              categorical_feature = cat_fx)\n",
    "\n",
    "                # run train\n",
    "                lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                'num_leaves': num_leaves} \n",
    "                lgbm_info = {}\n",
    "                lgbm_model = lightgbm.train(lgbm_params, lgbm_train, \n",
    "                                            num_boost_round = num_trees,\n",
    "                                            fobj            = optimize_log_loss, \n",
    "                                            feval           = avg_log_loss,\n",
    "                                            evals_result    = lgbm_info,\n",
    "                                            valid_sets      = [lgbm_train, lgbm_valid], \n",
    "                                            valid_names     = ['train', 'valid'],\n",
    "                                            verbose_eval    = 5)\n",
    "                \n",
    "                if np.min(lgbm_info['valid']['avg_binary_log_loss']) < best_loss:\n",
    "                    best_model = lgbm_model\n",
    "                    best_info = lgbm_info\n",
    "                    best_loss = np.min(lgbm_info['valid']['avg_binary_log_loss'])\n",
    "                    best_info['num_trees'] = num_trees\n",
    "                    best_info['learning_rate'] = learning_rate\n",
    "                    best_info['num_leaves'] = num_leaves\n",
    "                    \n",
    "                    \n",
    "                best_valid_iter = np.argmin(lgbm_info['valid']['avg_binary_log_loss'])\n",
    "                \n",
    "                # update experimental results\n",
    "                exp = exp.append({'num_trees': num_trees, \n",
    "                                  'learning_rate':learning_rate,\n",
    "                                  'num_leaves':num_leaves, \n",
    "                                  'best_round':best_valid_iter+1, \n",
    "                                  'avg_binary_log_loss':lgbm_info['valid']['avg_binary_log_loss'][best_valid_iter]},\n",
    "                                 ignore_index=True)\n",
    "                \n",
    "        \n",
    "        # save file\n",
    "        best_valid_iter = np.argmin(best_info['valid']['avg_binary_log_loss'])\n",
    "\n",
    "        model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                        best_info['num_trees'],\n",
    "                                                                        int(best_info['learning_rate']*1000),\n",
    "                                                                        best_info['num_leaves'],\n",
    "                                                                        best_valid_iter + 1\n",
    "                                                                       )\n",
    "        \n",
    "        best_model.save_model(model_file_name)\n",
    "        print (\"Model saved to\", model_file_name)\n",
    "        \n",
    "        best_model = lightgbm.Booster(model_file=model_file_name)\n",
    "        print (\"Check valid score:\", avg_log_loss(preds=best_model.predict(valid.iloc[:,:-1].values),\n",
    "                                                  train_data=lgbm_valid))\n",
    "\n",
    "    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n",
      "CatFX: ['workclass' 'marital_status' 'occupation' 'relationship' 'race' 'sex'\n",
      " 'native_country']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucchese/.local/lib/python3.6/site-packages/lightgbm/basic.py:1205: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/lucchese/.local/lib/python3.6/site-packages/lightgbm/basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\ttrain's avg_binary_log_loss: 0.5195\tvalid's avg_binary_log_loss: 0.520269\n",
      "[10]\ttrain's avg_binary_log_loss: 0.442694\tvalid's avg_binary_log_loss: 0.443485\n",
      "[15]\ttrain's avg_binary_log_loss: 0.404239\tvalid's avg_binary_log_loss: 0.404565\n",
      "[20]\ttrain's avg_binary_log_loss: 0.383888\tvalid's avg_binary_log_loss: 0.384591\n",
      "[25]\ttrain's avg_binary_log_loss: 0.372059\tvalid's avg_binary_log_loss: 0.372916\n",
      "[30]\ttrain's avg_binary_log_loss: 0.364054\tvalid's avg_binary_log_loss: 0.365226\n",
      "[35]\ttrain's avg_binary_log_loss: 0.359023\tvalid's avg_binary_log_loss: 0.360431\n",
      "[40]\ttrain's avg_binary_log_loss: 0.354723\tvalid's avg_binary_log_loss: 0.35629\n",
      "[45]\ttrain's avg_binary_log_loss: 0.346831\tvalid's avg_binary_log_loss: 0.349523\n",
      "[50]\ttrain's avg_binary_log_loss: 0.332285\tvalid's avg_binary_log_loss: 0.336143\n",
      "[55]\ttrain's avg_binary_log_loss: 0.326506\tvalid's avg_binary_log_loss: 0.331138\n",
      "[60]\ttrain's avg_binary_log_loss: 0.318182\tvalid's avg_binary_log_loss: 0.32389\n",
      "[65]\ttrain's avg_binary_log_loss: 0.312118\tvalid's avg_binary_log_loss: 0.318482\n",
      "[70]\ttrain's avg_binary_log_loss: 0.309225\tvalid's avg_binary_log_loss: 0.316324\n",
      "[75]\ttrain's avg_binary_log_loss: 0.308228\tvalid's avg_binary_log_loss: 0.315905\n",
      "[80]\ttrain's avg_binary_log_loss: 0.304639\tvalid's avg_binary_log_loss: 0.312861\n",
      "[85]\ttrain's avg_binary_log_loss: 0.302875\tvalid's avg_binary_log_loss: 0.311535\n",
      "[90]\ttrain's avg_binary_log_loss: 0.301099\tvalid's avg_binary_log_loss: 0.310236\n",
      "[95]\ttrain's avg_binary_log_loss: 0.298815\tvalid's avg_binary_log_loss: 0.308413\n",
      "[100]\ttrain's avg_binary_log_loss: 0.298178\tvalid's avg_binary_log_loss: 0.308143\n",
      "[105]\ttrain's avg_binary_log_loss: 0.296133\tvalid's avg_binary_log_loss: 0.306573\n",
      "[110]\ttrain's avg_binary_log_loss: 0.294962\tvalid's avg_binary_log_loss: 0.305781\n",
      "[115]\ttrain's avg_binary_log_loss: 0.294326\tvalid's avg_binary_log_loss: 0.305505\n",
      "[120]\ttrain's avg_binary_log_loss: 0.292473\tvalid's avg_binary_log_loss: 0.30415\n",
      "[125]\ttrain's avg_binary_log_loss: 0.291884\tvalid's avg_binary_log_loss: 0.303843\n",
      "[130]\ttrain's avg_binary_log_loss: 0.290919\tvalid's avg_binary_log_loss: 0.303309\n",
      "[135]\ttrain's avg_binary_log_loss: 0.289891\tvalid's avg_binary_log_loss: 0.302703\n",
      "[140]\ttrain's avg_binary_log_loss: 0.288578\tvalid's avg_binary_log_loss: 0.301621\n",
      "[145]\ttrain's avg_binary_log_loss: 0.288096\tvalid's avg_binary_log_loss: 0.301603\n",
      "[150]\ttrain's avg_binary_log_loss: 0.287229\tvalid's avg_binary_log_loss: 0.301281\n",
      "[155]\ttrain's avg_binary_log_loss: 0.286115\tvalid's avg_binary_log_loss: 0.300422\n",
      "[160]\ttrain's avg_binary_log_loss: 0.285523\tvalid's avg_binary_log_loss: 0.300122\n",
      "[165]\ttrain's avg_binary_log_loss: 0.28508\tvalid's avg_binary_log_loss: 0.300049\n",
      "[170]\ttrain's avg_binary_log_loss: 0.284396\tvalid's avg_binary_log_loss: 0.299671\n",
      "[175]\ttrain's avg_binary_log_loss: 0.284013\tvalid's avg_binary_log_loss: 0.299611\n",
      "[180]\ttrain's avg_binary_log_loss: 0.283122\tvalid's avg_binary_log_loss: 0.298957\n",
      "[185]\ttrain's avg_binary_log_loss: 0.282794\tvalid's avg_binary_log_loss: 0.298794\n",
      "[190]\ttrain's avg_binary_log_loss: 0.281846\tvalid's avg_binary_log_loss: 0.298127\n",
      "[195]\ttrain's avg_binary_log_loss: 0.281299\tvalid's avg_binary_log_loss: 0.297973\n",
      "[200]\ttrain's avg_binary_log_loss: 0.280975\tvalid's avg_binary_log_loss: 0.297961\n",
      "[205]\ttrain's avg_binary_log_loss: 0.280439\tvalid's avg_binary_log_loss: 0.297844\n",
      "[210]\ttrain's avg_binary_log_loss: 0.279836\tvalid's avg_binary_log_loss: 0.29764\n",
      "[215]\ttrain's avg_binary_log_loss: 0.279328\tvalid's avg_binary_log_loss: 0.297395\n",
      "[220]\ttrain's avg_binary_log_loss: 0.278995\tvalid's avg_binary_log_loss: 0.297354\n",
      "[225]\ttrain's avg_binary_log_loss: 0.278666\tvalid's avg_binary_log_loss: 0.297261\n",
      "[230]\ttrain's avg_binary_log_loss: 0.278107\tvalid's avg_binary_log_loss: 0.297029\n",
      "[235]\ttrain's avg_binary_log_loss: 0.277556\tvalid's avg_binary_log_loss: 0.296811\n",
      "[240]\ttrain's avg_binary_log_loss: 0.277261\tvalid's avg_binary_log_loss: 0.296731\n",
      "[245]\ttrain's avg_binary_log_loss: 0.277011\tvalid's avg_binary_log_loss: 0.296621\n",
      "[250]\ttrain's avg_binary_log_loss: 0.276761\tvalid's avg_binary_log_loss: 0.296578\n",
      "[255]\ttrain's avg_binary_log_loss: 0.276553\tvalid's avg_binary_log_loss: 0.296544\n",
      "[260]\ttrain's avg_binary_log_loss: 0.276254\tvalid's avg_binary_log_loss: 0.296493\n",
      "[265]\ttrain's avg_binary_log_loss: 0.276039\tvalid's avg_binary_log_loss: 0.296491\n",
      "[270]\ttrain's avg_binary_log_loss: 0.275801\tvalid's avg_binary_log_loss: 0.296568\n",
      "[275]\ttrain's avg_binary_log_loss: 0.275013\tvalid's avg_binary_log_loss: 0.296155\n",
      "[280]\ttrain's avg_binary_log_loss: 0.274656\tvalid's avg_binary_log_loss: 0.296143\n",
      "[285]\ttrain's avg_binary_log_loss: 0.274172\tvalid's avg_binary_log_loss: 0.296098\n",
      "[290]\ttrain's avg_binary_log_loss: 0.273919\tvalid's avg_binary_log_loss: 0.296115\n",
      "[295]\ttrain's avg_binary_log_loss: 0.273588\tvalid's avg_binary_log_loss: 0.296138\n",
      "[300]\ttrain's avg_binary_log_loss: 0.27334\tvalid's avg_binary_log_loss: 0.296146\n",
      "[305]\ttrain's avg_binary_log_loss: 0.273087\tvalid's avg_binary_log_loss: 0.296178\n",
      "[310]\ttrain's avg_binary_log_loss: 0.272691\tvalid's avg_binary_log_loss: 0.296188\n",
      "[315]\ttrain's avg_binary_log_loss: 0.272081\tvalid's avg_binary_log_loss: 0.295973\n",
      "[320]\ttrain's avg_binary_log_loss: 0.271861\tvalid's avg_binary_log_loss: 0.295991\n",
      "[325]\ttrain's avg_binary_log_loss: 0.271586\tvalid's avg_binary_log_loss: 0.296049\n",
      "[330]\ttrain's avg_binary_log_loss: 0.271186\tvalid's avg_binary_log_loss: 0.295909\n",
      "[335]\ttrain's avg_binary_log_loss: 0.270944\tvalid's avg_binary_log_loss: 0.295982\n",
      "[340]\ttrain's avg_binary_log_loss: 0.270492\tvalid's avg_binary_log_loss: 0.295852\n",
      "[345]\ttrain's avg_binary_log_loss: 0.27025\tvalid's avg_binary_log_loss: 0.295909\n",
      "[350]\ttrain's avg_binary_log_loss: 0.269972\tvalid's avg_binary_log_loss: 0.295878\n",
      "[355]\ttrain's avg_binary_log_loss: 0.269555\tvalid's avg_binary_log_loss: 0.295865\n",
      "[360]\ttrain's avg_binary_log_loss: 0.269356\tvalid's avg_binary_log_loss: 0.295909\n",
      "[365]\ttrain's avg_binary_log_loss: 0.269146\tvalid's avg_binary_log_loss: 0.295876\n",
      "[370]\ttrain's avg_binary_log_loss: 0.268957\tvalid's avg_binary_log_loss: 0.295838\n",
      "[375]\ttrain's avg_binary_log_loss: 0.268421\tvalid's avg_binary_log_loss: 0.29575\n",
      "[380]\ttrain's avg_binary_log_loss: 0.268071\tvalid's avg_binary_log_loss: 0.295547\n",
      "[385]\ttrain's avg_binary_log_loss: 0.267841\tvalid's avg_binary_log_loss: 0.295588\n",
      "[390]\ttrain's avg_binary_log_loss: 0.267625\tvalid's avg_binary_log_loss: 0.295608\n",
      "[395]\ttrain's avg_binary_log_loss: 0.267368\tvalid's avg_binary_log_loss: 0.295563\n",
      "[400]\ttrain's avg_binary_log_loss: 0.267179\tvalid's avg_binary_log_loss: 0.295593\n",
      "[405]\ttrain's avg_binary_log_loss: 0.266981\tvalid's avg_binary_log_loss: 0.295585\n",
      "[410]\ttrain's avg_binary_log_loss: 0.266742\tvalid's avg_binary_log_loss: 0.295662\n",
      "[415]\ttrain's avg_binary_log_loss: 0.266543\tvalid's avg_binary_log_loss: 0.295651\n",
      "[420]\ttrain's avg_binary_log_loss: 0.266376\tvalid's avg_binary_log_loss: 0.295654\n",
      "[425]\ttrain's avg_binary_log_loss: 0.266213\tvalid's avg_binary_log_loss: 0.295742\n",
      "[430]\ttrain's avg_binary_log_loss: 0.26605\tvalid's avg_binary_log_loss: 0.295791\n",
      "[435]\ttrain's avg_binary_log_loss: 0.265842\tvalid's avg_binary_log_loss: 0.29586\n",
      "[440]\ttrain's avg_binary_log_loss: 0.265644\tvalid's avg_binary_log_loss: 0.295774\n",
      "[445]\ttrain's avg_binary_log_loss: 0.265467\tvalid's avg_binary_log_loss: 0.295802\n",
      "[450]\ttrain's avg_binary_log_loss: 0.265313\tvalid's avg_binary_log_loss: 0.295829\n",
      "[455]\ttrain's avg_binary_log_loss: 0.265128\tvalid's avg_binary_log_loss: 0.295869\n",
      "[460]\ttrain's avg_binary_log_loss: 0.264844\tvalid's avg_binary_log_loss: 0.295832\n",
      "[465]\ttrain's avg_binary_log_loss: 0.26454\tvalid's avg_binary_log_loss: 0.295706\n",
      "[470]\ttrain's avg_binary_log_loss: 0.264352\tvalid's avg_binary_log_loss: 0.295737\n",
      "[475]\ttrain's avg_binary_log_loss: 0.264176\tvalid's avg_binary_log_loss: 0.295796\n",
      "[480]\ttrain's avg_binary_log_loss: 0.264\tvalid's avg_binary_log_loss: 0.295888\n",
      "[485]\ttrain's avg_binary_log_loss: 0.263842\tvalid's avg_binary_log_loss: 0.295865\n",
      "[490]\ttrain's avg_binary_log_loss: 0.263671\tvalid's avg_binary_log_loss: 0.295858\n",
      "[495]\ttrain's avg_binary_log_loss: 0.263494\tvalid's avg_binary_log_loss: 0.295857\n",
      "[500]\ttrain's avg_binary_log_loss: 0.263347\tvalid's avg_binary_log_loss: 0.295932\n",
      "Model saved to ../out/models/std_gbdt_census_T500_S0100_L16_R394.model\n",
      "Check valid score: ('avg_binary_log_loss', 0.2959318732349072, False)\n",
      "   num_trees  learning_rate  num_leaves  best_round  avg_binary_log_loss\n",
      "0      500.0            0.1        16.0       394.0             0.295525\n"
     ]
    }
   ],
   "source": [
    "# enable/disable LGBM Baseline\n",
    "if True:\n",
    "    experiments = train_gradient_boosting_baseline(\"../data/census/train_ori.csv.bz2\",\n",
    "                                                     \"../data/census/valid_ori.csv.bz2\",\n",
    "                                                     \"../data/census/test_ori.csv.bz2\",\n",
    "                                                     \"../out/models/std_gbdt_census\")  \n",
    "\n",
    "    experiments.to_csv('../out/models/std_gbdt_census.csv', index=False)\n",
    "\n",
    "    print (experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
