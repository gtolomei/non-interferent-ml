{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    " - http://lightgbm.readthedocs.io/en/latest/\n",
    " - http://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
    " - https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "import pickle\n",
    "import json\n",
    "import functools\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nilib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path to dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"census\" # wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR=\"../data/{}\".format(DATASET_NAME)\n",
    "MODELS_DIR=\"../out/models/{}\".format(DATASET_NAME)\n",
    "TRAINING_FILENAME=DATASET_DIR + \"/\" + \"train_ori.csv.bz2\"\n",
    "TRAINING_FILENAME_ATT=DATASET_DIR + \"/\" + \"train_B{}.csv.bz2\"\n",
    "VALIDATION_FILENAME=DATASET_DIR + \"/\" + \"valid_ori.csv.bz2\"\n",
    "VALIDATION_FILENAME_ATT=DATASET_DIR + \"/\" + \"valid_B{}.csv.bz2\"\n",
    "TEST_FILENAME=DATASET_DIR + \"/\" + \"test_ori.csv.bz2\"\n",
    "TEST_FILENAME_ATT=DATASET_DIR + \"/\" + \"test_B{}.csv.bz2\"\n",
    "MODEL_FILENAME=MODELS_DIR + \"/adv-boosting_{}_B{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_BUDGETS= [15] #[5, 15, 150, 300] # [20, 30, 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adv_boosting_data(model, data, groups, num_atks=1):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    groups : grouping of same attacked instance \n",
    "    returns the new data matrix and new groups\n",
    "    \n",
    "    WARNING: currently works only for binary classification\n",
    "    '''\n",
    "    # score the datataset\n",
    "    labels = data[:,-1]\n",
    "    \n",
    "    # check mispredictions\n",
    "    predictions = model.predict(data[:,:-1]) # exclude labels\n",
    "    matchings = labels * predictions\n",
    "    \n",
    "    # select original data + attacked instances\n",
    "    new_selected = [] # id of selected instances\n",
    "    new_groups   = []\n",
    "    \n",
    "    offset = 0\n",
    "    for g in groups:\n",
    "        if g==0:\n",
    "            print (\"Error !!!!\")\n",
    "        elif g==1:\n",
    "            # there are no attacks, just add original\n",
    "            new_selected += [offset]\n",
    "            new_groups   += [1]\n",
    "        else:\n",
    "            # get a slice of the matching scores\n",
    "            g_matchings = matchings[offset:offset+g]\n",
    "\n",
    "            # most misclassified (smallest margin)\n",
    "            # skip original\n",
    "            #adv_instance = np.argmin(g_matchings[1:])+1\n",
    "            adv_instances = np.argsort(g_matchings[1:])\n",
    "            adv_instances = adv_instances[:num_atks]\n",
    "            adv_instances += offset +1\n",
    "\n",
    "            # add original and adversarial\n",
    "            new_selected += [offset] + list(adv_instances)\n",
    "            new_groups   += [1 + len(adv_instances)]\n",
    "        \n",
    "        offset += g\n",
    "    \n",
    "    new_dataset = data[new_selected,:]\n",
    "    \n",
    "    return new_dataset, new_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_adv_boosting_model(train, valid, cat_fx, input_model=None, num_trees=1, params=None):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    returns the new model (is model modified inplace?)\n",
    "    '''\n",
    "    \n",
    "    if cat_fx is None or len(cat_fx)==0:\n",
    "        cat_fx = \"auto\"\n",
    "        \n",
    "    assert train.shape[1]==valid.shape[1], \"Train/Valid Mismatch!\"\n",
    "\n",
    "    lgbm_train = lightgbm.Dataset(data=train[:,:-1], \n",
    "                                  label=train[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_valid = lightgbm.Dataset(data=valid[:,:-1], \n",
    "                                  label=valid[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_info = {}\n",
    "    lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                num_boost_round = num_trees, \n",
    "                                init_model = input_model,\n",
    "                                fobj = optimize_log_loss, \n",
    "                                feval = avg_log_loss,\n",
    "                                evals_result = lgbm_info,\n",
    "                                valid_sets   = [lgbm_train, lgbm_valid], \n",
    "                                valid_names  = ['train', 'valid'],\n",
    "                                verbose_eval=5)\n",
    "\n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdvBoosting(atk_train, atk_valid, trees, \n",
    "                 cat_fx,\n",
    "                 params,\n",
    "                 output_model_file,\n",
    "                 partial_save=1000, \n",
    "                 adv_rounds=1):\n",
    "    ''' \n",
    "    atk_data: full dataset including all valid attacks\n",
    "    atk_groups: lenght of each attack set\n",
    "    trees: total number of trees to be produced\n",
    "    adv_rounds: adversarial instance injecting frequency\n",
    "    '''\n",
    "    # temp lgbm file\n",
    "    temp = output_model_file+\".tmp\"\n",
    "    \n",
    "    # get groups and remove instance ids\n",
    "    atk_groups = atk_train['instance_id'].value_counts().sort_index().values\n",
    "    atk_valid_groups = atk_valid['instance_id'].value_counts().sort_index().values\n",
    "    \n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(atk_train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])  \n",
    "    # print (\"CatFX:\", atk_train.columns.values[cat_fx])\n",
    "\n",
    "    # prepare data (avoiding pandas)\n",
    "    atk_data   = atk_train.iloc[:,1:].values\n",
    "    atk_valid  = atk_valid.iloc[:,1:].values\n",
    "    cat_fx = [x - 1 for x in cat_fx]\n",
    "\n",
    "    # train first trees\n",
    "    original_ids = np.cumsum(atk_groups[:-1])\n",
    "    original_ids = np.insert(original_ids, 0, 0)\n",
    "    \n",
    "    original_valid_ids = np.cumsum(atk_valid_groups[:-1])\n",
    "    original_valid_ids = np.insert(original_valid_ids, 0, 0)\n",
    "    \n",
    "    model, model_info = extend_adv_boosting_model(atk_data[original_ids, :], \n",
    "                                                  atk_valid[original_valid_ids, :],\n",
    "                                                  cat_fx=cat_fx,\n",
    "                                                  input_model=None, \n",
    "                                                  num_trees=adv_rounds, \n",
    "                                                  params=params)\n",
    "    \n",
    "    best_model = model\n",
    "    best_info  = model_info\n",
    "    best_loss  = np.min(model_info['valid']['avg_binary_log_loss'])\n",
    "    best_round = 1\n",
    "        \n",
    "    # train remaining trees\n",
    "    for t in range(adv_rounds+1, trees+1, adv_rounds):\n",
    "        # attack dataset\n",
    "        adv_data, _       = gen_adv_boosting_data(model, atk_data, atk_groups)\n",
    "        adv_valid_data, _ = gen_adv_boosting_data(model, atk_valid, atk_valid_groups)\n",
    "        \n",
    "        # train additional trees\n",
    "        model.save_model(temp)\n",
    "        model, model_info = extend_adv_boosting_model(adv_data, \n",
    "                                                      adv_valid_data,\n",
    "                                                      cat_fx=cat_fx,\n",
    "                                                      input_model=temp, \n",
    "                                                      num_trees=adv_rounds, \n",
    "                                                      params=params)\n",
    "\n",
    "        if np.min(model_info['valid']['avg_binary_log_loss']) < best_loss:\n",
    "            best_model = model\n",
    "            best_info  = model_info\n",
    "            best_loss  = np.min(model_info['valid']['avg_binary_log_loss'])\n",
    "            best_round = t\n",
    "            \n",
    "    \n",
    "    return best_model, best_info, best_loss, best_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial_boosting(train_file, valid_file, test_file, output_model_file):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'best_round', 'avg_binary_log_loss', 'filename'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    assert \"instance_id\" in train.columns.values, \"Wrong training set file for GBDT\"\n",
    "\n",
    "    for num_trees in [50]:\n",
    "        for learning_rate in [0.01, 0.05, 0.1]:\n",
    "            for num_leaves in [8, 16, 24]:\n",
    "                      \n",
    "                lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                'num_leaves': num_leaves} \n",
    "                \n",
    "                lgbm_model, lgbm_info, best_loss, best_valid_iter = AdvBoosting(train,\n",
    "                                                    valid,\n",
    "                                                    trees=num_trees, \n",
    "                                                    cat_fx = cat_fx, \n",
    "                                                    output_model_file=output_model_file, \n",
    "                                                    adv_rounds=1,\n",
    "                                                    params=lgbm_params)\n",
    "                \n",
    "                # save file\n",
    "                model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                                        num_trees,\n",
    "                                                                                        int(learning_rate*1000),\n",
    "                                                                                        num_leaves,\n",
    "                                                                                        best_valid_iter\n",
    "                                                                                       )\n",
    "                ####\n",
    "                # update experimental results\n",
    "                exp = exp.append({'num_trees': num_trees, \n",
    "                                  'learning_rate':learning_rate,\n",
    "                                  'num_leaves':num_leaves, \n",
    "                                  'best_round':best_valid_iter, \n",
    "                                  'avg_binary_log_loss':best_loss,\n",
    "                                  'filename': model_file_name},\n",
    "                                 ignore_index=True)\n",
    "        \n",
    "                lgbm_model.save_model(model_file_name)\n",
    "                print (\"Model saved to\", model_file_name)\n",
    "                \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n",
      "[5]\ttrain's avg_binary_log_loss: 0.662905\tvalid's avg_binary_log_loss: 0.662915\n",
      "[10]\ttrain's avg_binary_log_loss: 0.63464\tvalid's avg_binary_log_loss: 0.634613\n",
      "[15]\ttrain's avg_binary_log_loss: 0.608925\tvalid's avg_binary_log_loss: 0.608931\n",
      "[20]\ttrain's avg_binary_log_loss: 0.585436\tvalid's avg_binary_log_loss: 0.585458\n",
      "[25]\ttrain's avg_binary_log_loss: 0.563903\tvalid's avg_binary_log_loss: 0.563936\n",
      "[30]\ttrain's avg_binary_log_loss: 0.544151\tvalid's avg_binary_log_loss: 0.544211\n",
      "[35]\ttrain's avg_binary_log_loss: 0.525977\tvalid's avg_binary_log_loss: 0.526066\n",
      "[40]\ttrain's avg_binary_log_loss: 0.509242\tvalid's avg_binary_log_loss: 0.509347\n",
      "[45]\ttrain's avg_binary_log_loss: 0.493795\tvalid's avg_binary_log_loss: 0.493919\n",
      "[50]\ttrain's avg_binary_log_loss: 0.479516\tvalid's avg_binary_log_loss: 0.479618\n",
      "Model saved to ../out/models/census/adv-boosting_census_B15_T50_S0010_L8_R50.model\n",
      "[5]\ttrain's avg_binary_log_loss: 0.662213\tvalid's avg_binary_log_loss: 0.662164\n",
      "[10]\ttrain's avg_binary_log_loss: 0.633456\tvalid's avg_binary_log_loss: 0.633324\n",
      "[15]\ttrain's avg_binary_log_loss: 0.607315\tvalid's avg_binary_log_loss: 0.607143\n",
      "[20]\ttrain's avg_binary_log_loss: 0.583223\tvalid's avg_binary_log_loss: 0.583073\n",
      "[25]\ttrain's avg_binary_log_loss: 0.561141\tvalid's avg_binary_log_loss: 0.561033\n",
      "[30]\ttrain's avg_binary_log_loss: 0.540913\tvalid's avg_binary_log_loss: 0.540822\n",
      "[35]\ttrain's avg_binary_log_loss: 0.522292\tvalid's avg_binary_log_loss: 0.522234\n",
      "[40]\ttrain's avg_binary_log_loss: 0.505155\tvalid's avg_binary_log_loss: 0.505136\n",
      "[45]\ttrain's avg_binary_log_loss: 0.489343\tvalid's avg_binary_log_loss: 0.489362\n",
      "[50]\ttrain's avg_binary_log_loss: 0.474624\tvalid's avg_binary_log_loss: 0.474682\n",
      "Model saved to ../out/models/census/adv-boosting_census_B15_T50_S0010_L16_R50.model\n",
      "[5]\ttrain's avg_binary_log_loss: 0.662193\tvalid's avg_binary_log_loss: 0.66218\n",
      "[10]\ttrain's avg_binary_log_loss: 0.633223\tvalid's avg_binary_log_loss: 0.633189\n",
      "[15]\ttrain's avg_binary_log_loss: 0.606659\tvalid's avg_binary_log_loss: 0.606664\n",
      "[20]\ttrain's avg_binary_log_loss: 0.582592\tvalid's avg_binary_log_loss: 0.582628\n",
      "[25]\ttrain's avg_binary_log_loss: 0.560478\tvalid's avg_binary_log_loss: 0.560536\n",
      "[30]\ttrain's avg_binary_log_loss: 0.540148\tvalid's avg_binary_log_loss: 0.540249\n",
      "[35]\ttrain's avg_binary_log_loss: 0.521488\tvalid's avg_binary_log_loss: 0.521634\n",
      "[40]\ttrain's avg_binary_log_loss: 0.504291\tvalid's avg_binary_log_loss: 0.50448\n",
      "[45]\ttrain's avg_binary_log_loss: 0.488417\tvalid's avg_binary_log_loss: 0.48863\n",
      "[50]\ttrain's avg_binary_log_loss: 0.473696\tvalid's avg_binary_log_loss: 0.473948\n",
      "Model saved to ../out/models/census/adv-boosting_census_B15_T50_S0010_L24_R50.model\n",
      "[5]\ttrain's avg_binary_log_loss: 0.564354\tvalid's avg_binary_log_loss: 0.564497\n",
      "[10]\ttrain's avg_binary_log_loss: 0.478794\tvalid's avg_binary_log_loss: 0.478961\n",
      "[15]\ttrain's avg_binary_log_loss: 0.42119\tvalid's avg_binary_log_loss: 0.421175\n",
      "[20]\ttrain's avg_binary_log_loss: 0.38072\tvalid's avg_binary_log_loss: 0.380527\n",
      "[25]\ttrain's avg_binary_log_loss: 0.35327\tvalid's avg_binary_log_loss: 0.352838\n",
      "[30]\ttrain's avg_binary_log_loss: 0.332885\tvalid's avg_binary_log_loss: 0.332212\n",
      "[35]\ttrain's avg_binary_log_loss: 0.318082\tvalid's avg_binary_log_loss: 0.317208\n",
      "[40]\ttrain's avg_binary_log_loss: 0.307163\tvalid's avg_binary_log_loss: 0.30619\n",
      "[45]\ttrain's avg_binary_log_loss: 0.298702\tvalid's avg_binary_log_loss: 0.297661\n",
      "[50]\ttrain's avg_binary_log_loss: 0.292532\tvalid's avg_binary_log_loss: 0.291347\n",
      "Model saved to ../out/models/census/adv-boosting_census_B15_T50_S0050_L8_R50.model\n",
      "[5]\ttrain's avg_binary_log_loss: 0.561359\tvalid's avg_binary_log_loss: 0.561247\n",
      "[10]\ttrain's avg_binary_log_loss: 0.474112\tvalid's avg_binary_log_loss: 0.47408\n",
      "[15]\ttrain's avg_binary_log_loss: 0.41494\tvalid's avg_binary_log_loss: 0.415064\n",
      "[20]\ttrain's avg_binary_log_loss: 0.374157\tvalid's avg_binary_log_loss: 0.374353\n",
      "[25]\ttrain's avg_binary_log_loss: 0.345603\tvalid's avg_binary_log_loss: 0.345752\n",
      "[30]\ttrain's avg_binary_log_loss: 0.325147\tvalid's avg_binary_log_loss: 0.325257\n",
      "[35]\ttrain's avg_binary_log_loss: 0.31062\tvalid's avg_binary_log_loss: 0.310622\n",
      "[40]\ttrain's avg_binary_log_loss: 0.299887\tvalid's avg_binary_log_loss: 0.299862\n",
      "[45]\ttrain's avg_binary_log_loss: 0.292022\tvalid's avg_binary_log_loss: 0.291984\n",
      "[50]\ttrain's avg_binary_log_loss: 0.286437\tvalid's avg_binary_log_loss: 0.286376\n",
      "Model saved to ../out/models/census/adv-boosting_census_B15_T50_S0050_L16_R50.model\n",
      "[5]\ttrain's avg_binary_log_loss: 0.561374\tvalid's avg_binary_log_loss: 0.561388\n",
      "[10]\ttrain's avg_binary_log_loss: 0.473617\tvalid's avg_binary_log_loss: 0.473884\n",
      "[15]\ttrain's avg_binary_log_loss: 0.4138\tvalid's avg_binary_log_loss: 0.414358\n",
      "[20]\ttrain's avg_binary_log_loss: 0.372502\tvalid's avg_binary_log_loss: 0.373223\n",
      "[25]\ttrain's avg_binary_log_loss: 0.343683\tvalid's avg_binary_log_loss: 0.344552\n",
      "[30]\ttrain's avg_binary_log_loss: 0.323326\tvalid's avg_binary_log_loss: 0.324192\n",
      "[35]\ttrain's avg_binary_log_loss: 0.308468\tvalid's avg_binary_log_loss: 0.309252\n",
      "[40]\ttrain's avg_binary_log_loss: 0.297975\tvalid's avg_binary_log_loss: 0.298707\n",
      "[45]\ttrain's avg_binary_log_loss: 0.290181\tvalid's avg_binary_log_loss: 0.290984\n",
      "[50]\ttrain's avg_binary_log_loss: 0.284482\tvalid's avg_binary_log_loss: 0.28539\n",
      "Model saved to ../out/models/census/adv-boosting_census_B15_T50_S0050_L24_R50.model\n",
      "[5]\ttrain's avg_binary_log_loss: 0.477839\tvalid's avg_binary_log_loss: 0.478036\n",
      "[10]\ttrain's avg_binary_log_loss: 0.379111\tvalid's avg_binary_log_loss: 0.378872\n",
      "[15]\ttrain's avg_binary_log_loss: 0.330426\tvalid's avg_binary_log_loss: 0.32931\n",
      "[20]\ttrain's avg_binary_log_loss: 0.305553\tvalid's avg_binary_log_loss: 0.304451\n",
      "[25]\ttrain's avg_binary_log_loss: 0.292244\tvalid's avg_binary_log_loss: 0.291018\n",
      "[30]\ttrain's avg_binary_log_loss: 0.28243\tvalid's avg_binary_log_loss: 0.281123\n",
      "[35]\ttrain's avg_binary_log_loss: 0.277887\tvalid's avg_binary_log_loss: 0.276584\n",
      "[40]\ttrain's avg_binary_log_loss: 0.274276\tvalid's avg_binary_log_loss: 0.273052\n",
      "[45]\ttrain's avg_binary_log_loss: 0.271858\tvalid's avg_binary_log_loss: 0.270821\n",
      "[50]\ttrain's avg_binary_log_loss: 0.270122\tvalid's avg_binary_log_loss: 0.269215\n",
      "Model saved to ../out/models/census/adv-boosting_census_B15_T50_S0100_L8_R50.model\n",
      "[5]\ttrain's avg_binary_log_loss: 0.472742\tvalid's avg_binary_log_loss: 0.472871\n",
      "[10]\ttrain's avg_binary_log_loss: 0.373226\tvalid's avg_binary_log_loss: 0.37343\n",
      "[15]\ttrain's avg_binary_log_loss: 0.324034\tvalid's avg_binary_log_loss: 0.324058\n",
      "[20]\ttrain's avg_binary_log_loss: 0.299479\tvalid's avg_binary_log_loss: 0.299361\n",
      "[25]\ttrain's avg_binary_log_loss: 0.285966\tvalid's avg_binary_log_loss: 0.286034\n",
      "[30]\ttrain's avg_binary_log_loss: 0.277924\tvalid's avg_binary_log_loss: 0.278311\n",
      "[35]\ttrain's avg_binary_log_loss: 0.272569\tvalid's avg_binary_log_loss: 0.273311\n",
      "[40]\ttrain's avg_binary_log_loss: 0.269336\tvalid's avg_binary_log_loss: 0.27039\n",
      "[45]\ttrain's avg_binary_log_loss: 0.266831\tvalid's avg_binary_log_loss: 0.268237\n",
      "[50]\ttrain's avg_binary_log_loss: 0.260285\tvalid's avg_binary_log_loss: 0.262118\n",
      "Model saved to ../out/models/census/adv-boosting_census_B15_T50_S0100_L16_R50.model\n",
      "[5]\ttrain's avg_binary_log_loss: 0.472676\tvalid's avg_binary_log_loss: 0.472825\n",
      "[10]\ttrain's avg_binary_log_loss: 0.371999\tvalid's avg_binary_log_loss: 0.372379\n",
      "[15]\ttrain's avg_binary_log_loss: 0.322463\tvalid's avg_binary_log_loss: 0.323105\n",
      "[20]\ttrain's avg_binary_log_loss: 0.297377\tvalid's avg_binary_log_loss: 0.297737\n",
      "[25]\ttrain's avg_binary_log_loss: 0.283853\tvalid's avg_binary_log_loss: 0.284525\n",
      "[30]\ttrain's avg_binary_log_loss: 0.27616\tvalid's avg_binary_log_loss: 0.277384\n",
      "[35]\ttrain's avg_binary_log_loss: 0.270537\tvalid's avg_binary_log_loss: 0.272126\n",
      "[40]\ttrain's avg_binary_log_loss: 0.267698\tvalid's avg_binary_log_loss: 0.269741\n",
      "[45]\ttrain's avg_binary_log_loss: 0.265112\tvalid's avg_binary_log_loss: 0.267857\n",
      "[50]\ttrain's avg_binary_log_loss: 0.259858\tvalid's avg_binary_log_loss: 0.26309\n",
      "Model saved to ../out/models/census/adv-boosting_census_B15_T50_S0100_L24_R47.model\n",
      "  num_trees  learning_rate num_leaves best_round  avg_binary_log_loss  \\\n",
      "0        50           0.01          8         50             0.479618   \n",
      "1        50           0.01         16         50             0.474682   \n",
      "2        50           0.01         24         50             0.473948   \n",
      "3        50           0.05          8         50             0.291347   \n",
      "4        50           0.05         16         50             0.286376   \n",
      "5        50           0.05         24         50             0.285390   \n",
      "6        50           0.10          8         50             0.269215   \n",
      "7        50           0.10         16         50             0.262118   \n",
      "8        50           0.10         24         47             0.262908   \n",
      "\n",
      "                                            filename  \n",
      "0  ../out/models/census/adv-boosting_census_B15_T...  \n",
      "1  ../out/models/census/adv-boosting_census_B15_T...  \n",
      "2  ../out/models/census/adv-boosting_census_B15_T...  \n",
      "3  ../out/models/census/adv-boosting_census_B15_T...  \n",
      "4  ../out/models/census/adv-boosting_census_B15_T...  \n",
      "5  ../out/models/census/adv-boosting_census_B15_T...  \n",
      "6  ../out/models/census/adv-boosting_census_B15_T...  \n",
      "7  ../out/models/census/adv-boosting_census_B15_T...  \n",
      "8  ../out/models/census/adv-boosting_census_B15_T...  \n"
     ]
    }
   ],
   "source": [
    "for B in TRAINING_BUDGETS:\n",
    "\n",
    "        experiments = train_adversarial_boosting(TRAINING_FILENAME_ATT.format(B),\n",
    "                                                 VALIDATION_FILENAME_ATT.format(B),\n",
    "                                                 TEST_FILENAME_ATT.format(B),\n",
    "                                                 MODEL_FILENAME.format(DATASET_NAME, B))  \n",
    "\n",
    "        experiments.to_csv(MODEL_FILENAME.format(DATASET_NAME, B) + \".csv\", index=False)\n",
    "\n",
    "        print(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_trees</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_leaves</th>\n",
       "      <th>best_round</th>\n",
       "      <th>avg_binary_log_loss</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>16</td>\n",
       "      <td>50</td>\n",
       "      <td>0.262118</td>\n",
       "      <td>../out/models/census/adv-boosting_census_B15_T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>24</td>\n",
       "      <td>47</td>\n",
       "      <td>0.262908</td>\n",
       "      <td>../out/models/census/adv-boosting_census_B15_T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>0.269215</td>\n",
       "      <td>../out/models/census/adv-boosting_census_B15_T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>0.05</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "      <td>0.285390</td>\n",
       "      <td>../out/models/census/adv-boosting_census_B15_T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>0.05</td>\n",
       "      <td>16</td>\n",
       "      <td>50</td>\n",
       "      <td>0.286376</td>\n",
       "      <td>../out/models/census/adv-boosting_census_B15_T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>0.05</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>0.291347</td>\n",
       "      <td>../out/models/census/adv-boosting_census_B15_T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "      <td>0.473948</td>\n",
       "      <td>../out/models/census/adv-boosting_census_B15_T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>16</td>\n",
       "      <td>50</td>\n",
       "      <td>0.474682</td>\n",
       "      <td>../out/models/census/adv-boosting_census_B15_T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>0.479618</td>\n",
       "      <td>../out/models/census/adv-boosting_census_B15_T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  num_trees  learning_rate num_leaves best_round  avg_binary_log_loss  \\\n",
       "7        50           0.10         16         50             0.262118   \n",
       "8        50           0.10         24         47             0.262908   \n",
       "6        50           0.10          8         50             0.269215   \n",
       "5        50           0.05         24         50             0.285390   \n",
       "4        50           0.05         16         50             0.286376   \n",
       "3        50           0.05          8         50             0.291347   \n",
       "2        50           0.01         24         50             0.473948   \n",
       "1        50           0.01         16         50             0.474682   \n",
       "0        50           0.01          8         50             0.479618   \n",
       "\n",
       "                                            filename  \n",
       "7  ../out/models/census/adv-boosting_census_B15_T...  \n",
       "8  ../out/models/census/adv-boosting_census_B15_T...  \n",
       "6  ../out/models/census/adv-boosting_census_B15_T...  \n",
       "5  ../out/models/census/adv-boosting_census_B15_T...  \n",
       "4  ../out/models/census/adv-boosting_census_B15_T...  \n",
       "3  ../out/models/census/adv-boosting_census_B15_T...  \n",
       "2  ../out/models/census/adv-boosting_census_B15_T...  \n",
       "1  ../out/models/census/adv-boosting_census_B15_T...  \n",
       "0  ../out/models/census/adv-boosting_census_B15_T...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments.sort_values('avg_binary_log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model is: ../out/models/census/adv-boosting_census_B15_T50_S0100_L16_R50.model\n"
     ]
    }
   ],
   "source": [
    "print ('best model is:', experiments.sort_values('avg_binary_log_loss').iloc[0]['filename'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
