{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    " - http://lightgbm.readthedocs.io/en/latest/\n",
    " - http://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
    " - https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "import pickle\n",
    "import json\n",
    "import functools\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from nilib import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adv_boosting_data(model, data, groups, num_atks=1):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    groups : grouping of same attacked instance \n",
    "    returns the new data matrix and new groups\n",
    "    \n",
    "    WARNING: currently works only for binary classification\n",
    "    '''\n",
    "    # score the datataset\n",
    "    labels = data[:,-1]\n",
    "    \n",
    "    # check mispredictions\n",
    "    predictions = model.predict(data[:,:-1]) # exclude labels\n",
    "    matchings = labels * predictions\n",
    "    \n",
    "    # select original data + attacked instances\n",
    "    new_selected = [] # id of selected instances\n",
    "    new_groups   = []\n",
    "    \n",
    "    offset = 0\n",
    "    for g in groups:\n",
    "        if g==0:\n",
    "            print (\"Error !!!!\")\n",
    "        elif g==1:\n",
    "            # there are no attacks, just add original\n",
    "            new_selected += [offset]\n",
    "            new_groups   += [1]\n",
    "        else:\n",
    "            # get a slice of the matching scores\n",
    "            g_matchings = matchings[offset:offset+g]\n",
    "\n",
    "            # most misclassified (smallest margin)\n",
    "            # skip original\n",
    "            #adv_instance = np.argmin(g_matchings[1:])+1\n",
    "            adv_instances = np.argsort(g_matchings[1:])\n",
    "            adv_instances = adv_instances[:num_atks]\n",
    "            adv_instances += offset +1\n",
    "\n",
    "            # add original and adversarial\n",
    "            new_selected += [offset] + list(adv_instances)\n",
    "            new_groups   += [1 + len(adv_instances)]\n",
    "        \n",
    "        offset += g\n",
    "    \n",
    "    new_dataset = data[new_selected,:]\n",
    "    \n",
    "    return new_dataset, new_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_adv_boosting_model(train, valid, cat_fx, input_model=None, num_trees=1, params=None):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    returns the new model (is model modified inplace?)\n",
    "    '''\n",
    "    \n",
    "    if cat_fx is None or len(cat_fx)==0:\n",
    "        cat_fx = \"auto\"\n",
    "        \n",
    "    assert train.shape[1]==valid.shape[1], \"Train/Valid Mismatch!\"\n",
    "\n",
    "    lgbm_train = lightgbm.Dataset(data=train[:,:-1], \n",
    "                                  label=train[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_valid = lightgbm.Dataset(data=valid[:,:-1], \n",
    "                                  label=valid[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_info = {}\n",
    "    lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                num_boost_round = num_trees, \n",
    "                                init_model = input_model,\n",
    "                                fobj = optimize_log_loss, \n",
    "                                feval = avg_log_loss,\n",
    "                                evals_result = lgbm_info,\n",
    "                                valid_sets   = [lgbm_train, lgbm_valid], \n",
    "                                valid_names  = ['train', 'valid'],\n",
    "                                verbose_eval=5)\n",
    "\n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdvBoosting(atk_train, atk_valid, trees, \n",
    "                 cat_fx,\n",
    "                 params,\n",
    "                 output_model_file,\n",
    "                 partial_save=1000, \n",
    "                 adv_rounds=1):\n",
    "    ''' \n",
    "    atk_data: full dataset including all valid attacks\n",
    "    atk_groups: lenght of each attack set\n",
    "    trees: total number of trees to be produced\n",
    "    adv_rounds: adversarial instance injecting frequency\n",
    "    '''\n",
    "    # temp lgbm file\n",
    "    temp = output_model_file+\".tmp\"\n",
    "    \n",
    "    # get groups and remove instance ids\n",
    "    atk_groups = atk_train['instance_id'].value_counts().sort_index().values\n",
    "    atk_valid_groups = atk_valid['instance_id'].value_counts().sort_index().values\n",
    "    \n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(atk_train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])  \n",
    "    # print (\"CatFX:\", atk_train.columns.values[cat_fx])\n",
    "\n",
    "    # prepare data (avoiding pandas)\n",
    "    atk_data   = atk_train.iloc[:,1:].values\n",
    "    atk_valid  = atk_valid.iloc[:,1:].values\n",
    "    cat_fx = [x - 1 for x in cat_fx]\n",
    "\n",
    "    # train first trees\n",
    "    original_ids = np.cumsum(atk_groups[:-1])\n",
    "    original_ids = np.insert(original_ids, 0, 0)\n",
    "    \n",
    "    original_valid_ids = np.cumsum(atk_valid_groups[:-1])\n",
    "    original_valid_ids = np.insert(original_valid_ids, 0, 0)\n",
    "    \n",
    "    model, model_info = extend_adv_boosting_model(atk_data[original_ids, :], \n",
    "                                                  atk_valid[original_valid_ids, :],\n",
    "                                                  cat_fx=cat_fx,\n",
    "                                                  input_model=None, \n",
    "                                                  num_trees=adv_rounds, \n",
    "                                                  params=params)\n",
    "    \n",
    "    best_model = model\n",
    "    best_info = model_info\n",
    "    best_loss = np.min(model_info['valid']['avg_binary_log_loss'])\n",
    "    best_round = 1\n",
    "        \n",
    "    # train remaining trees\n",
    "    for t in range(adv_rounds+1, trees+1, adv_rounds):\n",
    "        # attack dataset\n",
    "        adv_data, _       = gen_adv_boosting_data(model, atk_data, atk_groups)\n",
    "        adv_valid_data, _ = gen_adv_boosting_data(model, atk_valid, atk_valid_groups)\n",
    "        \n",
    "        # train additional trees\n",
    "        model.save_model(temp)\n",
    "        model, model_info = extend_adv_boosting_model(adv_data, \n",
    "                                                      adv_valid_data,\n",
    "                                                      cat_fx=cat_fx,\n",
    "                                                      input_model=temp, \n",
    "                                                      num_trees=adv_rounds, \n",
    "                                                      params=params)\n",
    "\n",
    "        if np.min(model_info['valid']['avg_binary_log_loss']) < best_loss:\n",
    "            best_model = model\n",
    "            best_info  = model_info\n",
    "            best_loss  = np.min(model_info['valid']['avg_binary_log_loss'])\n",
    "            best_round = t\n",
    "        \n",
    "        # save partial model\n",
    "        if t % partial_save == 0 and t != trees:\n",
    "            partial_filename = \"{:s}_T{:d}-of-{:d}_S{:04d}_L{:d}.model.tmp\".format(output_model_file, \n",
    "                                                                                   t, \n",
    "                                                                                   trees, \n",
    "                                                                                   int(params['learning_rate'] * 1000),\n",
    "                                                                                   params['num_leaves']\n",
    "                                                                                  )\n",
    "            \n",
    "            print(\"Save partial model to {}\".format(partial_filename))\n",
    "            model.save_model(filename=partial_filename)\n",
    "            \n",
    "    \n",
    "    return model, model_info, best_loss, best_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial_boosting(train_file, valid_file, test_file, output_model_file):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'best_round', 'avg_binary_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    assert \"instance_id\" in train.columns.values, \"Wrong training set file for GBDT\"\n",
    "\n",
    "    for num_trees in [200]:\n",
    "        for learning_rate in [0.1]: #[0.01, 0.05, 0.1]:\n",
    "            for num_leaves in [16]: #[8, 16, 24]:\n",
    "                      \n",
    "                lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                'num_leaves': num_leaves} \n",
    "                \n",
    "                lgbm_model, lgbm_info, best_loss, best_valid_iter = AdvBoosting(train,\n",
    "                                                    valid,\n",
    "                                                    trees=num_trees, \n",
    "                                                    cat_fx = cat_fx, \n",
    "                                                    output_model_file=output_model_file, \n",
    "                                                    adv_rounds=1,\n",
    "                                                    params=lgbm_params)\n",
    "                \n",
    "\n",
    "                ####\n",
    "                #best_model = lightgbm.Booster(model_file=model_file_name)\n",
    "                atk_valid_groups = valid['instance_id'].value_counts().sort_index().values\n",
    "                cat_fx = np.where(valid.columns.isin(cat_fx))[0]\n",
    "                cat_fx = list([int(x) for x in cat_fx])  \n",
    "\n",
    "                atk_valid  = valid.iloc[:,1:].values\n",
    "                cat_fx = [x - 1 for x in cat_fx]\n",
    "                \n",
    "                original_valid_ids = np.cumsum(atk_valid_groups[:-1])\n",
    "                original_valid_ids = np.insert(original_valid_ids, 0, 0)\n",
    "\n",
    "                lgbm_valid = lightgbm.Dataset(data=atk_valid[original_valid_ids,:-1], \n",
    "                                              label=atk_valid[original_valid_ids,-1],\n",
    "                                              categorical_feature = cat_fx)\n",
    "                \n",
    "                \n",
    "                lgbm_valid_att = lightgbm.Dataset(data=atk_valid[:,:-1], \n",
    "                                              label=atk_valid[:,-1],\n",
    "                                              categorical_feature = cat_fx)\n",
    "                \n",
    "                print (\"Check valid score without attacks:\", avg_log_loss(preds=lgbm_model.predict(atk_valid[original_valid_ids,:-1]),\n",
    "                                                  train_data=lgbm_valid))\n",
    "                \n",
    "                print (\"Check valid score with attacks:\", avg_log_loss(preds=lgbm_model.predict(atk_valid[:,:-1]),\n",
    "                                                                       train_data=lgbm_valid_att))\n",
    "\n",
    "                \n",
    "                ####\n",
    "                # update experimental results\n",
    "                exp = exp.append({'num_trees': num_trees, \n",
    "                                  'learning_rate':learning_rate,\n",
    "                                  'num_leaves':num_leaves, \n",
    "                                  'best_round':best_valid_iter, \n",
    "                                  'avg_binary_log_loss':best_loss},\n",
    "                                 ignore_index=True)\n",
    "        \n",
    "        # save file\n",
    "        model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                                num_trees,\n",
    "                                                                                int(learning_rate*1000),\n",
    "                                                                                num_leaves,\n",
    "                                                                                best_valid_iter\n",
    "                                                                               )\n",
    "        lgbm_model.save_model(model_file_name)\n",
    "        print (\"Model saved to\", model_file_name)\n",
    "                \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucchese/.local/lib/python3.6/site-packages/lightgbm/basic.py:1205: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/lucchese/.local/lib/python3.6/site-packages/lightgbm/basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\ttrain's avg_binary_log_loss: 0.473641\tvalid's avg_binary_log_loss: 0.473848\n",
      "[10]\ttrain's avg_binary_log_loss: 0.375873\tvalid's avg_binary_log_loss: 0.376209\n",
      "[15]\ttrain's avg_binary_log_loss: 0.327713\tvalid's avg_binary_log_loss: 0.327717\n",
      "[20]\ttrain's avg_binary_log_loss: 0.302276\tvalid's avg_binary_log_loss: 0.302374\n",
      "[25]\ttrain's avg_binary_log_loss: 0.288803\tvalid's avg_binary_log_loss: 0.288819\n",
      "[30]\ttrain's avg_binary_log_loss: 0.281296\tvalid's avg_binary_log_loss: 0.281551\n",
      "[35]\ttrain's avg_binary_log_loss: 0.275863\tvalid's avg_binary_log_loss: 0.276531\n",
      "[40]\ttrain's avg_binary_log_loss: 0.272852\tvalid's avg_binary_log_loss: 0.2734\n",
      "[45]\ttrain's avg_binary_log_loss: 0.270462\tvalid's avg_binary_log_loss: 0.271391\n",
      "[50]\ttrain's avg_binary_log_loss: 0.268955\tvalid's avg_binary_log_loss: 0.270248\n",
      "[55]\ttrain's avg_binary_log_loss: 0.263705\tvalid's avg_binary_log_loss: 0.265344\n",
      "[60]\ttrain's avg_binary_log_loss: 0.262287\tvalid's avg_binary_log_loss: 0.264185\n",
      "[65]\ttrain's avg_binary_log_loss: 0.255353\tvalid's avg_binary_log_loss: 0.258198\n",
      "[70]\ttrain's avg_binary_log_loss: 0.24399\tvalid's avg_binary_log_loss: 0.247866\n",
      "[75]\ttrain's avg_binary_log_loss: 0.243582\tvalid's avg_binary_log_loss: 0.247499\n",
      "[80]\ttrain's avg_binary_log_loss: 0.237756\tvalid's avg_binary_log_loss: 0.24233\n",
      "[85]\ttrain's avg_binary_log_loss: 0.233509\tvalid's avg_binary_log_loss: 0.238656\n",
      "[90]\ttrain's avg_binary_log_loss: 0.233153\tvalid's avg_binary_log_loss: 0.238519\n",
      "[95]\ttrain's avg_binary_log_loss: 0.230266\tvalid's avg_binary_log_loss: 0.235932\n",
      "[100]\ttrain's avg_binary_log_loss: 0.229718\tvalid's avg_binary_log_loss: 0.235613\n",
      "[105]\ttrain's avg_binary_log_loss: 0.227914\tvalid's avg_binary_log_loss: 0.234192\n",
      "[110]\ttrain's avg_binary_log_loss: 0.225507\tvalid's avg_binary_log_loss: 0.232139\n",
      "[115]\ttrain's avg_binary_log_loss: 0.224098\tvalid's avg_binary_log_loss: 0.230867\n",
      "[120]\ttrain's avg_binary_log_loss: 0.223587\tvalid's avg_binary_log_loss: 0.230565\n",
      "[125]\ttrain's avg_binary_log_loss: 0.223113\tvalid's avg_binary_log_loss: 0.230198\n",
      "[130]\ttrain's avg_binary_log_loss: 0.222675\tvalid's avg_binary_log_loss: 0.229911\n",
      "[135]\ttrain's avg_binary_log_loss: 0.221988\tvalid's avg_binary_log_loss: 0.229515\n",
      "[140]\ttrain's avg_binary_log_loss: 0.220901\tvalid's avg_binary_log_loss: 0.228592\n",
      "[145]\ttrain's avg_binary_log_loss: 0.220524\tvalid's avg_binary_log_loss: 0.228312\n",
      "[150]\ttrain's avg_binary_log_loss: 0.219633\tvalid's avg_binary_log_loss: 0.22765\n",
      "[155]\ttrain's avg_binary_log_loss: 0.219274\tvalid's avg_binary_log_loss: 0.22743\n",
      "[160]\ttrain's avg_binary_log_loss: 0.21832\tvalid's avg_binary_log_loss: 0.226633\n",
      "[165]\ttrain's avg_binary_log_loss: 0.218025\tvalid's avg_binary_log_loss: 0.226426\n",
      "[170]\ttrain's avg_binary_log_loss: 0.217197\tvalid's avg_binary_log_loss: 0.225724\n",
      "[175]\ttrain's avg_binary_log_loss: 0.216509\tvalid's avg_binary_log_loss: 0.22523\n",
      "[180]\ttrain's avg_binary_log_loss: 0.216228\tvalid's avg_binary_log_loss: 0.225196\n",
      "[185]\ttrain's avg_binary_log_loss: 0.215479\tvalid's avg_binary_log_loss: 0.224571\n",
      "[190]\ttrain's avg_binary_log_loss: 0.215217\tvalid's avg_binary_log_loss: 0.22442\n",
      "[195]\ttrain's avg_binary_log_loss: 0.214999\tvalid's avg_binary_log_loss: 0.224274\n",
      "[200]\ttrain's avg_binary_log_loss: 0.2144\tvalid's avg_binary_log_loss: 0.223993\n",
      "Check valid score without attacks: ('avg_binary_log_loss', 0.32306689370078223, False)\n",
      "Check valid score with attacks: ('avg_binary_log_loss', 0.035280414017167375, False)\n",
      "Model saved to ../out/models/adv_boosting_census_B150_T200_S0100_L16_R200.model\n",
      "   num_trees  learning_rate  num_leaves  best_round  avg_binary_log_loss\n",
      "0      200.0            0.1        16.0       200.0             0.223993\n"
     ]
    }
   ],
   "source": [
    "# enable/disable\n",
    "if True:\n",
    "    for B in [150]: #[5, 15, 150, 300]:\n",
    "\n",
    "        experiments = train_adversarial_boosting ( \"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/test_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../out/models/adv_boosting_census_B{:d}\".format(B))  \n",
    "\n",
    "        experiments.to_csv('../out/models/adv_boosting_census_B{:d}.csv'.format(B), index=False)\n",
    "\n",
    "        print (experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**some old results**\n",
    "\n",
    "    Check valid score without attacks: ('avg_binary_log_loss', 0.3237495543276321, False)\n",
    "    Check valid score with attacks: ('avg_binary_log_loss', 0.03632538905070899, False)\n",
    "    Model saved to ../out/models/adv_boosting_census_B150_T200_S0100_L16_R200.model\n",
    "       num_trees  learning_rate  num_leaves  best_round  avg_binary_log_loss\n",
    "    0      200.0            0.1        16.0       200.0             0.224304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
