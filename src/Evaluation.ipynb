{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models\n",
    "\n",
    "This notebook contains the code used for evaluating the following learning models:\n",
    "\n",
    "-  **Standard GBDT** (_baseline 1_)\n",
    "-  **Adversarial Boosting** (_baseline 2_)\n",
    "-  **Non-Interferent GBDT** (our proposal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-28 17:30:41,007 *** INFO [robust_forest.py:1151 - __init__()] *** ***** Robust Decision Tree successfully created *****\n",
      "2019-05-28 17:30:41,010 *** INFO [robust_forest.py:1152 - __init__()] *** *\tTree ID: 0\n",
      "2019-05-28 17:30:41,011 *** INFO [robust_forest.py:1153 - __init__()] *** *\tAttacker: <robust_forest.Attacker object at 0x7f6e302a9668>\n",
      "2019-05-28 17:30:41,012 *** INFO [robust_forest.py:1155 - __init__()] *** *\tSplitting criterion: SSE\n",
      "2019-05-28 17:30:41,013 *** INFO [robust_forest.py:1156 - __init__()] *** *\tMax depth: 8\n",
      "2019-05-28 17:30:41,014 *** INFO [robust_forest.py:1158 - __init__()] *** *\tMin instances per tree node: 20\n",
      "2019-05-28 17:30:41,015 *** INFO [robust_forest.py:1160 - __init__()] *** *\tMax samples: 100.0%\n",
      "2019-05-28 17:30:41,016 *** INFO [robust_forest.py:1162 - __init__()] *** *\tMax features: 100.0%\n",
      "2019-05-28 17:30:41,017 *** INFO [robust_forest.py:1164 - __init__()] *** *\tFeature blacklist: set()\n",
      "2019-05-28 17:30:41,018 *** INFO [robust_forest.py:1165 - __init__()] *** *\tAffinity: False\n",
      "2019-05-28 17:30:41,019 *** INFO [robust_forest.py:1167 - __init__()] *** *****************************************************\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import dill\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm\n",
    "import functools\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "from nilib import *\n",
    "from robust_forest import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard evaluation metric\n",
    "\n",
    "The following function is the one used for evaluating the quality of the learned model (either _standard_, _adversarial-boosting_, or _non-interferent_). This is the standard <code>avg_log_loss</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(p):\n",
    "    return np.log(p/(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(preds):\n",
    "    if np.min(preds)<-0.001:\n",
    "        return np.where(preds>=0,  1.0, -1.0)\n",
    "    else:\n",
    "        return np.where(preds>=.5, 1.0, -1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>avg_log_loss</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss(preds, train_data):\n",
    "    \n",
    "    labels = train_data.get_label()\n",
    "    losses = np.log(1.0 + np.exp(-preds*labels))\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return 'avg_binary_log_loss', avg_loss, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_log_loss(y_true, y_pred):\n",
    "    losses = np.log(1.0 + np.exp(-y_pred*y_true))\n",
    "    avg_loss = np.mean(losses)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom evaluation metric\n",
    "\n",
    "Similarly to what we have done for <code>fobj</code>, <code>feval</code> can be computed from a weighted combination of two evaluation metrics:\n",
    "\n",
    "-  <code>avg_log_loss</code> (standard, defined above);\n",
    "-  <code>avg_log_loss_uma</code> (custom, defined below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>avg_log_loss_uma</code>\n",
    "\n",
    "This is the binary log loss yet modified to operate on groups of perturbed instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom metric\n",
    "\n",
    "def binary_log_loss(pred, true_label):\n",
    "\n",
    "    return np.log(1.0 + np.exp(-pred * true_label))\n",
    "\n",
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss_uma(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    offset = 0\n",
    "    max_logloss = []\n",
    "    avg_max_logloss = 0.0\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "    \n",
    "        for atk in attack_lens:\n",
    "            losses = [binary_log_loss(h,t) for h,t in zip(preds[offset:offset+atk], labels[offset:offset+atk])]\n",
    "            max_logloss.append(max(losses))\n",
    "            \n",
    "            offset += atk\n",
    "        \n",
    "        avg_max_logloss = np.mean(max_logloss)  \n",
    "        \n",
    "    return 'avg_binary_log_loss_under_max_attack', avg_max_logloss, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_log_loss_uma(preds, test, test_groups=None, svm=False):\n",
    "    \n",
    "    lgbm_test = lightgbm.Dataset(data=test.iloc[:,:-1].values, \n",
    "                                 label=test.iloc[:,-1].values,\n",
    "                                 group=test_groups,\n",
    "                                 free_raw_data=False)\n",
    "    \n",
    "    return avg_log_loss_uma(preds,lgbm_test)[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_binary_err_rate</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_binary_err_rate(y_true, y_pred):\n",
    "    errs = np.sum(binarize(y_pred) != y_true)\n",
    "    return errs/len(y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_roc_auc</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_roc_auc(y_true, y_pred):\n",
    "    return roc_auc_score(y_true=y_true, y_score=y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_specificity</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_specificity(y_true, y_pred):\n",
    "    y_pred = binarize(y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true=y_true, y_pred=y_pred).ravel()\n",
    "\n",
    "    return tn/(tn + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_precision</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_precision(y_true, y_pred):\n",
    "    y_pred = binarize(y_pred)\n",
    "    return precision_score(y_true=y_true, y_pred=y_pred, average='weighted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_recall</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_recall(y_true, y_pred):\n",
    "    y_pred = binarize(y_pred)\n",
    "    return recall_score(y_true=y_true, y_pred=y_pred, average='weighted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_f1</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_f1(y_true, y_pred):\n",
    "    y_pred = binarize(y_pred)\n",
    "    return f1_score(y_true=y_true, y_pred=y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate each model w.r.t. _all_ evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "def model_predict(model,test_set):\n",
    "    X = test_set.iloc[:,:-1].values\n",
    "\n",
    "    if isinstance(model, sklearn.ensemble.BaggingClassifier):\n",
    "        print (\"BAGGING\")\n",
    "        print ( np.min( model.predict_proba(X)[:,0] ), np.max( model.predict_proba(X)[:,0] ) )\n",
    "        print ( np.min( model.predict_proba(X)[:,1] ), np.max( model.predict_proba(X)[:,1] ) )\n",
    "        return model.predict_proba(X)[:,1]\n",
    "        # return model.predict(X)\n",
    "    else:\n",
    "        print (\"LightGBM\")\n",
    "        print (np.unique( model.predict(X) ) )\n",
    "        return model.predict(X)\n",
    "\n",
    "def model_worst_predict(model, test_set, test_groups):\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "    preds  = model_predict(model, test_set)\n",
    "    \n",
    "    offset = 0\n",
    "    true_labels = []\n",
    "    worst_predictions = []\n",
    "    \n",
    "    for g in test_groups:\n",
    "        true_label = labels[offset]\n",
    "        true_labels.append(true_label)\n",
    "        predictions_att = preds[offset:offset+g]\n",
    "        if true_label == 1:\n",
    "            worst_predictions.append(np.min(predictions_att))\n",
    "        else:\n",
    "            worst_predictions.append(np.max(predictions_att))\n",
    "    \n",
    "        offset += g\n",
    "\n",
    "    return np.array(true_labels), np.array(worst_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_learned_models(eval_metrics, model, model_type, test, test_groups=None, budget=0):\n",
    "    # output dataframe\n",
    "    header = ['Model'] + ['Budget'] + [m.__name__.replace('eval_','').replace('_',' ').strip().title() \n",
    "                                       for m in eval_metrics]\n",
    "    df = pd.DataFrame(columns=header)\n",
    "    first_row = [model_type] + [budget] + [None for m in eval_metrics]\n",
    "    df.loc[0] = first_row\n",
    "    \n",
    "    # predictions for plan and atk datasets\n",
    "    if test_groups is None: # NOT ATKed\n",
    "        y_true = test.iloc[:,-1].values\n",
    "        y_pred = model_predict(model, test)\n",
    "    else:\n",
    "        y_true, y_pred = model_worst_predict(model, test, test_groups)\n",
    "        \n",
    "    for eval_metric in eval_metrics:\n",
    "        res = eval_metric(y_true=y_true, y_pred=y_pred)\n",
    "        print(\"{} learning - {} = {:.5f}\"\n",
    "                  .format(model_type, eval_metric.__name__, res))\n",
    "        column_metric = eval_metric.__name__\n",
    "        df[column_metric.replace('eval_','').replace('_',' ').strip().title()] = res\n",
    "\n",
    "    print(\"******************************************************************************************************\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load attacked datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an attacked dataset with a specific budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_attacked_dataset(budget):\n",
    "    # load train/valid/test (attacked)\n",
    "    train_att, valid_att, test_att = load_atk_train_valid_test(TRAINING_FILENAME_ATT.format(budget), \n",
    "                                                                  VALIDATION_FILENAME_ATT.format(budget), \n",
    "                                                                  TEST_FILENAME_ATT.format(budget))\n",
    "\n",
    "    test_groups = test_att['instance_id'].value_counts().sort_index().values\n",
    "    test_att = test_att.iloc[:, 1:]\n",
    "\n",
    "    valid_groups = valid_att['instance_id'].value_counts().sort_index().values\n",
    "    valid_att = valid_att.iloc[:, 1:]\n",
    "\n",
    "    train_groups = train_att['instance_id'].value_counts().sort_index().values\n",
    "    train_att = train_att.iloc[:, 1:]\n",
    "    \n",
    "    return train_att, train_groups, valid_att, valid_groups, test_att, test_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load _all_ the attacked datasets given a list of budgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_attacked_datasets(budgets):\n",
    "    att_datasets = {}\n",
    "    for b in budgets:\n",
    "        att_datasets[b] = load_attacked_dataset(b)\n",
    "    \n",
    "    return att_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate all models w.r.t. standard metrics (i.e., attack-free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_name(model_filename):\n",
    "    model_fileroot = model_filename.split('/')[-1].split('.')[0]\n",
    "    model_name = model_fileroot.split('_')[0].title()\n",
    "    training_budget = ''\n",
    "    budget = model_fileroot.split('_B')[-1].split('_')[0]\n",
    "    try: \n",
    "        int(budget)\n",
    "        training_budget = ' [train budget={}]'.format(budget)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return model_name + training_budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_file):\n",
    "    model = None\n",
    "    try:\n",
    "        model = lightgbm.Booster(model_file=model_file)\n",
    "    except:\n",
    "        print(\"LightGBM loading exception\")\n",
    "        try:\n",
    "            with open(model_file, 'rb') as mf:\n",
    "                model = dill.load(mf)\n",
    "                print(model)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Dill loading exception\")\n",
    "            pass\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_all_models(eval_metrics, models_dir, test, model_filenames=None):\n",
    "    \n",
    "    if model_filenames is None:\n",
    "        model_csv = sorted(glob.glob(models_dir + \"/*.csv\"))\n",
    "        model_filenames = []\n",
    "\n",
    "        for m in model_csv:\n",
    "            model_df = pd.read_csv(m)\n",
    "            # print(model_df)\n",
    "            model_filenames.append(model_df.sort_values(by='metric')['filename'].iloc[0])\n",
    "    \n",
    "    print (\"### Evaluating Models:\", model_filenames)\n",
    "    \n",
    "    df = pd.concat([eval_learned_models(eval_metrics, \n",
    "                                        load_model(mf), \n",
    "                                        extract_model_name(mf), \n",
    "                                        test) for mf in model_filenames],\n",
    "                   axis=0,\n",
    "                   sort=False\n",
    "                  )\n",
    "    \n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_all_models_under_attack_budget(eval_metrics, models_dir, test, test_groups, budget, model_filenames=None):\n",
    "    \n",
    "    #model_filenames = sorted(glob.glob(models_dir + \"/*.model\"))\n",
    "    if model_filenames is None:\n",
    "        model_csv = sorted(glob.glob(models_dir + \"/*.csv\"))\n",
    "        model_filenames = []\n",
    "\n",
    "        for m in model_csv:\n",
    "            model_df = pd.read_csv(m)\n",
    "            model_filenames.append(model_df.sort_values(by='metric')['filename'].iloc[0])\n",
    "    \n",
    "    print (\"### Evaluating Models:\", model_filenames)\n",
    "\n",
    "    df = pd.concat([eval_learned_models(eval_metrics, \n",
    "                                        load_model(mf), \n",
    "                                        extract_model_name(mf), \n",
    "                                        test,\n",
    "                                        test_groups, \n",
    "                                        budget=budget\n",
    "                                       ) for mf in model_filenames],\n",
    "                   axis=0,\n",
    "                   sort=False\n",
    "                  )\n",
    "    \n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_all_models_under_attack(eval_metrics, models_dir, att_tests, budgets, model_filenames=None):\n",
    "    \n",
    "    eval_att_dfs = []\n",
    "\n",
    "    for b in budgets:\n",
    "    \n",
    "        eval_att_dfs.append(\n",
    "            eval_all_models_under_attack_budget(eval_metrics, models_dir, att_tests[b][4], att_tests[b][5], \n",
    "                                                b, model_filenames))\n",
    "        \n",
    "        \n",
    "    eval_att_df = functools.reduce(lambda left,right: pd.merge(left,right,on=['Model', 'Budget']), eval_att_dfs)\n",
    "    eval_att_df = pd.concat(eval_att_dfs, axis=0, sort=False)\n",
    "    eval_att_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return eval_att_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_METRICS = [eval_log_loss, \n",
    "                eval_binary_err_rate,\n",
    "                eval_specificity,\n",
    "                eval_precision,\n",
    "                eval_recall,\n",
    "                eval_f1,\n",
    "                eval_roc_auc\n",
    "               ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"census\"\n",
    "TRAINING_BUDGETS= [30, 60]\n",
    "\n",
    "DATASET_DIR=\"../data/{}\".format(DATASET_NAME)\n",
    "ATK_DIR=DATASET_DIR + \"/attacks\"\n",
    "MODELS_DIR=\"../out/models/{}\".format(DATASET_NAME)\n",
    "OUTPUT_FILENAME=\"../out/results/{}\".format(DATASET_NAME)\n",
    "\n",
    "TRAINING_FILENAME=DATASET_DIR + \"/\" + \"train.csv.bz2\"\n",
    "TRAINING_FILENAME_ATT=ATK_DIR + \"/\" + \"train_B{}.atks.bz2\"\n",
    "\n",
    "VALIDATION_FILENAME=DATASET_DIR + \"/\" + \"valid.csv.bz2\"\n",
    "VALIDATION_FILENAME_ATT=ATK_DIR + \"/\" + \"valid_B{}.atks.bz2\"\n",
    "\n",
    "TEST_FILENAME=DATASET_DIR + \"/\" + \"test.csv.bz2\"\n",
    "TEST_FILENAME_ATT=ATK_DIR + \"/\" + \"test_B{}.atks.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Models\n",
    "# adv_models = [\"../out/models/census/adv-boosting_census_B30_T200_S0050_L24_R200.model\",\n",
    "#               \"../out/models/census/adv-boosting_census_B60_T200_S0050_L24_R197.model\"]\n",
    "adv_models = [\"../out/models/census/adv-boosting_census_B30_T200_S0050_L24_R200.model\",\n",
    "              \"../out/models/census/adv-boosting_census_B60_T200_S0050_L24_R200.model\"]\n",
    "\n",
    "# 20 trees\n",
    "adv_models = [\"../out/models/census/adv-boosting_census_B30_T200_S0050_L24_R200.T20.model\",\n",
    "              \"../out/models/census/adv-boosting_census_B60_T200_S0050_L24_R200.T20.model\"]\n",
    "\n",
    "# 10K instances\n",
    "adv_models = [\"../out/models/census/adv-boosting_census_B60_T200_S0050_L24_R20.model\",\n",
    "              \"../out/models/census/adv-boosting_census_B30_T200_S0050_L24_R41.model\"]\n",
    "\n",
    "gdbt_models = [\"../out/models/census/std-gbdt_census_T500_S0050_L24_R207.model\",\n",
    "               \"../out/models/census/red-gbdt_census_T500_S0050_L24_R123.model\"]\n",
    "# adding max_depth constraint\n",
    "gdbt_models = [\"../out/models/census/std-gbdt_census_T200_S0050_L24_R195.model\",\n",
    "               \"../out/models/census/red-gbdt_census_T200_S0050_L24_R101.model\"]\n",
    "\n",
    "robust_models = [\"../out/models/census/robust_census_B60_T100_D8_I20_20_20.model\"]\n",
    "robust_models = [\"../out/models/census/robust_census_B0_T100_D8_I20_20_20.model\"]\n",
    "\n",
    "test_models = gdbt_models[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without attacks\n",
    "TRAIN, VALID, TEST = load_atk_train_valid_test(TRAINING_FILENAME, VALIDATION_FILENAME, TEST_FILENAME)\n",
    "\n",
    "eval_std_df = eval_all_models(EVAL_METRICS, MODELS_DIR, TEST, test_models)\n",
    "eval_std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%capture tests\n",
    "\n",
    "# With attacks\n",
    "att_datasets = load_attacked_datasets(TRAINING_BUDGETS)\n",
    "\n",
    "eval_att_df = eval_all_models_under_attack(MODELS_DIR, att_datasets, TRAINING_BUDGETS,\n",
    "                                           test_models)\n",
    "\n",
    "overall_df = pd.concat([eval_std_df, eval_att_df], \n",
    "                       axis=0, \n",
    "                       sort=False)\n",
    "overall_df.reset_index(inplace=True, drop=True)\n",
    "overall_df.to_csv(OUTPUT_FILENAME + \".csv\", sep=\",\", index=False)\n",
    "\n",
    "overall_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"wine\"\n",
    "TRAINING_BUDGETS= [30] \n",
    "\n",
    "DATASET_DIR=\"../data/{}\".format(DATASET_NAME)\n",
    "ATK_DIR=DATASET_DIR + \"/attacks\"\n",
    "MODELS_DIR=\"../out/models/{}\".format(DATASET_NAME)\n",
    "OUTPUT_FILENAME=\"../out/results/{}\".format(DATASET_NAME)\n",
    "\n",
    "TRAINING_FILENAME=DATASET_DIR + \"/\" + \"train.csv.bz2\"\n",
    "TRAINING_FILENAME_ATT=ATK_DIR + \"/\" + \"train_B{}.atks.bz2\"\n",
    "\n",
    "VALIDATION_FILENAME=DATASET_DIR + \"/\" + \"valid.csv.bz2\"\n",
    "VALIDATION_FILENAME_ATT=ATK_DIR + \"/\" + \"valid_B{}.atks.bz2\"\n",
    "\n",
    "TEST_FILENAME=DATASET_DIR + \"/\" + \"test.csv.bz2\"\n",
    "TEST_FILENAME_ATT=ATK_DIR + \"/\" + \"test_B{}.atks.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Models\n",
    "adv_models = [\"../out/models/wine/adv-boosting_wine_B30_T200_S0050_L24_R167.model\",\n",
    "              \"../out/models/wine/adv-boosting_wine_B60_T200_S0050_L24_R200.model\"]\n",
    "\n",
    "gdbt_models = [\"../out/models/wine/std-gbdt_wine_T500_S0050_L24_R497.model\",\n",
    "               \"../out/models/wine/red-gbdt_wine_T500_S0050_L24_R497.model\"]\n",
    "\n",
    "gdbt_models = [\"../out/models/wine/std-gbdt_wine_T200_S0050_L24_R199.model\"]\n",
    "\n",
    "robust_models = [\"../out/models/wine/robust_wine_B30_T50_D8_I20.model\",\n",
    "                 \"../out/models/wine/robust_wine_B60_T50_D8_I20.model\"]\n",
    "# robust_models = [\"../out/models/wine/robust_wine_B30_T50_D8_I20_1.model\",\n",
    "#                  \"../out/models/wine/robust_wine_B60_T50_D8_I20_1.model\"]\n",
    "robust_models = [\"../out/models/wine/par-robust_wine_B30_T100_D2_I20.model\"]\n",
    "\n",
    "test_models = [\"../out/models/wine/par-robust_wine_B30_T10_D8_I20.model\",\n",
    "              \"../out/models/wine/std-gbdt_wine_T200_S0050_L24_R199.T10.model\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n",
      "### Evaluating Models: ['../out/models/wine/par-robust_wine_B30_T10_D8_I20.model', '../out/models/wine/std-gbdt_wine_T200_S0050_L24_R199.T10.model']\n",
      "LightGBM loading exception\n",
      "BaggingClassifier(base_estimator=RobustDecisionTree(attacker=<parallel_robust_forest.Attacker object at 0x7f6df72e3a58>,\n",
      "          feature_blacklist={}, max_depth=8, max_features=0.8,\n",
      "          max_samples=0.8, min_instances_per_node=20,\n",
      "          replace_features=False, replace_samples=False, seed=0,\n",
      "          split_optimizer=<parallel_robust_forest.SplitOptimizer object at 0x7f6e191f6208>,\n",
      "          tree_id=0),\n",
      "         bootstrap=False, bootstrap_features=False, max_features=1.0,\n",
      "         max_samples=1.0, n_estimators=10, n_jobs=None, oob_score=False,\n",
      "         random_state=None, verbose=0, warm_start=False)\n",
      "BAGGING\n",
      "-1.6912475864216958e-09 1.0000000230647854\n",
      "-2.3064785414916855e-08 1.0000000016912478\n",
      "Par-Robust [train budget=30] learning - eval_log_loss = 0.61272\n",
      "Par-Robust [train budget=30] learning - eval_binary_err_rate = 0.32000\n",
      "Par-Robust [train budget=30] learning - eval_specificity = 0.49266\n",
      "Par-Robust [train budget=30] learning - eval_precision = 0.67195\n",
      "Par-Robust [train budget=30] learning - eval_recall = 0.68000\n",
      "Par-Robust [train budget=30] learning - eval_f1 = 0.67407\n",
      "Par-Robust [train budget=30] learning - eval_roc_auc = 0.72075\n",
      "******************************************************************************************************\n",
      "LightGBM\n",
      "[-0.09945638 -0.09438027 -0.08914578 -0.08759938 -0.08316021 -0.07966984\n",
      " -0.07715807 -0.07704641 -0.07673475 -0.07596009 -0.07327991 -0.07110046\n",
      " -0.07034641 -0.06935924 -0.06781285 -0.06769776 -0.06684747 -0.0667358\n",
      " -0.06564949 -0.06518941 -0.06254281 -0.06078986 -0.05924346 -0.05399231\n",
      " -0.05260515 -0.05068581 -0.04926411 -0.04922679 -0.04886712 -0.0483255\n",
      " -0.04775231 -0.04675233 -0.04664067 -0.04599355 -0.04529369 -0.04528778\n",
      " -0.04420147 -0.03934184 -0.03891619 -0.03736979 -0.03581155 -0.03498309\n",
      " -0.03497718 -0.03400988 -0.03389087 -0.03292266 -0.03246349 -0.03234447\n",
      " -0.02796578 -0.02748484 -0.02578154 -0.02572617 -0.021826   -0.02159712\n",
      " -0.01873934 -0.01747408 -0.01746817 -0.01482539 -0.01379573 -0.01313612\n",
      " -0.01062435 -0.01051268 -0.00942636 -0.00941705 -0.00843033 -0.00715757\n",
      " -0.00561708 -0.00561117 -0.00456674 -0.00392338 -0.00389828 -0.00318744\n",
      " -0.00124131  0.00083965  0.00492265  0.00541261  0.00698764  0.00792439\n",
      "  0.00803605  0.00861753  0.00865214  0.00909394  0.00913905  0.01098566\n",
      "  0.01398459  0.0148849   0.01730693  0.02110534  0.02201528  0.02672633\n",
      "  0.03141771  0.03232589  0.03516639  0.03564672  0.03741435  0.04080748\n",
      "  0.04102741  0.04125267  0.04351302  0.04489246  0.0464322   0.04724339\n",
      "  0.04753808  0.04772828  0.04857128  0.04940337  0.05077844  0.05242102\n",
      "  0.05243076  0.05362603  0.05460008  0.05508438  0.05672438  0.0581761\n",
      "  0.05929788  0.05977092  0.06172373  0.06312634  0.06320689  0.06580575\n",
      "  0.06919061  0.07044495  0.07064672  0.07209146  0.07507663  0.07589429\n",
      "  0.07711754  0.07740998  0.07753076  0.07760072  0.07808713  0.08301915\n",
      "  0.08319128  0.08398716  0.08399236  0.08404679  0.08545785  0.08762832\n",
      "  0.08774679  0.08816958  0.0908651   0.09106428  0.09146751  0.09217553\n",
      "  0.09275423  0.09381405  0.09453024  0.09467151  0.09574146  0.09625054\n",
      "  0.09776796  0.09778893  0.09781628  0.10072863  0.10309062  0.10589702\n",
      "  0.10630626  0.11014515  0.11063462  0.11273877  0.11450795  0.11603708\n",
      "  0.11669264  0.11737017  0.11761513  0.11839907  0.11854885  0.11866052\n",
      "  0.11881067  0.11888338  0.11948516  0.11954344  0.11962454  0.12043579\n",
      "  0.12048009  0.12229603  0.12260231  0.12336301  0.12354046  0.12459376\n",
      "  0.12460646  0.1257491   0.12931667  0.12934673  0.13138667  0.13155619\n",
      "  0.13413962  0.13431764  0.13491234  0.1351929   0.13553239  0.13612038\n",
      "  0.13790064  0.13800535  0.13916116  0.14196554  0.14273453  0.14302656\n",
      "  0.14322941  0.14323206  0.14538258  0.14611344  0.14716363  0.14870452\n",
      "  0.14873014  0.14975073  0.149995    0.15047593  0.1512204   0.15215856\n",
      "  0.15271919  0.15341909  0.15356772  0.15473609  0.15522245  0.15530595\n",
      "  0.15930616  0.16030853  0.16040267  0.16194018  0.16326608  0.16443452\n",
      "  0.16470071  0.16546141  0.168137    0.16904521  0.1693551   0.17185647\n",
      "  0.17216024  0.17316519  0.17380346  0.17403223  0.17535635  0.179265\n",
      "  0.1799689   0.18152428  0.18245975  0.1830154   0.18733508  0.1873407\n",
      "  0.19456206  0.19530902  0.19637484  0.19681469  0.19746983  0.19859392\n",
      "  0.19967768  0.20119979  0.20192273  0.2057541   0.20602705  0.20668617\n",
      "  0.2073607   0.20872928  0.21016864  0.21117079  0.21206162  0.21226203\n",
      "  0.21234538  0.21302786  0.21311079  0.2134864   0.21501645  0.21689342\n",
      "  0.21908546  0.22031839  0.22098632  0.22398969  0.22411926  0.22437368\n",
      "  0.2252686   0.22543692  0.22650225  0.23132706  0.23195049  0.23240514\n",
      "  0.23415058  0.23623532  0.23634756  0.24028076  0.24149667  0.24479145\n",
      "  0.24576466  0.24762959  0.24968768  0.25048595  0.25267498  0.25463851\n",
      "  0.25467348  0.257301    0.25890145  0.25963095  0.26158799  0.26191972\n",
      "  0.26233759  0.2626544   0.2635886   0.26450961  0.26752575  0.26815371\n",
      "  0.26932721  0.26981636  0.27068123  0.27083316  0.27499741  0.27768282\n",
      "  0.28150519  0.28185118  0.28214571  0.28465967  0.28494559  0.28660206\n",
      "  0.28717342  0.28787454  0.28880832  0.28985598  0.29030925  0.2908597\n",
      "  0.29142624  0.2916629   0.29169487  0.29184417  0.29349929  0.29419101\n",
      "  0.29428379  0.2956358   0.29579328  0.29645858  0.29770587  0.30043446\n",
      "  0.30432955  0.30528587  0.30569764  0.30588049  0.30625431  0.30642328\n",
      "  0.30682175  0.30705579  0.30714064  0.30839189  0.31057108  0.31161451\n",
      "  0.31202852  0.31209271  0.31600791  0.3160871   0.31800957  0.31831445\n",
      "  0.31836397  0.31861805  0.32004686  0.32042345  0.32130024  0.32384149\n",
      "  0.32418856  0.32939236  0.32940529  0.33108149  0.33175195  0.33237148\n",
      "  0.33355504  0.33362335  0.3345757   0.3354055   0.33719362  0.33722668\n",
      "  0.33749186  0.33765404  0.33873139  0.33881151  0.33902375  0.33994965\n",
      "  0.34098254  0.3413874   0.34252     0.3431423   0.34450982  0.34641282\n",
      "  0.34845168  0.34875698  0.3489967   0.34916567  0.34955214  0.35384769\n",
      "  0.35627389  0.35707154  0.35778955  0.35995005  0.360372    0.36155513\n",
      "  0.36223527  0.36269813  0.36519209  0.3659991   0.36618704  0.36623288\n",
      "  0.36677753  0.36693991  0.36723319  0.36832334  0.36940821  0.36994096\n",
      "  0.37010613  0.37117505  0.37429499  0.37569767  0.37600508  0.3762357\n",
      "  0.37641013  0.37716624  0.37732676  0.37800254  0.37823873  0.37882922\n",
      "  0.38059634  0.38126546  0.3826393   0.38405265  0.38446294  0.3845382\n",
      "  0.38522579  0.38624777  0.38824997  0.38855177  0.38992245  0.39005422\n",
      "  0.39013397  0.3922445   0.39312186  0.39386431  0.39436632  0.39586136\n",
      "  0.39685331  0.39740412  0.39990534  0.40259554  0.4030377   0.40328413\n",
      "  0.40382836  0.40393147  0.40426296  0.40512694  0.40844969  0.4089398\n",
      "  0.40989212  0.41053039  0.41148146  0.41322703  0.41327649  0.41450913\n",
      "  0.41468449  0.41569001  0.41830895  0.41872654  0.42095823  0.42270647\n",
      "  0.42367598  0.42673558  0.42858872  0.42953395  0.43036063  0.43071657\n",
      "  0.43162187  0.43275978  0.43284088  0.43300208  0.43356306  0.43374834\n",
      "  0.43391824  0.43417072  0.43606962  0.43720896  0.43866642  0.44158564\n",
      "  0.44497335  0.44539572  0.44685828  0.44900246  0.45056968  0.45094499\n",
      "  0.4520179   0.45582582  0.45690317  0.45695231  0.46103593  0.47030379\n",
      "  0.47932491  0.49669616  0.51109846  0.51217581]\n",
      "Std-Gbdt learning - eval_log_loss = 0.62183\n",
      "Std-Gbdt learning - eval_binary_err_rate = 0.26923\n",
      "Std-Gbdt learning - eval_specificity = 0.35430\n",
      "Std-Gbdt learning - eval_precision = 0.74791\n",
      "Std-Gbdt learning - eval_recall = 0.73077\n",
      "Std-Gbdt learning - eval_f1 = 0.69745\n",
      "Std-Gbdt learning - eval_roc_auc = 0.79901\n",
      "******************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Budget</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>Binary Err Rate</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Par-Robust [train budget=30]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.612719</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.492662</td>\n",
       "      <td>0.671954</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.674068</td>\n",
       "      <td>0.720752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Std-Gbdt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.621829</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.354298</td>\n",
       "      <td>0.747911</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.697451</td>\n",
       "      <td>0.799008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model Budget  Log Loss  Binary Err Rate  \\\n",
       "0  Par-Robust [train budget=30]      0  0.612719         0.320000   \n",
       "1                      Std-Gbdt      0  0.621829         0.269231   \n",
       "\n",
       "   Specificity  Precision    Recall        F1   Roc Auc  \n",
       "0     0.492662   0.671954  0.680000  0.674068  0.720752  \n",
       "1     0.354298   0.747911  0.730769  0.697451  0.799008  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without attacks\n",
    "TRAIN, VALID, TEST = load_atk_train_valid_test(TRAINING_FILENAME, VALIDATION_FILENAME, TEST_FILENAME)\n",
    "\n",
    "eval_std_df = eval_all_models(EVAL_METRICS, MODELS_DIR, TEST, test_models)\n",
    "eval_std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# With attacks\n",
    "att_datasets = load_attacked_datasets(TRAINING_BUDGETS)\n",
    "\n",
    "eval_att_df = eval_all_models_under_attack(EVAL_METRICS, MODELS_DIR, att_datasets, TRAINING_BUDGETS,\n",
    "                                           test_models)\n",
    "\n",
    "overall_df = pd.concat([eval_std_df, eval_att_df], \n",
    "                       axis=0, \n",
    "                       sort=False)\n",
    "overall_df.reset_index(inplace=True, drop=True)\n",
    "overall_df.to_csv(OUTPUT_FILENAME + \".csv\", sep=\",\", index=False)\n",
    "\n",
    "overall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune Robust models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_pruned_models = [\"../out/models/census/robust_census_B0_T100_D8_I20_20.tmp\"]\n",
    "\n",
    "for m in to_be_pruned_models:\n",
    "    prune_trained_model(m, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune LGBM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_lgbm(in_file, out_file, n):\n",
    "    model = lightgbm.Booster(model_file=in_file)\n",
    "    model.save_model(out_file, num_iteration=n)\n",
    "    print (\"saved.\")\n",
    "    \n",
    "prune_lgbm(\"../out/models/wine/std-gbdt_wine_T200_S0050_L24_R199.model\",\n",
    "           \"../out/models/wine/std-gbdt_wine_T200_S0050_L24_R199.T10.model\",\n",
    "           10)\n",
    "# prune_lgbm(\"../out/models/census/adv-boosting_census_B60_T200_S0050_L24_R200.model\",\n",
    "#            \"../out/models/census/adv-boosting_census_B60_T200_S0050_L24_R200.T20.model\",\n",
    "#            20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_fx_imp(model, colnames):\n",
    "    fx_uses = model.feature_importance(importance_type='split')\n",
    "    fx_gain = model.feature_importance(importance_type='gain')\n",
    "\n",
    "    for i,f in enumerate(np.argsort(fx_gain)[::-1]):\n",
    "        print (\"{:2d} {:20s} {:.3f} {:4d}\".format(i, colnames[f], fx_gain[f], fx_uses[f]))\n",
    "\n",
    "print(\" -- GDBT --\")    \n",
    "gbdt = lightgbm.Booster(model_file=\"../out/models/census/std-gbdt_census_T100_S0050_L24_R100.model\")\n",
    "print(gbdt.num_trees())\n",
    "print_fx_imp(gbdt, TRAIN.columns)\n",
    "\n",
    "print(\" -- Reduced GDBT --\")    \n",
    "redf = lightgbm.Booster(model_file=\"../out/models/census/red-gbdt_census_T100_S0050_L24_R98.model\")\n",
    "print(redf.num_trees())\n",
    "print_fx_imp(redf, TRAIN.drop(columns=[\"workclass\", \n",
    "                                       \"marital_status\", \n",
    "                                       \"occupation\", \n",
    "                                       \"education_num\", \n",
    "                                       \"hours_per_week\", \n",
    "                                       \"capital_gain\"\n",
    "                                      ]).columns)\n",
    "\n",
    "\n",
    "print(\" -- Adv. Boosting --\")    \n",
    "advb = lightgbm.Booster(model_file=\"../out/models/census/adv-boosting_census_B30_T100_S0050_L24_R100.model\")\n",
    "print(advb.num_trees())\n",
    "print_fx_imp(advb, TRAIN.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = 40\n",
    "eval_learned_models(lightgbm.Booster(model_file=\"../out/models/wine2/red-gbdt_wine2_T500_S0050_L24_R281.model\"), \n",
    "                                        extract_model_name(\"../out/models/wine2/red-gbdt_wine2_T500_S0050_L24_R281.model\"), \n",
    "                                        att_datasets[bb][4].drop(columns=[\"alcohol\", \"residual_sugar\", \"volatile_acidity\"]), \n",
    "                                        att_datasets[bb][5], \n",
    "                                        budget=bb\n",
    "                                       ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
