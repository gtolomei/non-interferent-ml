{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models\n",
    "\n",
    "This notebook contains the code used for evaluating the following learning models:\n",
    "\n",
    "-  **Standard GBDT** (_baseline 1_)\n",
    "-  **Adversarial Boosting** (_baseline 2_)\n",
    "-  **Non-Interferent GBDT** (our proposal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-27 11:32:12,466 *** INFO [robust_forest.py:1151 - __init__()] *** ***** Robust Decision Tree successfully created *****\n",
      "2019-05-27 11:32:12,468 *** INFO [robust_forest.py:1152 - __init__()] *** *\tTree ID: 0\n",
      "2019-05-27 11:32:12,470 *** INFO [robust_forest.py:1153 - __init__()] *** *\tAttacker: <robust_forest.Attacker object at 0x7f85590785f8>\n",
      "2019-05-27 11:32:12,471 *** INFO [robust_forest.py:1155 - __init__()] *** *\tSplitting criterion: SSE\n",
      "2019-05-27 11:32:12,472 *** INFO [robust_forest.py:1156 - __init__()] *** *\tMax depth: 8\n",
      "2019-05-27 11:32:12,473 *** INFO [robust_forest.py:1158 - __init__()] *** *\tMin instances per tree node: 20\n",
      "2019-05-27 11:32:12,474 *** INFO [robust_forest.py:1160 - __init__()] *** *\tMax samples: 100.0%\n",
      "2019-05-27 11:32:12,475 *** INFO [robust_forest.py:1162 - __init__()] *** *\tMax features: 100.0%\n",
      "2019-05-27 11:32:12,476 *** INFO [robust_forest.py:1164 - __init__()] *** *\tFeature blacklist: set()\n",
      "2019-05-27 11:32:12,477 *** INFO [robust_forest.py:1165 - __init__()] *** *\tAffinity: False\n",
      "2019-05-27 11:32:12,478 *** INFO [robust_forest.py:1167 - __init__()] *** *****************************************************\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import dill\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm\n",
    "import functools\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix\n",
    "from nilib import *\n",
    "from robust_forest import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard evaluation metric\n",
    "\n",
    "The following function is the one used for evaluating the quality of the learned model (either _standard_, _adversarial-boosting_, or _non-interferent_). This is the standard <code>avg_log_loss</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(p):\n",
    "    return np.log(p/(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(preds):\n",
    "    pos = preds>0\n",
    "    neg = np.logical_not(pos)\n",
    "    preds[pos] = 1\n",
    "    preds[neg] = -1\n",
    "        \n",
    "    return preds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>avg_log_loss</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss(preds, train_data):\n",
    "    \n",
    "    labels = train_data.get_label()\n",
    "    losses = np.log(1.0 + np.exp(-preds*labels))\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return 'avg_binary_log_loss', avg_loss, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_log_loss(preds, test, test_groups=None, svm=False):\n",
    "        \n",
    "    labels = test.iloc[:,-1].values\n",
    "    losses = np.log(1.0 + np.exp(-preds*labels))\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom evaluation metric\n",
    "\n",
    "Similarly to what we have done for <code>fobj</code>, <code>feval</code> can be computed from a weighted combination of two evaluation metrics:\n",
    "\n",
    "-  <code>avg_log_loss</code> (standard, defined above);\n",
    "-  <code>avg_log_loss_uma</code> (custom, defined below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>avg_log_loss_uma</code>\n",
    "\n",
    "This is the binary log loss yet modified to operate on groups of perturbed instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom metric\n",
    "\n",
    "def binary_log_loss(pred, true_label):\n",
    "\n",
    "    return np.log(1.0 + np.exp(-pred * true_label))\n",
    "\n",
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss_uma(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    offset = 0\n",
    "    max_logloss = []\n",
    "    avg_max_logloss = 0.0\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "    \n",
    "        for atk in attack_lens:\n",
    "            losses = [binary_log_loss(h,t) for h,t in zip(preds[offset:offset+atk], labels[offset:offset+atk])]\n",
    "            max_logloss.append(max(losses))\n",
    "            \n",
    "            offset += atk\n",
    "        \n",
    "        avg_max_logloss = np.mean(max_logloss)  \n",
    "        \n",
    "    return 'avg_binary_log_loss_under_max_attack', avg_max_logloss, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_log_loss_uma(preds, test, test_groups=None, svm=False):\n",
    "    \n",
    "    lgbm_test = lightgbm.Dataset(data=test.iloc[:,:-1].values, \n",
    "                                 label=test.iloc[:,-1].values,\n",
    "                                 group=test_groups,\n",
    "                                 free_raw_data=False)\n",
    "    \n",
    "    return avg_log_loss_uma(preds,lgbm_test)[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>feval=avg_non_interferent_log_loss</code>\n",
    "\n",
    "Used for measuring the validity of any model (either _standard_, _baseline_, or _non-interferent_). More precisely, <code>avg_non_interferent_log_loss</code> is the weighted sum of the binary log loss and the binary log loss under maximal attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM takes lambda x,y: avg_weighted_sum_log_loss_log_loss_uma(preds, train_data, alpha=0.5)\n",
    "\n",
    "def avg_non_interferent_log_loss(preds, train_data, alpha=1.0):\n",
    "    \n",
    "    # binary logloss under maximal attack\n",
    "    _, loss_uma, _    = avg_log_loss_uma(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    # _, loss_plain, _  = avg_log_loss(preds, train_data)\n",
    "    \n",
    "    ids = []\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "        offset=0\n",
    "        for atk in attack_lens:\n",
    "            ids += [offset]\n",
    "            offset += atk      \n",
    "            \n",
    "    ids = np.array(ids)\n",
    "    labels = train_data.get_label()\n",
    "    losses = np.log(1.0 + np.exp(-preds[ids]*labels[ids]))\n",
    "    loss_plain = np.mean(losses)\n",
    "\n",
    "    # combine the above two losses together\n",
    "    weighted_loss = alpha*loss_uma + (1.0-alpha)*loss_plain\n",
    "\n",
    "    return 'avg_non_interferent_log_loss [alpha={:.2f}]'.format(alpha), weighted_loss, False\n",
    "\n",
    "def eval_non_interferent_log_loss(model, test, test_groups=None, svm=False, alpha=1.0):\n",
    "    \n",
    "    lgbm_test = lightgbm.Dataset(data=test.iloc[:,:-1].values, \n",
    "                                 label=test.iloc[:,-1].values,\n",
    "                                 group=test_groups,\n",
    "                                 free_raw_data=False)\n",
    "    \n",
    "    return avg_non_interferent_log_loss(model.predict(test.iloc[:,:-1].values), \n",
    "                                                  lgbm_test,\n",
    "                                                  alpha=alpha\n",
    "                                                 )[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional validity measures\n",
    "\n",
    "In addition to the evaluation metrics defined above (used for training), we also consider the following **4** measures of validity to compare the performance of each learned model:\n",
    "\n",
    "-  <code>eval_binary_err_rate</code>: This is the traditional binary error rate (1-accuracy);\n",
    "-  <code>eval_binary_err_rate_uma</code>: This is the binary error rate modified to operate on groups of perturbed instances under maximal attack.\n",
    "-  <code>eval_roc_auc</code>: This is the classical ROC AUC score;\n",
    "-  <code>eval_roc_auc_uma</code>: This is the ROC AUC score modified to operate on groups of perturbed instances under maximal attack.\n",
    "\n",
    "Again, note that those are **not** metrics used at training time (i.e., they do not define any <code>feval</code>), rather they are used to assess the (offline) quality of each learned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_binary_err_rate</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_binary_err_rate(preds, test_set, test_groups=None, svm=False):\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "        \n",
    "    errs = np.sum(binarize(preds) != labels)\n",
    "    return errs/len(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_binary_err_rate_uma</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_binary_err_rate_uma(preds, test_set, test_groups=None, svm=False):\n",
    "    preds = binarize(preds)\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "    \n",
    "    offset = 0\n",
    "    errs = 0\n",
    "\n",
    "    for g in test_groups:\n",
    "        predictions_att = preds[offset:offset+g]\n",
    "        true_label = labels[offset]\n",
    "        if np.any([p != true_label for p in predictions_att]):\n",
    "            errs += 1\n",
    "        offset += g\n",
    "\n",
    "    return errs/len(test_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_roc_auc</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_roc_auc(preds, test_set, test_groups=None, svm=False):\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "    return roc_auc_score(y_true=labels, y_score=preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_roc_auc_uma</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_roc_auc_uma(preds, test_set, test_groups=None, svm=False):\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "\n",
    "    offset = 0\n",
    "    true_labels = []\n",
    "    worst_predictions = []\n",
    "    \n",
    "    for g in test_groups:\n",
    "        \n",
    "        true_label = labels[offset]\n",
    "        true_labels.append(true_label)\n",
    "        predictions_att = preds[offset:offset+g]\n",
    "        if true_label == 1:\n",
    "            worst_predictions.append(np.min(predictions_att))\n",
    "        else:\n",
    "            worst_predictions.append(np.max(predictions_att))\n",
    "    \n",
    "        offset += g\n",
    "        \n",
    "    return roc_auc_score(y_true=true_labels, y_score=worst_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_specificity</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_specificity(preds, test_set, test_groups=None, svm=False):\n",
    "    preds = binarize(preds)\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true=labels, y_pred=preds).ravel()\n",
    "\n",
    "    return tn/(tn + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_specificity_uma</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_specificity_uma(preds, test_set, test_groups=None, svm=False):\n",
    "    preds = binarize(preds)\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "\n",
    "    offset = 0\n",
    "    true_labels = []\n",
    "    worst_predictions = []\n",
    "    \n",
    "    for g in test_groups:\n",
    "        true_label = labels[offset]\n",
    "        true_labels.append(true_label)\n",
    "        \n",
    "        predictions_att = preds[offset:offset+g]\n",
    "        if true_label == 1:\n",
    "            worst_predictions.append(np.min(predictions_att))\n",
    "        else:\n",
    "            worst_predictions.append(np.max(predictions_att))\n",
    "    \n",
    "        offset += g\n",
    "        \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true=true_labels, y_pred=worst_predictions).ravel()\n",
    "\n",
    "    return tn/(tn + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_precision</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_precision(preds, test_set, test_groups=None, svm=False):\n",
    "    preds = binarize(preds)\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true=labels, y_pred=preds).ravel()\n",
    "\n",
    "    return tp/(tp + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_precision_uma</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_precision_uma(preds, test_set, test_groups=None, svm=False):\n",
    "    preds = binarize(preds)\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "\n",
    "    offset = 0\n",
    "    true_labels = []\n",
    "    worst_predictions = []\n",
    "    \n",
    "    for g in test_groups:\n",
    "        true_label = labels[offset]\n",
    "        true_labels.append(true_label)\n",
    "        predictions_att = preds[offset:offset+g]\n",
    "        if true_label == 1:\n",
    "            worst_predictions.append(np.min(predictions_att))\n",
    "        else:\n",
    "            worst_predictions.append(np.max(predictions_att))\n",
    "    \n",
    "        offset += g\n",
    "        \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true=true_labels, y_pred=worst_predictions).ravel()\n",
    "\n",
    "    return tp/(tp + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_recall</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_recall(preds, test_set, test_groups=None, svm=False):\n",
    "    preds = binarize(preds)\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true=labels, y_pred=preds).ravel()\n",
    "\n",
    "    return tp/(tp + fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_recall_uma</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_recall_uma(preds, test_set, test_groups=None, svm=False):\n",
    "    preds = binarize(preds)\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "\n",
    "    offset = 0\n",
    "    true_labels = []\n",
    "    worst_predictions = []\n",
    "    \n",
    "    for g in test_groups:\n",
    "        true_label = labels[offset]\n",
    "        true_labels.append(true_label)\n",
    "        predictions_att = preds[offset:offset+g]\n",
    "        if true_label == 1:\n",
    "            worst_predictions.append(np.min(predictions_att))\n",
    "        else:\n",
    "            worst_predictions.append(np.max(predictions_att))\n",
    "    \n",
    "        offset += g\n",
    "        \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true=true_labels, y_pred=worst_predictions).ravel()\n",
    "\n",
    "    return tp/(tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_f1</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_f1(preds, test_set, test_groups=None, svm=False):\n",
    "    preds = binarize(preds)\n",
    "    labels = test_set.iloc[:,-1].values    \n",
    "    return f1_score(y_true=labels, y_pred=preds, average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_f1_uma</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_f1_uma(preds, test_set, test_groups=None, svm=False):\n",
    "    preds = binarize(preds)\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "\n",
    "    offset = 0\n",
    "    true_labels = []\n",
    "    worst_predictions = []\n",
    "    \n",
    "    for g in test_groups:\n",
    "        true_label = labels[offset]\n",
    "        true_labels.append(true_label)\n",
    "        predictions_att = preds[offset:offset+g]\n",
    "        if true_label == 1:\n",
    "            worst_predictions.append(np.min(predictions_att))\n",
    "        else:\n",
    "            worst_predictions.append(np.max(predictions_att))\n",
    "    \n",
    "        offset += g\n",
    "        \n",
    "    return f1_score(y_true=true_labels, y_pred=worst_predictions, average='binary')\n",
    "    \n",
    "\n",
    "#     precision_uma = eval_precision_uma(preds, test_set, test_groups=test_groups, svm=svm)\n",
    "#     recall_uma = eval_recall_uma(preds, test_set, test_groups=test_groups, svm=svm)\n",
    "    \n",
    "#     return 2 * (precision_uma * recall_uma)/(precision_uma + recall_uma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate each model w.r.t. _all_ evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(model,test_set, svm=False):\n",
    "    X = test_set.iloc[:,:-1].values \n",
    "    if not svm:\n",
    "        return model.predict(X)\n",
    "    else:\n",
    "        return logit(model.predict_proba(X)[:,1])\n",
    "\n",
    "\n",
    "# Not used yet\n",
    "def model_worst_predict(preds,test_set):\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "\n",
    "    offset = 0\n",
    "    true_labels = []\n",
    "    worst_predictions = []\n",
    "    \n",
    "    for g in test_groups:\n",
    "        true_label = labels[offset]\n",
    "        true_labels.append(true_label)\n",
    "        predictions_att = preds[offset:offset+g]\n",
    "        if true_label == 1:\n",
    "            worst_predictions.append(np.min(predictions_att))\n",
    "        else:\n",
    "            worst_predictions.append(np.max(predictions_att))\n",
    "    \n",
    "        offset += g\n",
    "        \n",
    "        \n",
    "    return true_labels, worst_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_learned_model(model, eval_metric, test, test_groups=None, svm=False):\n",
    "    return eval_metric(model, test, test_groups=test_groups, svm=svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_learned_models(model, model_type, test, test_groups=None, budget=0):\n",
    "\n",
    "    eval_metrics = EVAL_METRICS\n",
    "    d_test = \"D_test\"\n",
    "    if test_groups is not None:\n",
    "        eval_metrics = EVAL_METRICS_UNDER_MAX_ATTACK\n",
    "        d_test = \"D_test_att\"\n",
    "    \n",
    "    header = ['Model'] + ['Budget'] + [m.__name__.replace('eval_','').replace('_',' ').strip().title() \n",
    "                                       for m in EVAL_METRICS]\n",
    "    df = pd.DataFrame(columns=header)\n",
    "    first_row = [model_type] + [budget] + [None for m in EVAL_METRICS]\n",
    "    df.loc[0] = first_row\n",
    "    \n",
    "    svm = False\n",
    "    if model_type == \"SVM\":\n",
    "        svm = True\n",
    "\n",
    "    pred = model_predict(model, test, svm)\n",
    "        \n",
    "    for eval_metric in eval_metrics:\n",
    "        res = eval_learned_model(pred, eval_metric, test, test_groups=test_groups, svm=svm)\n",
    "        print(\"{} learning - {} on {} = {:.5f}\"\n",
    "                  .format(model_type, eval_metric.__name__, d_test, res))\n",
    "        column_metric = eval_metric.__name__\n",
    "        if eval_metric.__name__.endswith(\"uma\"):\n",
    "            column_metric = eval_metric.__name__.replace('uma', '')\n",
    "        df[column_metric.replace('eval_','').replace('_',' ').strip().title()] = res\n",
    "\n",
    "    print(\"******************************************************************************************************\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load attacked datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an attacked dataset with a specific budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_attacked_dataset(budget):\n",
    "    # load train/valid/test (attacked)\n",
    "    train_att, valid_att, test_att, _ = load_atk_train_valid_test(TRAINING_FILENAME_ATT.format(budget), \n",
    "                                                                  VALIDATION_FILENAME_ATT.format(budget), \n",
    "                                                                  TEST_FILENAME_ATT.format(budget))\n",
    "\n",
    "    test_groups = test_att['instance_id'].value_counts().sort_index().values\n",
    "    test_att = test_att.iloc[:, 1:]\n",
    "\n",
    "    valid_groups = valid_att['instance_id'].value_counts().sort_index().values\n",
    "    valid_att = valid_att.iloc[:, 1:]\n",
    "\n",
    "    train_groups = train_att['instance_id'].value_counts().sort_index().values\n",
    "    train_att = train_att.iloc[:, 1:]\n",
    "    \n",
    "    return train_att, train_groups, valid_att, valid_groups, test_att, test_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load _all_ the attacked datasets given a list of budgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_attacked_datasets(budgets):\n",
    "    att_datasets = {}\n",
    "    for b in budgets:\n",
    "        att_datasets[b] = load_attacked_dataset(b)\n",
    "    \n",
    "    return att_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate all models w.r.t. standard metrics (i.e., attack-free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_name(model_filename):\n",
    "    model_fileroot = model_filename.split('/')[-1].split('.')[0]\n",
    "    model_name = model_fileroot.split('_')[0].title()\n",
    "    training_budget = ''\n",
    "    budget = model_fileroot.split('_B')[-1].split('_')[0]\n",
    "    try: \n",
    "        int(budget)\n",
    "        training_budget = ' [train budget={}]'.format(budget)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return model_name + training_budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_file):\n",
    "    model = None\n",
    "    try:\n",
    "        model = lightgbm.Booster(model_file=model_file)\n",
    "    except:\n",
    "        print(\"LightGBM loading exception\")\n",
    "        try:\n",
    "            with open(model_file, 'rb') as mf:\n",
    "                model = dill.load(mf)\n",
    "                print(model)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Dill loading exception\")\n",
    "            pass\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_all_models(models_dir, test, model_filenames=None):\n",
    "    \n",
    "    if model_filenames is None:\n",
    "        model_csv = sorted(glob.glob(models_dir + \"/*.csv\"))\n",
    "        model_filenames = []\n",
    "\n",
    "        for m in model_csv:\n",
    "            model_df = pd.read_csv(m)\n",
    "            # print(model_df)\n",
    "            model_filenames.append(model_df.sort_values(by='metric')['filename'].iloc[0])\n",
    "    \n",
    "    print (\"### Evaluating Models:\", model_filenames)\n",
    "    \n",
    "    df = pd.concat([eval_learned_models(load_model(mf), \n",
    "                                        extract_model_name(mf), \n",
    "                                        test) for mf in model_filenames],\n",
    "                   axis=0,\n",
    "                   sort=False\n",
    "                  )\n",
    "    \n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_all_models_under_attack_budget(models_dir, test, test_groups, budget, model_filenames=None):\n",
    "    \n",
    "    #model_filenames = sorted(glob.glob(models_dir + \"/*.model\"))\n",
    "    if model_filenames is None:\n",
    "        model_csv = sorted(glob.glob(models_dir + \"/*.csv\"))\n",
    "        model_filenames = []\n",
    "\n",
    "        for m in model_csv:\n",
    "            model_df = pd.read_csv(m)\n",
    "            model_filenames.append(model_df.sort_values(by='metric')['filename'].iloc[0])\n",
    "    \n",
    "    print (\"### Evaluating Models:\", model_filenames)\n",
    "\n",
    "    df = pd.concat([eval_learned_models(load_model(mf), \n",
    "                                        extract_model_name(mf), \n",
    "                                        test,\n",
    "                                        test_groups, \n",
    "                                        budget=budget\n",
    "                                       ) for mf in model_filenames],\n",
    "                   axis=0,\n",
    "                   sort=False\n",
    "                  )\n",
    "    \n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_all_models_under_attack(models_dir, att_tests, budgets, model_filenames=None):\n",
    "    \n",
    "    eval_att_dfs = []\n",
    "\n",
    "    for b in budgets:\n",
    "    \n",
    "        eval_att_dfs.append(\n",
    "            eval_all_models_under_attack_budget(models_dir, att_tests[b][4], att_tests[b][5], \n",
    "                                                b, model_filenames))\n",
    "        \n",
    "        \n",
    "    eval_att_df = functools.reduce(lambda left,right: pd.merge(left,right,on=['Model', 'Budget']), eval_att_dfs)\n",
    "    eval_att_df = pd.concat(eval_att_dfs, axis=0, sort=False)\n",
    "    eval_att_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return eval_att_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_METRICS = [eval_log_loss, \n",
    "                eval_binary_err_rate,\n",
    "                eval_specificity,\n",
    "                eval_precision,\n",
    "                eval_recall,\n",
    "                eval_f1,\n",
    "                eval_roc_auc\n",
    "               ]\n",
    "\n",
    "EVAL_METRICS_UNDER_MAX_ATTACK = [eval_log_loss_uma,\n",
    "                                 eval_binary_err_rate_uma,\n",
    "                                 eval_specificity_uma,\n",
    "                                 eval_precision_uma,\n",
    "                                 eval_recall_uma,\n",
    "                                 eval_f1_uma,\n",
    "                                 eval_roc_auc_uma\n",
    "                                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"census\"\n",
    "TRAINING_BUDGETS= [30, 60]\n",
    "\n",
    "DATASET_DIR=\"../data/{}\".format(DATASET_NAME)\n",
    "ATK_DIR=DATASET_DIR + \"/attacks\"\n",
    "MODELS_DIR=\"../out/models/{}\".format(DATASET_NAME)\n",
    "OUTPUT_FILENAME=\"../out/results/{}\".format(DATASET_NAME)\n",
    "\n",
    "TRAINING_FILENAME=DATASET_DIR + \"/\" + \"train.csv.bz2\"\n",
    "TRAINING_FILENAME_ATT=ATK_DIR + \"/\" + \"train_B{}.atks.bz2\"\n",
    "\n",
    "VALIDATION_FILENAME=DATASET_DIR + \"/\" + \"valid.csv.bz2\"\n",
    "VALIDATION_FILENAME_ATT=ATK_DIR + \"/\" + \"valid_B{}.atks.bz2\"\n",
    "\n",
    "TEST_FILENAME=DATASET_DIR + \"/\" + \"test.csv.bz2\"\n",
    "TEST_FILENAME_ATT=ATK_DIR + \"/\" + \"test_B{}.atks.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Models\n",
    "# adv_models = [\"../out/models/census/adv-boosting_census_B30_T200_S0050_L24_R200.model\",\n",
    "#               \"../out/models/census/adv-boosting_census_B60_T200_S0050_L24_R197.model\"]\n",
    "adv_models = [\"../out/models/census/adv-boosting_census_B30_T200_S0050_L24_R200.model\",\n",
    "              \"../out/models/census/adv-boosting_census_B60_T200_S0050_L24_R200.model\"]\n",
    "\n",
    "# 20 trees\n",
    "adv_models = [\"../out/models/census/adv-boosting_census_B30_T200_S0050_L24_R200.T20.model\",\n",
    "              \"../out/models/census/adv-boosting_census_B60_T200_S0050_L24_R200.T20.model\"]\n",
    "\n",
    "# 10K instances\n",
    "adv_models = [\"../out/models/census/adv-boosting_census_B60_T200_S0050_L24_R20.model\",\n",
    "              \"../out/models/census/adv-boosting_census_B30_T200_S0050_L24_R41.model\"]\n",
    "\n",
    "gdbt_models = [\"../out/models/census/std-gbdt_census_T500_S0050_L24_R207.model\",\n",
    "               \"../out/models/census/red-gbdt_census_T500_S0050_L24_R123.model\"]\n",
    "# adding max_depth constraint\n",
    "gdbt_models = [\"../out/models/census/std-gbdt_census_T200_S0050_L24_R195.model\",\n",
    "               \"../out/models/census/red-gbdt_census_T200_S0050_L24_R101.model\"]\n",
    "\n",
    "robust_models = [\"../out/models/census/robust_census_B60_T100_D8_I20_20_20.model\"]\n",
    "robust_models = [\"../out/models/census/robust_census_B0_T100_D8_I20_20_20.model\"]\n",
    "\n",
    "test_models = gdbt_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n",
      "### Evaluating Models: ['../out/models/census/std-gbdt_census_T200_S0050_L24_R195.model', '../out/models/census/red-gbdt_census_T200_S0050_L24_R101.model']\n",
      "Std-Gbdt learning - eval_log_loss on D_test = 0.45497\n",
      "Std-Gbdt learning - eval_binary_err_rate on D_test = 0.13014\n",
      "Std-Gbdt learning - eval_specificity on D_test = 0.94288\n",
      "Std-Gbdt learning - eval_precision on D_test = 0.78560\n",
      "Std-Gbdt learning - eval_recall on D_test = 0.64486\n",
      "Std-Gbdt learning - eval_f1 on D_test = 0.70830\n",
      "Std-Gbdt learning - eval_roc_auc on D_test = 0.79387\n",
      "******************************************************************************************************\n",
      "Red-Gbdt learning - eval_log_loss on D_test = 0.54586\n",
      "Red-Gbdt learning - eval_binary_err_rate on D_test = 0.24502\n",
      "Red-Gbdt learning - eval_specificity on D_test = 1.00000\n",
      "Red-Gbdt learning - eval_precision on D_test = nan\n",
      "Red-Gbdt learning - eval_recall on D_test = 0.00000\n",
      "Red-Gbdt learning - eval_f1 on D_test = 0.00000\n",
      "Red-Gbdt learning - eval_roc_auc on D_test = 0.50000\n",
      "******************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucchese/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  \n",
      "/home/lucchese/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Budget</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>Binary Err Rate</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Std-Gbdt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.454968</td>\n",
       "      <td>0.130142</td>\n",
       "      <td>0.942882</td>\n",
       "      <td>0.785596</td>\n",
       "      <td>0.644856</td>\n",
       "      <td>0.708302</td>\n",
       "      <td>0.793869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Red-Gbdt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.545863</td>\n",
       "      <td>0.245024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model Budget  Log Loss  Binary Err Rate  Specificity  Precision  \\\n",
       "0  Std-Gbdt      0  0.454968         0.130142     0.942882   0.785596   \n",
       "1  Red-Gbdt      0  0.545863         0.245024     1.000000        NaN   \n",
       "\n",
       "     Recall        F1   Roc Auc  \n",
       "0  0.644856  0.708302  0.793869  \n",
       "1  0.000000  0.000000  0.500000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without attacks\n",
    "TRAIN, VALID, TEST, _ = load_atk_train_valid_test(TRAINING_FILENAME, VALIDATION_FILENAME, TEST_FILENAME)\n",
    "\n",
    "eval_std_df = eval_all_models(MODELS_DIR, TEST, test_models)\n",
    "eval_std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n",
      "Loading pre-processed files...\n",
      "### Evaluating Models: ['../out/models/census/std-gbdt_census_T200_S0050_L24_R195.model', '../out/models/census/red-gbdt_census_T200_S0050_L24_R101.model']\n",
      "Std-Gbdt learning - eval_log_loss_uma on D_test_att = 0.51090\n",
      "Std-Gbdt learning - eval_binary_err_rate_uma on D_test_att = 0.21838\n",
      "Std-Gbdt learning - eval_specificity_uma on D_test_att = 0.94259\n",
      "Std-Gbdt learning - eval_precision_uma on D_test_att = 0.61756\n",
      "Std-Gbdt learning - eval_recall_uma on D_test_att = 0.28565\n",
      "Std-Gbdt learning - eval_f1_uma on D_test_att = 0.39062\n",
      "Std-Gbdt learning - eval_roc_auc_uma on D_test_att = 0.61412\n",
      "******************************************************************************************************\n",
      "Red-Gbdt learning - eval_log_loss_uma on D_test_att = 0.54586\n",
      "Red-Gbdt learning - eval_binary_err_rate_uma on D_test_att = 0.24502\n",
      "Red-Gbdt learning - eval_specificity_uma on D_test_att = 1.00000\n",
      "Red-Gbdt learning - eval_precision_uma on D_test_att = nan\n",
      "Red-Gbdt learning - eval_recall_uma on D_test_att = 0.00000\n",
      "Red-Gbdt learning - eval_f1_uma on D_test_att = 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucchese/.local/lib/python3.6/site-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/home/lucchese/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red-Gbdt learning - eval_roc_auc_uma on D_test_att = 0.50000\n",
      "******************************************************************************************************\n",
      "### Evaluating Models: ['../out/models/census/std-gbdt_census_T200_S0050_L24_R195.model', '../out/models/census/red-gbdt_census_T200_S0050_L24_R101.model']\n",
      "Std-Gbdt learning - eval_log_loss_uma on D_test_att = 0.54900\n",
      "Std-Gbdt learning - eval_binary_err_rate_uma on D_test_att = 0.24458\n",
      "Std-Gbdt learning - eval_specificity_uma on D_test_att = 0.91652\n",
      "Std-Gbdt learning - eval_precision_uma on D_test_att = 0.50175\n",
      "Std-Gbdt learning - eval_recall_uma on D_test_att = 0.25903\n",
      "Std-Gbdt learning - eval_f1_uma on D_test_att = 0.34167\n",
      "Std-Gbdt learning - eval_roc_auc_uma on D_test_att = 0.58777\n",
      "******************************************************************************************************\n",
      "Red-Gbdt learning - eval_log_loss_uma on D_test_att = 0.54586\n",
      "Red-Gbdt learning - eval_binary_err_rate_uma on D_test_att = 0.24502\n",
      "Red-Gbdt learning - eval_specificity_uma on D_test_att = 1.00000\n",
      "Red-Gbdt learning - eval_precision_uma on D_test_att = nan\n",
      "Red-Gbdt learning - eval_recall_uma on D_test_att = 0.00000\n",
      "Red-Gbdt learning - eval_f1_uma on D_test_att = 0.00000\n",
      "Red-Gbdt learning - eval_roc_auc_uma on D_test_att = 0.50000\n",
      "******************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Budget</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>Binary Err Rate</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Std-Gbdt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.454968</td>\n",
       "      <td>0.130142</td>\n",
       "      <td>0.942882</td>\n",
       "      <td>0.785596</td>\n",
       "      <td>0.644856</td>\n",
       "      <td>0.708302</td>\n",
       "      <td>0.793869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Red-Gbdt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.545863</td>\n",
       "      <td>0.245024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Std-Gbdt</td>\n",
       "      <td>30</td>\n",
       "      <td>0.510896</td>\n",
       "      <td>0.218377</td>\n",
       "      <td>0.942589</td>\n",
       "      <td>0.617561</td>\n",
       "      <td>0.285650</td>\n",
       "      <td>0.390620</td>\n",
       "      <td>0.614120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Red-Gbdt</td>\n",
       "      <td>30</td>\n",
       "      <td>0.545863</td>\n",
       "      <td>0.245024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Std-Gbdt</td>\n",
       "      <td>60</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>0.244582</td>\n",
       "      <td>0.916520</td>\n",
       "      <td>0.501748</td>\n",
       "      <td>0.259025</td>\n",
       "      <td>0.341667</td>\n",
       "      <td>0.587773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Red-Gbdt</td>\n",
       "      <td>60</td>\n",
       "      <td>0.545863</td>\n",
       "      <td>0.245024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model Budget  Log Loss  Binary Err Rate  Specificity  Precision  \\\n",
       "0  Std-Gbdt      0  0.454968         0.130142     0.942882   0.785596   \n",
       "1  Red-Gbdt      0  0.545863         0.245024     1.000000        NaN   \n",
       "2  Std-Gbdt     30  0.510896         0.218377     0.942589   0.617561   \n",
       "3  Red-Gbdt     30  0.545863         0.245024     1.000000        NaN   \n",
       "4  Std-Gbdt     60  0.549000         0.244582     0.916520   0.501748   \n",
       "5  Red-Gbdt     60  0.545863         0.245024     1.000000        NaN   \n",
       "\n",
       "     Recall        F1   Roc Auc  \n",
       "0  0.644856  0.708302  0.793869  \n",
       "1  0.000000  0.000000  0.500000  \n",
       "2  0.285650  0.390620  0.614120  \n",
       "3  0.000000  0.000000  0.500000  \n",
       "4  0.259025  0.341667  0.587773  \n",
       "5  0.000000  0.000000  0.500000  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%capture tests\n",
    "\n",
    "# With attacks\n",
    "att_datasets = load_attacked_datasets(TRAINING_BUDGETS)\n",
    "\n",
    "eval_att_df = eval_all_models_under_attack(MODELS_DIR, att_datasets, TRAINING_BUDGETS,\n",
    "                                           test_models)\n",
    "\n",
    "overall_df = pd.concat([eval_std_df, eval_att_df], \n",
    "                       axis=0, \n",
    "                       sort=False)\n",
    "overall_df.reset_index(inplace=True, drop=True)\n",
    "overall_df.to_csv(OUTPUT_FILENAME + \".csv\", sep=\",\", index=False)\n",
    "\n",
    "overall_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"wine\"\n",
    "TRAINING_BUDGETS= [30, 60] \n",
    "\n",
    "DATASET_DIR=\"../data/{}\".format(DATASET_NAME)\n",
    "ATK_DIR=DATASET_DIR + \"/attacks\"\n",
    "MODELS_DIR=\"../out/models/{}\".format(DATASET_NAME)\n",
    "OUTPUT_FILENAME=\"../out/results/{}\".format(DATASET_NAME)\n",
    "\n",
    "TRAINING_FILENAME=DATASET_DIR + \"/\" + \"train.csv.bz2\"\n",
    "TRAINING_FILENAME_ATT=ATK_DIR + \"/\" + \"train_B{}.atks.bz2\"\n",
    "\n",
    "VALIDATION_FILENAME=DATASET_DIR + \"/\" + \"valid.csv.bz2\"\n",
    "VALIDATION_FILENAME_ATT=ATK_DIR + \"/\" + \"valid_B{}.atks.bz2\"\n",
    "\n",
    "TEST_FILENAME=DATASET_DIR + \"/\" + \"test.csv.bz2\"\n",
    "TEST_FILENAME_ATT=ATK_DIR + \"/\" + \"test_B{}.atks.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Models\n",
    "adv_models = [\"../out/models/wine/adv-boosting_wine_B30_T200_S0050_L24_R167.model\",\n",
    "              \"../out/models/wine/adv-boosting_wine_B60_T200_S0050_L24_R200.model\"]\n",
    "\n",
    "gdbt_models = [\"../out/models/wine/std-gbdt_wine_T500_S0050_L24_R497.model\",\n",
    "               \"../out/models/wine/red-gbdt_wine_T500_S0050_L24_R497.model\"]\n",
    "\n",
    "gdbt_models = [\"../out/models/wine/std-gbdt_wine_T200_S0050_L24_R199.model\"]\n",
    "\n",
    "robust_models = [\"../out/models/wine/robust_wine_B30_T50_D8_I20.model\",\n",
    "                 \"../out/models/wine/robust_wine_B60_T50_D8_I20.model\"]\n",
    "# robust_models = [\"../out/models/wine/robust_wine_B30_T50_D8_I20_1.model\",\n",
    "#                  \"../out/models/wine/robust_wine_B60_T50_D8_I20_1.model\"]\n",
    "\n",
    "test_models = gdbt_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n",
      "### Evaluating Models: ['../out/models/wine/std-gbdt_wine_T200_S0050_L24_R199.model']\n",
      "Std-Gbdt learning - eval_log_loss on D_test = 0.54035\n",
      "Std-Gbdt learning - eval_binary_err_rate on D_test = 0.22385\n",
      "Std-Gbdt learning - eval_specificity on D_test = 0.64990\n",
      "Std-Gbdt learning - eval_precision on D_test = 0.80716\n",
      "Std-Gbdt learning - eval_recall on D_test = 0.84933\n",
      "Std-Gbdt learning - eval_f1 on D_test = 0.82771\n",
      "Std-Gbdt learning - eval_roc_auc on D_test = 0.74961\n",
      "******************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Budget</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>Binary Err Rate</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Std-Gbdt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.540347</td>\n",
       "      <td>0.223846</td>\n",
       "      <td>0.649895</td>\n",
       "      <td>0.807159</td>\n",
       "      <td>0.849332</td>\n",
       "      <td>0.827709</td>\n",
       "      <td>0.749613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model Budget  Log Loss  Binary Err Rate  Specificity  Precision  \\\n",
       "0  Std-Gbdt      0  0.540347         0.223846     0.649895   0.807159   \n",
       "\n",
       "     Recall        F1   Roc Auc  \n",
       "0  0.849332  0.827709  0.749613  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without attacks\n",
    "TRAIN, VALID, TEST, _ = load_atk_train_valid_test(TRAINING_FILENAME, VALIDATION_FILENAME, TEST_FILENAME)\n",
    "\n",
    "eval_std_df = eval_all_models(MODELS_DIR, TEST, test_models)\n",
    "eval_std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n",
      "Loading pre-processed files...\n",
      "### Evaluating Models: ['../out/models/wine/std-gbdt_wine_T200_S0050_L24_R199.model']\n",
      "Std-Gbdt learning - eval_log_loss_uma on D_test_att = 0.62675\n",
      "Std-Gbdt learning - eval_binary_err_rate_uma on D_test_att = 0.37538\n",
      "Std-Gbdt learning - eval_specificity_uma on D_test_att = 0.27044\n",
      "Std-Gbdt learning - eval_precision_uma on D_test_att = 0.66246\n",
      "Std-Gbdt learning - eval_recall_uma on D_test_att = 0.82989\n",
      "Std-Gbdt learning - eval_f1_uma on D_test_att = 0.73679\n",
      "Std-Gbdt learning - eval_roc_auc_uma on D_test_att = 0.55017\n",
      "******************************************************************************************************\n",
      "### Evaluating Models: ['../out/models/wine/std-gbdt_wine_T200_S0050_L24_R199.model']\n",
      "Std-Gbdt learning - eval_log_loss_uma on D_test_att = 0.62972\n",
      "Std-Gbdt learning - eval_binary_err_rate_uma on D_test_att = 0.37769\n",
      "Std-Gbdt learning - eval_specificity_uma on D_test_att = 0.26415\n",
      "Std-Gbdt learning - eval_precision_uma on D_test_att = 0.66054\n",
      "Std-Gbdt learning - eval_recall_uma on D_test_att = 0.82989\n",
      "Std-Gbdt learning - eval_f1_uma on D_test_att = 0.73560\n",
      "Std-Gbdt learning - eval_roc_auc_uma on D_test_att = 0.54702\n",
      "******************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Budget</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>Binary Err Rate</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Std-Gbdt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.540347</td>\n",
       "      <td>0.223846</td>\n",
       "      <td>0.649895</td>\n",
       "      <td>0.807159</td>\n",
       "      <td>0.849332</td>\n",
       "      <td>0.827709</td>\n",
       "      <td>0.749613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Std-Gbdt</td>\n",
       "      <td>30</td>\n",
       "      <td>0.626749</td>\n",
       "      <td>0.375385</td>\n",
       "      <td>0.270440</td>\n",
       "      <td>0.662464</td>\n",
       "      <td>0.829891</td>\n",
       "      <td>0.736785</td>\n",
       "      <td>0.550165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Std-Gbdt</td>\n",
       "      <td>60</td>\n",
       "      <td>0.629723</td>\n",
       "      <td>0.377692</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.660542</td>\n",
       "      <td>0.829891</td>\n",
       "      <td>0.735595</td>\n",
       "      <td>0.547021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model Budget  Log Loss  Binary Err Rate  Specificity  Precision  \\\n",
       "0  Std-Gbdt      0  0.540347         0.223846     0.649895   0.807159   \n",
       "1  Std-Gbdt     30  0.626749         0.375385     0.270440   0.662464   \n",
       "2  Std-Gbdt     60  0.629723         0.377692     0.264151   0.660542   \n",
       "\n",
       "     Recall        F1   Roc Auc  \n",
       "0  0.849332  0.827709  0.749613  \n",
       "1  0.829891  0.736785  0.550165  \n",
       "2  0.829891  0.735595  0.547021  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With attacks\n",
    "att_datasets = load_attacked_datasets(TRAINING_BUDGETS)\n",
    "\n",
    "eval_att_df = eval_all_models_under_attack(MODELS_DIR, att_datasets, TRAINING_BUDGETS,\n",
    "                                           test_models)\n",
    "\n",
    "overall_df = pd.concat([eval_std_df, eval_att_df], \n",
    "                       axis=0, \n",
    "                       sort=False)\n",
    "overall_df.reset_index(inplace=True, drop=True)\n",
    "overall_df.to_csv(OUTPUT_FILENAME + \".csv\", sep=\",\", index=False)\n",
    "\n",
    "overall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune Robust models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving: ../out/models/census/robust_census_B0_T100_D8_I20_20_20.model\n"
     ]
    }
   ],
   "source": [
    "to_be_pruned_models = [\"../out/models/census/robust_census_B0_T100_D8_I20_20.tmp\"]\n",
    "\n",
    "for m in to_be_pruned_models:\n",
    "    prune_trained_model(m, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune LGBM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_lgbm(in_file, out_file, n):\n",
    "    model = lightgbm.Booster(model_file=in_file)\n",
    "    model.save_model(out_file, num_iteration=n)\n",
    "    print (\"saved.\")\n",
    "    \n",
    "prune_lgbm(\"../out/models/census/adv-boosting_census_B30_T200_S0050_L24_R200.model\",\n",
    "           \"../out/models/census/adv-boosting_census_B30_T200_S0050_L24_R200.T20.model\",\n",
    "           20)\n",
    "prune_lgbm(\"../out/models/census/adv-boosting_census_B60_T200_S0050_L24_R200.model\",\n",
    "           \"../out/models/census/adv-boosting_census_B60_T200_S0050_L24_R200.T20.model\",\n",
    "           20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_fx_imp(model, colnames):\n",
    "    fx_uses = model.feature_importance(importance_type='split')\n",
    "    fx_gain = model.feature_importance(importance_type='gain')\n",
    "\n",
    "    for i,f in enumerate(np.argsort(fx_gain)[::-1]):\n",
    "        print (\"{:2d} {:20s} {:.3f} {:4d}\".format(i, colnames[f], fx_gain[f], fx_uses[f]))\n",
    "\n",
    "print(\" -- GDBT --\")    \n",
    "gbdt = lightgbm.Booster(model_file=\"../out/models/census/std-gbdt_census_T100_S0050_L24_R100.model\")\n",
    "print(gbdt.num_trees())\n",
    "print_fx_imp(gbdt, TRAIN.columns)\n",
    "\n",
    "print(\" -- Reduced GDBT --\")    \n",
    "redf = lightgbm.Booster(model_file=\"../out/models/census/red-gbdt_census_T100_S0050_L24_R98.model\")\n",
    "print(redf.num_trees())\n",
    "print_fx_imp(redf, TRAIN.drop(columns=[\"workclass\", \n",
    "                                       \"marital_status\", \n",
    "                                       \"occupation\", \n",
    "                                       \"education_num\", \n",
    "                                       \"hours_per_week\", \n",
    "                                       \"capital_gain\"\n",
    "                                      ]).columns)\n",
    "\n",
    "\n",
    "print(\" -- Adv. Boosting --\")    \n",
    "advb = lightgbm.Booster(model_file=\"../out/models/census/adv-boosting_census_B30_T100_S0050_L24_R100.model\")\n",
    "print(advb.num_trees())\n",
    "print_fx_imp(advb, TRAIN.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = 40\n",
    "eval_learned_models(lightgbm.Booster(model_file=\"../out/models/wine2/red-gbdt_wine2_T500_S0050_L24_R281.model\"), \n",
    "                                        extract_model_name(\"../out/models/wine2/red-gbdt_wine2_T500_S0050_L24_R281.model\"), \n",
    "                                        att_datasets[bb][4].drop(columns=[\"alcohol\", \"residual_sugar\", \"volatile_acidity\"]), \n",
    "                                        att_datasets[bb][5], \n",
    "                                        budget=bb\n",
    "                                       ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
