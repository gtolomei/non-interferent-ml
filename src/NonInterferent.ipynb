{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models\n",
    "\n",
    "This notebook contains the code used for training the following learning models:\n",
    "\n",
    "-  **Standard GBDT** (_baseline 1_)\n",
    "-  **Adversarial Boosting** (_baseline 2_)\n",
    "-  **Non-Interferent GBDT** (our proposal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    " - http://lightgbm.readthedocs.io/en/latest/\n",
    " - http://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
    " - https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "import pickle\n",
    "import json\n",
    "import functools\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(dataset, categorical_features):\n",
    "    dataset_le = dataset.copy()\n",
    "    for column in dataset_le.columns:\n",
    "        if column in categorical_features:\n",
    "            dataset_le[column] = dataset_le[column].astype('category')\n",
    "            dataset_le[column] = dataset_le[column].cat.codes.astype(np.int32)\n",
    "    return dataset_le\n",
    "\n",
    "def load_atk_train_valid_test(atk_train_file, atk_valid_file, atk_test_file, \n",
    "                              train_split=0.6, valid_split=0.2, force=False):\n",
    "    \n",
    "    \n",
    "    if  (force or \n",
    "          not os.path.exists(atk_train_file+\".cat.bz2\") or\n",
    "          not os.path.exists(atk_valid_file+\".cat.bz2\") or\n",
    "          not os.path.exists(atk_test_file+\".cat.bz2\") or \n",
    "          not os.path.exists(atk_train_file+\".cat.json\") ):\n",
    "    \n",
    "        print (\"Pre-processing original files...\")\n",
    "\n",
    "        print (\"Loading:\", atk_train_file)\n",
    "        print (\"Loading:\", atk_valid_file)\n",
    "        print (\"Loading:\", atk_test_file)\n",
    "\n",
    "        train = pd.read_csv(atk_train_file)\n",
    "        valid = pd.read_csv(atk_valid_file)\n",
    "        test  = pd.read_csv(atk_test_file)\n",
    "        \n",
    "        print (\"Train/Valid/Test sizes:\", train.shape, valid.shape, test.shape)\n",
    "        print (\"Train/Valid/Test split: {:.2f} {:.2f} {:.2f}\"\n",
    "                   .format( train.shape[0]/(train.shape[0]+valid.shape[0]+test.shape[0]),\n",
    "                            valid.shape[0]/(train.shape[0]+valid.shape[0]+test.shape[0]),\n",
    "                            test.shape[0] /(train.shape[0]+valid.shape[0]+test.shape[0]) ) )\n",
    "\n",
    "\n",
    "        # split-back into train valid test\n",
    "        if 'instance_id' in train.columns.values:\n",
    "            print ('   ... with instance ids')\n",
    "            valid['instance_id'] += train.iloc[-1,0]\n",
    "            test['instance_id']  += valid.iloc[-1,0]\n",
    "            assert max(train['instance_id'])<min(valid['instance_id']), \"Instance ID mismatch\"\n",
    "            assert max(valid['instance_id'])<min(test['instance_id']), \"Instance ID mismatch\"\n",
    "            \n",
    "            groups = np.concatenate( [ train['instance_id'].value_counts().sort_index().values,\n",
    "                                       valid['instance_id'].value_counts().sort_index().values,\n",
    "                                       test['instance_id'].value_counts().sort_index().values ] )\n",
    "            \n",
    "            num_train_groups = int( len(groups)*train_split )\n",
    "            train_size = sum(groups[:num_train_groups])\n",
    "            num_valid_groups = int( len(groups)*valid_split )\n",
    "            valid_size = sum(groups[num_train_groups:num_train_groups+num_valid_groups])\n",
    "        else:\n",
    "            full_size = len(train) + len(valid) + len(test)\n",
    "            train_size = int( full_size*train_split )\n",
    "            valid_size = int( full_size*valid_split )\n",
    "        \n",
    "        # concat to process correctly label encoding\n",
    "        full = pd.concat( [train, valid, test] )\n",
    "\n",
    "        # get index of categorical features (-1 because of instance_id)\n",
    "        cat_fx = full.columns.values[np.where(full.dtypes=='object')[0]]\n",
    "        cat_fx = list(cat_fx)    \n",
    "        full = label_encode(full, cat_fx)\n",
    "        with open(atk_train_file+\".cat.json\", 'w') as fp:\n",
    "            json.dump(cat_fx, fp)\n",
    "        print (\"CatFX:\", cat_fx)\n",
    "\n",
    "        train_cat = full.iloc[0:train_size,:]\n",
    "        valid_cat = full.iloc[train_size:train_size+valid_size,:]\n",
    "        test_cat  = full.iloc[train_size+valid_size:,:]\n",
    "        \n",
    "        assert len(train_cat)+len(valid_cat)+len(test_cat)==len(full), \"Split sizes mismatch\"\n",
    "        \n",
    "\n",
    "        print (\"Train/Valid/Test sizes:\", train_cat.shape, valid_cat.shape, test_cat.shape)\n",
    "        print (\"Train/Valid/Test split: {:.2f} {:.2f} {:.2f}\"\n",
    "                   .format( train_cat.shape[0]/(train_cat.shape[0]+valid_cat.shape[0]+test_cat.shape[0]),\n",
    "                            valid_cat.shape[0]/(train_cat.shape[0]+valid_cat.shape[0]+test_cat.shape[0]),\n",
    "                            test_cat.shape[0] /(train_cat.shape[0]+valid_cat.shape[0]+test_cat.shape[0]) ) )\n",
    "\n",
    "        # save to file\n",
    "        print (\"Saving processed files *.cat.bz2\")\n",
    "        train_cat.to_csv(atk_train_file+\".cat.bz2\", compression=\"bz2\", index=False)\n",
    "        valid_cat.to_csv(atk_valid_file+\".cat.bz2\", compression=\"bz2\", index=False)\n",
    "        test_cat.to_csv (atk_test_file+\".cat.bz2\",  compression=\"bz2\", index=False)\n",
    "        \n",
    "    else:\n",
    "        print (\"Loading pre-processed files...\")\n",
    "\n",
    "        train_cat = pd.read_csv(atk_train_file+\".cat.bz2\")\n",
    "        valid_cat = pd.read_csv(atk_valid_file+\".cat.bz2\")\n",
    "        test_cat  = pd.read_csv(atk_test_file+\".cat.bz2\")\n",
    "        \n",
    "        with open(atk_train_file+\".cat.json\", 'r') as fp:\n",
    "            cat_fx = json.load(fp)\n",
    "    \n",
    "    # return data\n",
    "    return train_cat, valid_cat, test_cat, cat_fx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard\n",
    "\n",
    "The following function, called <code>optimize_log_loss</code>, is the one that should be optimized (i.e., minimized) for learning _standard_ and _baseline_ approaches. More specifically, this is the standard binary log loss which is used to train any _standard_ or _baseline_ model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $L$ = <code>optimize_log_loss</code>\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}}\\ell(h(\\mathbf{x}), y)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\ell(h(\\mathbf{x}), y) = log(1+e^{(-yh(\\mathbf{x}))})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_log_loss(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    exp_pl = np.exp(preds * labels)\n",
    "    # http://www.wolframalpha.com/input/?i=differentiate+log(1+%2B+exp(-kx)+)\n",
    "    grads = -labels / (1.0 +  exp_pl)  \n",
    "    # http://www.wolframalpha.com/input/?i=d%5E2%2Fdx%5E2+log(1+%2B+exp(-kx)+)\n",
    "    hess = labels**2 * exp_pl / (1.0 + exp_pl)**2 \n",
    "\n",
    "    # this is to optimize average logloss\n",
    "    norm = 1.0/len(preds)\n",
    "    grads *= norm\n",
    "    hess *= norm\n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom\n",
    "\n",
    "In addition to the standard binary log loss used to train a model, we introduce our custom <code>optimize_non_interferent_log_loss</code>, which is computed as the weighted combination of two objective functions, as follows:\n",
    "\n",
    "-  $L$ = <code>optimize_log_loss</code> (standard, already seen above);\n",
    "-  $L^A$ = <code>optimize_log_loss_uma</code> (custom, defined below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $L^A$ = <code>optimize_log_loss_uma</code>\n",
    "\n",
    "This function is used to train a **full** _non-interferent_ model; in other words, full non-interferent models are learned by optimizing (i.e., minimizing) the function which measures the binary log loss **under the maximal attack** possible.\n",
    "\n",
    "$$\n",
    "L^A = \\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}} \\log  \\left( \\sum_{\\mathbf{x}' \\in \\mathit{MaxAtk}({\\mathbf{x}},{A})} e^{\\ell(h(\\mathbf{x}'), y)} \\right).\n",
    "$$\n",
    "\n",
    "where still:\n",
    "\n",
    "$$\n",
    "\\ell(h(\\mathbf{x}), y) = log(1+e^{(-yh(\\mathbf{x}))})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_log_loss_uma(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    weights = train_data.get_weight()\n",
    "    \n",
    "    grads = np.zeros_like(labels, dtype=np.float64)\n",
    "    hess = np.zeros_like(grads)\n",
    "    \n",
    "    norm = 1.0 / float(len(labels))\n",
    "\n",
    "    exp_pl = np.exp(- preds * labels)\n",
    "\n",
    "    x_grad = weights * exp_pl\n",
    "\n",
    "    grads = norm * x_grad * (- labels)\n",
    "    hess  = norm * x_grad * (1.0 - x_grad)\n",
    "\n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>optimize_non_interferent_log_loss</code>\n",
    "\n",
    "$$\n",
    "\\alpha\\cdot L^A + (1-\\alpha)\\cdot L\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha \\cdot \\underbrace{\\Bigg[\\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}} \\log  \\left( \\sum_{\\mathbf{x}' \\in \\mathit{MaxAtk}({\\mathbf{x}},{A})} e^{\\ell(h(\\mathbf{x}'), y)} \\right)\\Bigg]}_{L^A} + (1-\\alpha) \\cdot \\underbrace{\\Bigg[\\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}} \\ell(h(\\mathbf{x}, y))\\Bigg]}_{L}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_non_interferent_log_loss(preds, train_data, alpha=1.0):\n",
    "    # binary logloss under maximal attack\n",
    "    # grads_uma, hess_uma = optimize_log_loss_uma(preds, train_data)\n",
    "    grads_uma, hess_uma = optimize_log_loss_uma(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    grads_plain, hess_plain = optimize_log_loss(preds, train_data)\n",
    "    \n",
    "    #print (\"uma:   \", grads_uma.min(), grads_uma.max(), hess_uma.min(), hess_uma.max())\n",
    "    #print (\"plain: \", grads_plain.min(), grads_plain.max(), hess_plain.min(), hess_plain.max())\n",
    "    #print (\"uma:   \", np.quantile(grads_uma,[.25, .75]), np.quantile( hess_uma, [.25, .75]) )\n",
    "    #print (\"plain: \", np.quantile(grads_plain,[.25, .75]), np.quantile( hess_plain, [.25, .75]) )\n",
    "    \n",
    "    # combine the above two losses together\n",
    "    k = 100 #0000\n",
    "    grads = alpha*grads_uma + (1.0-alpha)*grads_plain\n",
    "    hess  = alpha*hess_uma  + (1.0-alpha)*hess_plain\n",
    "    grads *= k\n",
    "    hess *= k\n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using one objective function for both _standard_ and _non-interferent_ learning\n",
    "\n",
    "The advantage of the <code>optimize_non_interferent_log_loss</code> function defined above is that we can wrap it so that we can use it as the only objective function (<code>fobj</code>) passed in to LightGBM. \n",
    "\n",
    "In other words, if we call <code>fobj=optimize_non_interferent_log_loss</code> with <code>alpha=0.0</code>, this will end up optimizing (i.e., minimizing) the \"vanilla\" objective function (i.e., the standard binary log loss, defined by the function <code>optimize_log_loss</code> above).\n",
    "\n",
    "Conversely, calling <code>fobj=optimize_non_interferent_log_loss</code> with <code>alpha=1.0</code> turns into optimizing (i.e., minimizing) the full non-interferent objective function (i.e., the custom binary log loss under max attack, defined by the function <code>optimize_log_loss_uma</code> above).\n",
    "\n",
    "Anything that sits in between (i.e., <code>0 < alpha < 1</code>) optimizes an objective function that trades off between the standard and the full non-interferent term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard\n",
    "\n",
    "The following function is the one used for evaluating the quality of the learned model (either _standard_, _adversarial-boosting_, or _non-interferent_). This is the standard <code>avg_log_loss</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(p):\n",
    "    return np.log(p/(1-p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>avg_log_loss</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss(preds, train_data):\n",
    "    \n",
    "    labels = train_data.get_label()\n",
    "    losses = np.log(1.0 + np.exp(-preds*labels))\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return 'avg_binary_log_loss', avg_loss, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom\n",
    "\n",
    "Similarly to what we have done for <code>fobj</code>, <code>feval</code> can be computed from a weighted combination of two evaluation metrics:\n",
    "\n",
    "-  <code>avg_log_loss</code> (standard, defined above);\n",
    "-  <code>avg_log_loss_uma</code> (custom, defined below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>avg_log_loss_uma</code>\n",
    "\n",
    "This is the binary log loss yet modified to operate on groups of perturbed instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom metrics\n",
    "def binary_log_loss(pred, true_label):\n",
    "\n",
    "    return np.log(1.0 + np.exp(-pred * true_label))\n",
    "\n",
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss_uma(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    offset = 0\n",
    "    max_logloss = []\n",
    "    avg_max_logloss = 0.0\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "    \n",
    "        for atk in attack_lens:\n",
    "            losses = [binary_log_loss(h,t) for h,t in zip(preds[offset:offset+atk], labels[offset:offset+atk])]\n",
    "            max_logloss.append(max(losses))\n",
    "\n",
    "            offset += atk\n",
    "        \n",
    "        avg_max_logloss = np.mean(max_logloss)  \n",
    "\n",
    "    return 'avg_binary_log_loss_under_max_attack', avg_max_logloss, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>feval=avg_non_interferent_log_loss</code>\n",
    "\n",
    "Used for measuring the validity of any model (either _standard_, _baseline_, or _non-interferent_). More precisely, <code>avg_non_interferent_log_loss</code> is the weighted sum of the binary log loss and the binary log loss under maximal attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_non_interferent_log_loss(preds, train_data, alpha=1.0):\n",
    "    \n",
    "    # binary logloss under maximal attack\n",
    "    _, loss_uma, _    = avg_log_loss_uma(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    #_, loss_plain, _  = avg_log_loss(preds, train_data)\n",
    "    ids = []\n",
    "    attack_lens = train_data.get_group()\n",
    "    if attack_lens is not None:\n",
    "        offset=0\n",
    "        for atk in attack_lens:\n",
    "            ids += [offset]\n",
    "            offset += atk      \n",
    "            \n",
    "    ids = np.array(ids)\n",
    "    labels = train_data.get_label()\n",
    "    losses = binary_log_loss(pred=preds[ids], true_label=labels[ids])\n",
    "    loss_plain = np.mean(losses)\n",
    "\n",
    "    # combine the above two losses together\n",
    "    weighted_loss = alpha*loss_uma + (1.0-alpha)*loss_plain\n",
    "\n",
    "    return 'avg_non_interferent_log_loss [alpha={:.2f}]'.format(alpha), weighted_loss, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Non-Interferent GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_non_interf_model(atk_train, atk_valid, cat_fx, \n",
    "                            alpha=1.0, num_trees=1, params=None):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    returns the new model (is model modified inplace?)\n",
    "    '''\n",
    "    \n",
    "    if cat_fx is None or len(cat_fx)==0:\n",
    "        cat_fx = \"auto\"\n",
    "        \n",
    "    assert atk_train.shape[1]==atk_valid.shape[1], \"Train/Valid Mismatch!\"\n",
    "    \n",
    "    train_groups = atk_train['instance_id'].value_counts().sort_index().values\n",
    "    valid_groups = atk_valid['instance_id'].value_counts().sort_index().values\n",
    "\n",
    "    original_train_ids = np.cumsum(train_groups[:-1])\n",
    "    original_train_ids = np.insert(original_train_ids, 0, 0)\n",
    "    \n",
    "    original_valid_ids = np.cumsum(valid_groups[:-1])\n",
    "    original_valid_ids = np.insert(original_valid_ids, 0, 0)\n",
    "\n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(atk_train.columns.isin(cat_fx))[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])\n",
    "    print (\"CatFX:\", atk_train.columns.values[cat_fx])\n",
    "\n",
    "    # remove instance id\n",
    "    atk_train = atk_train.iloc[:,1:].values\n",
    "    atk_valid = atk_valid.iloc[:,1:].values\n",
    "    cat_fx = [x - 1 for x in cat_fx]\n",
    "        \n",
    "    unatk_train = atk_train[original_train_ids,:]\n",
    "    unatk_valid = atk_valid[original_valid_ids,:]\n",
    "\n",
    "    \n",
    "    # -------------------------\n",
    "    # train first iteration\n",
    "    lgbm_train = lightgbm.Dataset(data=unatk_train[:,:-1], \n",
    "                                  label=unatk_train[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_valid = lightgbm.Dataset(data=unatk_valid[:,:-1], \n",
    "                                  label=unatk_valid[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "\n",
    "    lgbm_info = {}\n",
    "    lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                num_boost_round = 1,\n",
    "                                fobj  = optimize_log_loss, \n",
    "                                feval = avg_log_loss,\n",
    "                                evals_result = lgbm_info,\n",
    "                                valid_sets   = [lgbm_train, lgbm_valid], \n",
    "                                valid_names  = ['train', 'valid'],\n",
    "                                verbose_eval=5)\n",
    "\n",
    "    # -------------------------\n",
    "    # train other iteration\n",
    "    \n",
    "    def get_ni_w(preds, labels, groups):\n",
    "        # \"weights\"\n",
    "        w = np.ones(len(groups))\n",
    "\n",
    "        offset = 0\n",
    "        for instance_id, g in enumerate(groups):\n",
    "            exp_pl = np.exp(- preds[offset:offset+g] * labels[offset:offset+g])\n",
    "            # can we replace with e^max\n",
    "            w[instance_id] = 1.0 / np.max(1.0 + exp_pl)\n",
    "            offset += g  \n",
    "\n",
    "        return w\n",
    "    \n",
    "\n",
    "    for t in range (1, num_trees):\n",
    "    \n",
    "        # get predictions on atk instances\n",
    "        train_preds  = lgbm_model.predict(atk_train[:,:-1])\n",
    "        train_labels = atk_train[:,-1]\n",
    "        train_weights = get_ni_w(train_preds, train_labels, train_groups)\n",
    "                \n",
    "        # repeat for validation\n",
    "        valid_preds  = lgbm_model.predict(atk_valid[:,:-1])\n",
    "        valid_labels = atk_valid[:,-1]\n",
    "        valid_weights = get_ni_w(valid_preds, valid_labels, valid_groups)\n",
    "        \n",
    "        # prepare data and train\n",
    "        lgbm_train = lightgbm.Dataset(data=unatk_train[:,:-1], \n",
    "                                      label=unatk_train[:,-1],\n",
    "                                      weight=train_weights,\n",
    "                                      categorical_feature = cat_fx)\n",
    "\n",
    "        lgbm_valid = lightgbm.Dataset(data=unatk_valid[:,:-1], \n",
    "                                      label=unatk_valid[:,-1],\n",
    "                                      weight=valid_weights,\n",
    "                                      categorical_feature = cat_fx)\n",
    "\n",
    "        new_lgbm_info = {}\n",
    "        lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                    num_boost_round = 1, \n",
    "                                    init_model = lgbm_model,\n",
    "                                    fobj  = functools.partial(optimize_non_interferent_log_loss, alpha=alpha), \n",
    "                                    feval = avg_log_loss,# functools.partial(avg_non_interferent_log_loss, alpha=alpha),\n",
    "                                    evals_result = new_lgbm_info,\n",
    "                                    valid_sets   = [lgbm_train, lgbm_valid], \n",
    "                                    valid_names  = ['train', 'valid'],\n",
    "                                    verbose_eval=5)\n",
    "        \n",
    "        awesome_hack = \"avg_binary_log_loss\"\n",
    "        lgbm_info['train'][awesome_hack] += new_lgbm_info['train'][awesome_hack]\n",
    "        lgbm_info['valid'][awesome_hack] += new_lgbm_info['valid'][awesome_hack]\n",
    "\n",
    "\n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_non_interferent(train_file, valid_file, test_file, output_model_file):\n",
    "    exp = pd.DataFrame(columns=['num_trees', 'learning_rate', 'num_leaves', 'alpha', 'best_round', 'avg_non_interferent_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    atk_train, atk_valid, atk_test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "    \n",
    "    for num_trees in [150]:\n",
    "        for alpha in [0.1, 0.5, 1.0]: #[0.25, 0.50, 0.75, 1.00]:\n",
    "            best_model = None\n",
    "            best_info = None\n",
    "            best_loss = np.inf\n",
    "            awesome_hack = \"avg_non_interferent_log_loss\" + \" [alpha={:.2f}]\".format(alpha)\n",
    "            awesome_hack = \"avg_binary_log_loss\"\n",
    "            \n",
    "            for learning_rate in [0.1]: #[0.01, 0.05, 0.1]:\n",
    "                for num_leaves in [16]: #[8, 16, 24, 32]:\n",
    "                    \n",
    "                    \n",
    "                    lgbm_params = { 'learning_rate': learning_rate, \n",
    "                                    'num_leaves': num_leaves} \n",
    "                    lgbm_model, lgbm_info = extend_non_interf_model(atk_train, atk_valid, cat_fx, \n",
    "                                alpha=alpha, num_trees=num_trees, params=lgbm_params)\n",
    "                    \n",
    "                    if np.min(lgbm_info['valid'][awesome_hack]) < best_loss:\n",
    "                        best_model = lgbm_model\n",
    "                        best_info = lgbm_info\n",
    "                        best_loss = np.min(lgbm_info['valid'][awesome_hack])\n",
    "                        best_info['num_trees'] = num_trees\n",
    "                        best_info['learning_rate'] = learning_rate\n",
    "                        best_info['num_leaves'] = num_leaves\n",
    "                \n",
    "\n",
    "                    # save file\n",
    "\n",
    "                    best_valid_iter = np.argmin(lgbm_info['valid'][awesome_hack])\n",
    "\n",
    "                    # update experimental results\n",
    "                    exp = exp.append({'num_trees': num_trees, \n",
    "                                  'learning_rate':learning_rate,\n",
    "                                  'num_leaves':num_leaves, \n",
    "                                  'alpha': alpha,\n",
    "                                  'best_round':best_valid_iter+1, \n",
    "                                  'avg_non_interferent_log_loss':lgbm_info['valid'][awesome_hack][best_valid_iter]},\n",
    "                                 ignore_index=True)\n",
    "            \n",
    "                best_valid_iter = np.argmin(best_info['valid'][awesome_hack])\n",
    "            \n",
    "                model_file_name = \"{:s}_T{:d}_S{:04d}_L{:d}_A{:03d}_R{:d}.model\".format(output_model_file,\n",
    "                                                                                    best_info['num_trees'],\n",
    "                                                                                    int(best_info['learning_rate']*1000),\n",
    "                                                                                    best_info['num_leaves'],\n",
    "                                                                                    int(alpha * 100),\n",
    "                                                                                    best_valid_iter + 1\n",
    "                                                                                   )\n",
    "            \n",
    "            \n",
    "                best_model.save_model(model_file_name)\n",
    "                print (\"Model saved to\", model_file_name)\n",
    "                    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n",
      "CatFX: ['workclass' 'marital_status' 'occupation' 'relationship' 'race' 'sex'\n",
      " 'native_country']\n",
      "[5]\ttrain's avg_binary_log_loss: 0.518768\tvalid's avg_binary_log_loss: 0.519605\n",
      "[10]\ttrain's avg_binary_log_loss: 0.43366\tvalid's avg_binary_log_loss: 0.435319\n",
      "[15]\ttrain's avg_binary_log_loss: 0.381765\tvalid's avg_binary_log_loss: 0.383529\n",
      "[20]\ttrain's avg_binary_log_loss: 0.347723\tvalid's avg_binary_log_loss: 0.34957\n",
      "[25]\ttrain's avg_binary_log_loss: 0.326516\tvalid's avg_binary_log_loss: 0.328776\n",
      "[30]\ttrain's avg_binary_log_loss: 0.314443\tvalid's avg_binary_log_loss: 0.317073\n",
      "[35]\ttrain's avg_binary_log_loss: 0.306115\tvalid's avg_binary_log_loss: 0.309176\n",
      "[40]\ttrain's avg_binary_log_loss: 0.299677\tvalid's avg_binary_log_loss: 0.303293\n",
      "[45]\ttrain's avg_binary_log_loss: 0.295728\tvalid's avg_binary_log_loss: 0.29994\n",
      "[50]\ttrain's avg_binary_log_loss: 0.2918\tvalid's avg_binary_log_loss: 0.297164\n",
      "[55]\ttrain's avg_binary_log_loss: 0.288521\tvalid's avg_binary_log_loss: 0.294707\n",
      "[60]\ttrain's avg_binary_log_loss: 0.286208\tvalid's avg_binary_log_loss: 0.293359\n",
      "[65]\ttrain's avg_binary_log_loss: 0.283313\tvalid's avg_binary_log_loss: 0.291155\n",
      "[70]\ttrain's avg_binary_log_loss: 0.281196\tvalid's avg_binary_log_loss: 0.289723\n",
      "[75]\ttrain's avg_binary_log_loss: 0.279197\tvalid's avg_binary_log_loss: 0.288404\n",
      "[80]\ttrain's avg_binary_log_loss: 0.277663\tvalid's avg_binary_log_loss: 0.287739\n",
      "[85]\ttrain's avg_binary_log_loss: 0.276146\tvalid's avg_binary_log_loss: 0.287392\n",
      "[90]\ttrain's avg_binary_log_loss: 0.274679\tvalid's avg_binary_log_loss: 0.286693\n",
      "[95]\ttrain's avg_binary_log_loss: 0.273308\tvalid's avg_binary_log_loss: 0.286484\n",
      "[100]\ttrain's avg_binary_log_loss: 0.271996\tvalid's avg_binary_log_loss: 0.285566\n",
      "[105]\ttrain's avg_binary_log_loss: 0.270687\tvalid's avg_binary_log_loss: 0.285134\n",
      "[110]\ttrain's avg_binary_log_loss: 0.269563\tvalid's avg_binary_log_loss: 0.284833\n",
      "[115]\ttrain's avg_binary_log_loss: 0.268484\tvalid's avg_binary_log_loss: 0.284495\n",
      "[120]\ttrain's avg_binary_log_loss: 0.267387\tvalid's avg_binary_log_loss: 0.284294\n",
      "[125]\ttrain's avg_binary_log_loss: 0.266458\tvalid's avg_binary_log_loss: 0.284039\n",
      "[130]\ttrain's avg_binary_log_loss: 0.265444\tvalid's avg_binary_log_loss: 0.283922\n",
      "[135]\ttrain's avg_binary_log_loss: 0.264632\tvalid's avg_binary_log_loss: 0.283816\n",
      "[140]\ttrain's avg_binary_log_loss: 0.263654\tvalid's avg_binary_log_loss: 0.283805\n",
      "[145]\ttrain's avg_binary_log_loss: 0.262787\tvalid's avg_binary_log_loss: 0.283984\n",
      "[150]\ttrain's avg_binary_log_loss: 0.262005\tvalid's avg_binary_log_loss: 0.28393\n",
      "Model saved to ../out/models/non_interferent_census_B150_T150_S0100_L16_A010_R140.model\n",
      "CatFX: ['workclass' 'marital_status' 'occupation' 'relationship' 'race' 'sex'\n",
      " 'native_country']\n",
      "[5]\ttrain's avg_binary_log_loss: 0.518884\tvalid's avg_binary_log_loss: 0.519669\n",
      "[10]\ttrain's avg_binary_log_loss: 0.434778\tvalid's avg_binary_log_loss: 0.436308\n",
      "[15]\ttrain's avg_binary_log_loss: 0.382131\tvalid's avg_binary_log_loss: 0.383868\n",
      "[20]\ttrain's avg_binary_log_loss: 0.348358\tvalid's avg_binary_log_loss: 0.350326\n",
      "[25]\ttrain's avg_binary_log_loss: 0.32748\tvalid's avg_binary_log_loss: 0.329778\n",
      "[30]\ttrain's avg_binary_log_loss: 0.315254\tvalid's avg_binary_log_loss: 0.317834\n",
      "[35]\ttrain's avg_binary_log_loss: 0.306346\tvalid's avg_binary_log_loss: 0.309336\n",
      "[40]\ttrain's avg_binary_log_loss: 0.300107\tvalid's avg_binary_log_loss: 0.303297\n",
      "[45]\ttrain's avg_binary_log_loss: 0.295142\tvalid's avg_binary_log_loss: 0.299065\n",
      "[50]\ttrain's avg_binary_log_loss: 0.291412\tvalid's avg_binary_log_loss: 0.296159\n",
      "[55]\ttrain's avg_binary_log_loss: 0.2887\tvalid's avg_binary_log_loss: 0.294228\n",
      "[60]\ttrain's avg_binary_log_loss: 0.286153\tvalid's avg_binary_log_loss: 0.292658\n",
      "[65]\ttrain's avg_binary_log_loss: 0.283811\tvalid's avg_binary_log_loss: 0.291065\n",
      "[70]\ttrain's avg_binary_log_loss: 0.281473\tvalid's avg_binary_log_loss: 0.289978\n",
      "[75]\ttrain's avg_binary_log_loss: 0.279676\tvalid's avg_binary_log_loss: 0.288983\n",
      "[80]\ttrain's avg_binary_log_loss: 0.278071\tvalid's avg_binary_log_loss: 0.288317\n",
      "[85]\ttrain's avg_binary_log_loss: 0.276315\tvalid's avg_binary_log_loss: 0.287259\n",
      "[90]\ttrain's avg_binary_log_loss: 0.274875\tvalid's avg_binary_log_loss: 0.286721\n",
      "[95]\ttrain's avg_binary_log_loss: 0.273554\tvalid's avg_binary_log_loss: 0.286273\n",
      "[100]\ttrain's avg_binary_log_loss: 0.27232\tvalid's avg_binary_log_loss: 0.286091\n",
      "[105]\ttrain's avg_binary_log_loss: 0.271088\tvalid's avg_binary_log_loss: 0.285758\n",
      "[110]\ttrain's avg_binary_log_loss: 0.269904\tvalid's avg_binary_log_loss: 0.285525\n",
      "[115]\ttrain's avg_binary_log_loss: 0.268787\tvalid's avg_binary_log_loss: 0.285184\n",
      "[120]\ttrain's avg_binary_log_loss: 0.267662\tvalid's avg_binary_log_loss: 0.284839\n",
      "[125]\ttrain's avg_binary_log_loss: 0.266556\tvalid's avg_binary_log_loss: 0.284593\n",
      "[130]\ttrain's avg_binary_log_loss: 0.265562\tvalid's avg_binary_log_loss: 0.284298\n",
      "[135]\ttrain's avg_binary_log_loss: 0.26457\tvalid's avg_binary_log_loss: 0.283979\n",
      "[140]\ttrain's avg_binary_log_loss: 0.263686\tvalid's avg_binary_log_loss: 0.283781\n",
      "[145]\ttrain's avg_binary_log_loss: 0.26284\tvalid's avg_binary_log_loss: 0.283757\n",
      "[150]\ttrain's avg_binary_log_loss: 0.26203\tvalid's avg_binary_log_loss: 0.283594\n",
      "Model saved to ../out/models/non_interferent_census_B150_T150_S0100_L16_A050_R147.model\n",
      "CatFX: ['workclass' 'marital_status' 'occupation' 'relationship' 'race' 'sex'\n",
      " 'native_country']\n",
      "[5]\ttrain's avg_binary_log_loss: 0.518955\tvalid's avg_binary_log_loss: 0.519967\n",
      "[10]\ttrain's avg_binary_log_loss: 0.435474\tvalid's avg_binary_log_loss: 0.436906\n",
      "[15]\ttrain's avg_binary_log_loss: 0.380186\tvalid's avg_binary_log_loss: 0.381655\n",
      "[20]\ttrain's avg_binary_log_loss: 0.347954\tvalid's avg_binary_log_loss: 0.349692\n",
      "[25]\ttrain's avg_binary_log_loss: 0.329268\tvalid's avg_binary_log_loss: 0.332153\n",
      "[30]\ttrain's avg_binary_log_loss: 0.316535\tvalid's avg_binary_log_loss: 0.319881\n",
      "[35]\ttrain's avg_binary_log_loss: 0.308288\tvalid's avg_binary_log_loss: 0.311829\n",
      "[40]\ttrain's avg_binary_log_loss: 0.302732\tvalid's avg_binary_log_loss: 0.307041\n",
      "[45]\ttrain's avg_binary_log_loss: 0.298457\tvalid's avg_binary_log_loss: 0.303627\n",
      "[50]\ttrain's avg_binary_log_loss: 0.294611\tvalid's avg_binary_log_loss: 0.300561\n",
      "[55]\ttrain's avg_binary_log_loss: 0.291149\tvalid's avg_binary_log_loss: 0.297583\n",
      "[60]\ttrain's avg_binary_log_loss: 0.288184\tvalid's avg_binary_log_loss: 0.295744\n",
      "[65]\ttrain's avg_binary_log_loss: 0.285685\tvalid's avg_binary_log_loss: 0.294294\n",
      "[70]\ttrain's avg_binary_log_loss: 0.283738\tvalid's avg_binary_log_loss: 0.292957\n",
      "[75]\ttrain's avg_binary_log_loss: 0.282038\tvalid's avg_binary_log_loss: 0.292132\n",
      "[80]\ttrain's avg_binary_log_loss: 0.280296\tvalid's avg_binary_log_loss: 0.290978\n",
      "[85]\ttrain's avg_binary_log_loss: 0.278851\tvalid's avg_binary_log_loss: 0.290595\n",
      "[90]\ttrain's avg_binary_log_loss: 0.27729\tvalid's avg_binary_log_loss: 0.289488\n",
      "[95]\ttrain's avg_binary_log_loss: 0.275814\tvalid's avg_binary_log_loss: 0.289323\n",
      "[100]\ttrain's avg_binary_log_loss: 0.274703\tvalid's avg_binary_log_loss: 0.289132\n",
      "[105]\ttrain's avg_binary_log_loss: 0.273319\tvalid's avg_binary_log_loss: 0.288763\n",
      "[110]\ttrain's avg_binary_log_loss: 0.27221\tvalid's avg_binary_log_loss: 0.288345\n",
      "[115]\ttrain's avg_binary_log_loss: 0.270995\tvalid's avg_binary_log_loss: 0.288039\n",
      "[120]\ttrain's avg_binary_log_loss: 0.270128\tvalid's avg_binary_log_loss: 0.287796\n",
      "[125]\ttrain's avg_binary_log_loss: 0.269162\tvalid's avg_binary_log_loss: 0.287799\n",
      "[130]\ttrain's avg_binary_log_loss: 0.268269\tvalid's avg_binary_log_loss: 0.287541\n",
      "[135]\ttrain's avg_binary_log_loss: 0.26727\tvalid's avg_binary_log_loss: 0.287212\n",
      "[140]\ttrain's avg_binary_log_loss: 0.266417\tvalid's avg_binary_log_loss: 0.287201\n",
      "[145]\ttrain's avg_binary_log_loss: 0.265349\tvalid's avg_binary_log_loss: 0.287006\n",
      "[150]\ttrain's avg_binary_log_loss: 0.264534\tvalid's avg_binary_log_loss: 0.286699\n",
      "Model saved to ../out/models/non_interferent_census_B150_T150_S0100_L16_A100_R150.model\n",
      "   num_trees  learning_rate  num_leaves  alpha  best_round  \\\n",
      "0      150.0            0.1        16.0    0.1       140.0   \n",
      "1      150.0            0.1        16.0    0.5       147.0   \n",
      "2      150.0            0.1        16.0    1.0       150.0   \n",
      "\n",
      "   avg_non_interferent_log_loss  \n",
      "0                      0.283805  \n",
      "1                      0.283534  \n",
      "2                      0.286699  \n"
     ]
    }
   ],
   "source": [
    "# enable/disable\n",
    "if True:\n",
    "    for B in [150]: #[5, 15, 150, 300]:\n",
    "\n",
    "        experiments = train_non_interferent(\"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../data/census/test_B{:d}.csv.bz2\".format(B),\n",
    "                                                   \"../out/models/non_interferent_census_B{:d}\".format(B))  \n",
    "\n",
    "        experiments.to_csv('../out/models/non_interferent_census_B{:d}.csv'.format(B), index=False)\n",
    "\n",
    "        print (experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
